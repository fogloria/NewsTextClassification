{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN\n",
    "TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。\n",
    "\n",
    "这里我们基于TextCNN原始论文的设定，分别采用了100个大小为2,3,4的卷积核，最后得到的文本向量大小为100*3=300维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 15.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.21.3)\n",
      "Collecting requests (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 17.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl\n",
      "Collecting sacremoses (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 23.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.48.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.0)\n",
      "Collecting dataclasses; python_version < \"3.7\" (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\n",
      "Collecting sentencepiece!=0.1.92 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 46.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc1 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 55.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/f2/b3af9ce9df4b7e121dfeece41fc95e37b14f0153821f35d08edb0b0813ff/regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660kB)\n",
      "\u001b[K     |████████████████████████████████| 665kB 59.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting packaging (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.3.0)\n",
      "Collecting chardet<4,>=3.0.2 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 58.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 27.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 50.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 40.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Collecting click (from sacremoses->transformers)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 31.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6708c8659d4bc565496059329f6a15300e2fbfa10e21df9e716b8f5a7031a0f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: chardet, idna, urllib3, certifi, requests, filelock, regex, click, sacremoses, dataclasses, sentencepiece, tokenizers, packaging, transformers\n",
      "\u001b[33m  WARNING: The script chardetect is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sacremoses is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script transformers-cli is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed certifi-2020.6.20 chardet-3.0.4 click-7.1.2 dataclasses-0.7 filelock-3.0.12 idna-2.10 packaging-20.4 regex-2020.7.14 requests-2.24.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2 urllib3-1.25.10\n",
      "\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem               Size  Used Avail Use% Mounted on\n",
      "overlay                  8.0G  179M  7.9G   3% /\n",
      "tmpfs                     64M     0   64M   0% /dev\n",
      "tmpfs                     15G     0   15G   0% /sys/fs/cgroup\n",
      "/dev/mapper/ubuntu-root  150G   51G  100G  34% /dev/init\n",
      ":/export/smodsoz9        4.9G   20M  4.6G   1% /storage\n",
      ":/export/datasets        2.0T  673G  1.3T  36% /datasets\n",
      "shm                       12G     0   12G   0% /dev/shm\n",
      "tmpfs                     15G   12K   15G   1% /proc/driver/nvidia\n",
      "tmpfs                    3.0G  314M  2.7G  11% /run/nvidia-persistenced/socket\n",
      "udev                      15G     0   15G   0% /dev/nvidia0\n",
      "tmpfs                     15G     0   15G   0% /proc/asound\n",
      "tmpfs                     15G     0   15G   0% /proc/acpi\n",
      "tmpfs                     15G     0   15G   0% /proc/scsi\n",
      "tmpfs                     15G     0   15G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            29G        1.4G         16G        314M         11G         27G\n",
      "Swap:            0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-30 02:05:56,926 INFO: Use cuda: True, gpu id: 0.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "# 日志输出配置\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# 种子值\n",
    "seed = 666\n",
    "\n",
    "# 改变随机数生成器的种子\n",
    "random.seed(seed)\n",
    "# 指定随机数生成时所用算法开始的整数值\n",
    "np.random.seed(seed)\n",
    "# 为当前GPU设置随机种子；如果使用多个GPU，应该使用\n",
    "torch.cuda.manual_seed(seed)\n",
    "# 为CPU设置种子用于生成随机数，以使得结果是确定的\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 设置cuda\n",
    "gpu = 0\n",
    "# 如果给定gpu数以及gpu是否可用，来判定是否使用cuda\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "\n",
    "# 如果使用cuda，设置成gpu\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "# 如果不使用cuda，设置成cpu\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 日志输出计算资源    \n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分割成10陇\n",
    "fold_num = 10\n",
    "\n",
    "# 训练数据文件\n",
    "data_file = '../input/train_set.csv'\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-30 05:38:01,993 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "# 把所有数据转换到陇\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    # 定义返回用fold_data\n",
    "    fold_data = []\n",
    "    \n",
    "    # 使用TAB做分隔符，编码UTF-8读取数据文件\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    # 从读取的文件数据中，截取text字段（num条记录）\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    \n",
    "    # 从读取的文件数据中，截取label字段（num条记录）\n",
    "    labels = f['label'].tolist()[:num]\n",
    "\n",
    "    # 记录截取到的标签总数\n",
    "    total = len(labels)\n",
    "\n",
    "    # 根据记录的总数，生成有序的索引数组\n",
    "    index = list(range(total))\n",
    "    \n",
    "    # 对有序索引进行洗牌，使其变成无序\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    # 为保存无序text和label，定义下面两个变量\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 利用无序索引，从读到的文件数据，依次放入\n",
    "    # 无序的数组变量中\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "    \n",
    "    label2id = {}# 定义label2id字典\n",
    "    \n",
    "    # 从0开始到total，给各个key：val赋值成label：0～total\n",
    "    for i in range(total):\n",
    "        # 从无序的all_labels中取索引是i的label，并字符串化\n",
    "        label = str(all_labels[i])\n",
    "        \n",
    "        '''\n",
    "        收集所有标签是label的索引，到此标签下面\n",
    "        '''\n",
    "        \n",
    "        # 如果此label作为key在label2id字典中，不存在的话：\n",
    "        if label not in label2id:\n",
    "            \n",
    "            # label2id字典的label作为key，更改成[i]数组作为值\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "        # 如果已经存在的话，在既存数组后追加i\n",
    "            # label2id字典的label作为key，更改成[i]作为值\n",
    "            label2id[label].append(i)\n",
    "    \n",
    "    # 根据陇数fold_num，分陇存储各个索引值\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    \n",
    "    # 遍历label2id字典所有项目，进行处理\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        \n",
    "        # 根据标签相对应的索引数组数 除以陇数，向下取整后；得到批量尺寸\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        \n",
    "        # 由总索引数组数，减去（批量尺寸 * 陇数）；得到溢出的索引数量\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        \n",
    "        # 从0开始fold_num陇数，为每一陇分配数据\n",
    "        for i in range(fold_num):\n",
    "            \n",
    "            # 得到当前批量尺寸\n",
    "            # ：如果当前陇序号 >= 溢出数量，批量尺寸不变\n",
    "            # ：如果当前陇序号 <  溢出数量，批量尺寸 + 1\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            \n",
    "            # 按批量尺寸，分批放入对应的陇数组中\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            \n",
    "            # i陇的all_index[i]末尾，一次性追加batch_data序列值\n",
    "            all_index[i].extend(batch_data)\n",
    "    \n",
    "    # 根据总标签数，陇数，来确定批量尺寸\n",
    "    batch_size = int(total / fold_num)\n",
    "    # 定义溢出text数组\n",
    "    other_texts = []\n",
    "    # 定义溢出label数组\n",
    "    other_labels = []\n",
    "    # 定义溢出数\n",
    "    other_num = 0\n",
    "    \n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        \n",
    "        # 获取每一陇存储的索引数组的尺寸\n",
    "        num = len(all_index[fold])\n",
    "        \n",
    "        # 从每一陇索引数组，取出无序text中的所有text\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        \n",
    "        # 从每一陇索引数组，取出无序label中的所有label\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "        \n",
    "        # 如果每一陇存储的索引数量 大于批量尺寸的话：\n",
    "        if num > batch_size:\n",
    "            # 截取到批量尺寸的texts，放到陇text\n",
    "            fold_texts = texts[:batch_size]\n",
    "            \n",
    "            # 溢出批量尺寸的texts，放到溢出text\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            \n",
    "            # 截取到批量尺寸的labels，放到陇label\n",
    "            fold_labels = labels[:batch_size]\n",
    "            \n",
    "            # 溢出批量尺寸的labels，放到溢出label\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            \n",
    "            # 收集每一陇的溢出数，进行累计\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "        # 如果每一陇存储的索引数量 小于批量尺寸的话：\n",
    "            \n",
    "            # 设置结束索引\n",
    "            end = start + batch_size - num\n",
    "            \n",
    "            # 把当前texts再加上溢出部分的起始结束位置数据，赋值给陇text\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            \n",
    "            # 把当前labels再加上溢出部分的起始位置数据，赋值给陇label\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            \n",
    "            # 结束位置变成下一次的起始位置\n",
    "            start = end\n",
    "        else:\n",
    "        # 如果每一陇存储的索引数量 等于批量尺寸的话：\n",
    "            # texts和labels原封不动，赋值到陇text和陇label中\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "            \n",
    "        # 断言批量尺寸 等于 陇标签尺寸；\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # 根据此批量尺寸，生成有序索引数组\n",
    "        index = list(range(batch_size))\n",
    "        # 索引重新洗牌\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        # 定义洗过牌的陇text和陇label\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        \n",
    "        # 并遍历对洗过牌的陇text和陇label进行赋值\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "        \n",
    "        # 对洗过牌的text和label，一起合并到data中\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        \n",
    "        # 并追加到返回值\n",
    "        fold_data.append(data)\n",
    "        \n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "    # 返回fold_data\n",
    "    return fold_data\n",
    "\n",
    "fold_data = all_data2fold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立训练，开发，测试数据集\n",
    "\n",
    "# 陇ID <-- 9\n",
    "fold_id = 9\n",
    "\n",
    "# 把最后一陇数据作为开发数据\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# 训练数据\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "# 将不是最后一陇的数据作为训练集数据\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "# 合并label和text到训练数据集\n",
    "train_data = {'label': train_labels, 'text': train_texts}\n",
    "\n",
    "# 指定测试文件\n",
    "test_data_file = '../input/test_a.csv'\n",
    "\n",
    "# 使用TAB做分隔符，编码UTF-8读取数据文件\n",
    "f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\n",
    "\n",
    "# 读取所有text数据\n",
    "texts = f['text'].tolist()\n",
    "\n",
    "# 按初始化为0的标签数组，合并text到测试数据集\n",
    "test_data = {'label': [0] * len(texts), 'text': texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "import transformers\n",
    "from collections import Counter\n",
    "from transformers import BasicTokenizer\n",
    "\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        self.word_counter = Counter()\n",
    "\n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        self.label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "\n",
    "    def load_pretrained_embs(self, embfile):\n",
    "        with open(embfile, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            items = lines[0].split()\n",
    "            word_count, embedding_dim = int(items[0]), int(items[1])\n",
    "\n",
    "        index = len(self._id2extword)\n",
    "        embeddings = np.zeros((word_count + index, embedding_dim))\n",
    "        for line in lines[1:]:\n",
    "            values = line.split()\n",
    "            self._id2extword.append(values[0])\n",
    "            vector = np.array(values[1:], dtype='float64')\n",
    "            embeddings[self.unk] += vector\n",
    "            embeddings[index] = vector\n",
    "            index += 1\n",
    "\n",
    "        embeddings[self.unk] = embeddings[self.unk] / word_count\n",
    "        embeddings = embeddings / np.std(embeddings)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._extword2id = reverse(self._id2extword)\n",
    "\n",
    "        assert len(set(self._id2extword)) == len(self._id2extword)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "\n",
    "    def extword2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._extword2id.get(x, self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs, self.unk)\n",
    "\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)\n",
    "\n",
    "\n",
    "vocab = Vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
