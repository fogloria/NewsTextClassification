{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文链接：Hierarchical Attention Networks for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAN (Hierarchical Attention Networks for Document Classification) 是一个针对文本分类任务的层次化 attention 模型。\n",
    "\n",
    "它有两个显著的特点：\n",
    "- 通过\"词-句子-文章\"的层次化结构来表示一篇文本。\n",
    "- 该模型有两个层次的 attention 机制，分别存在于词层次 (word level) 和句子层次 (sentence level)。从而使该模型具有对文本中重要性不同的句子和词的能力给予不同 \"注意力\" 的能力。\n",
    "\n",
    "![](https://pic1.zhimg.com/80/v2-00012154e7f9ece4a98b9eeff6edba38_720w.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词级别的 attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size, dropout):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of words in the vocabulary of the model\n",
    "        :param emb_size: size of word embeddings\n",
    "        :param word_rnn_size: size of (bidirectional) word-level RNN\n",
    "        :param word_rnn_layers: number of layers in word-level RNN\n",
    "        :param word_att_size: size of word-level attention layer\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(WordAttention, self).__init__()\n",
    "\n",
    "        # Embeddings (look-up) layer\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # Bidirectional word-level RNN\n",
    "        self.word_rnn = nn.GRU(emb_size, word_rnn_size, num_layers=word_rnn_layers, bidirectional=True,\n",
    "                               dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Word-level attention network\n",
    "        self.word_attention = nn.Linear(2 * word_rnn_size, word_att_size)\n",
    "\n",
    "        # Word context vector to take dot-product with\n",
    "        self.word_context_vector = nn.Linear(word_att_size, 1, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, sentences, words_per_sentence):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param sentences: encoded sentence-level data, a tensor of dimension (n_sentences, word_pad_len, emb_size)\n",
    "        :param words_per_sentence: sentence lengths, a tensor of dimension (n_sentences)\n",
    "        :return: sentence embeddings, attention weights of words\n",
    "        \"\"\"\n",
    "        # Sort sentences by decreasing sentence lengths (SORTING #2)\n",
    "        words_per_sentence, sent_sort_ind = words_per_sentence.sort(dim=0, descending=True)\n",
    "        sentences = sentences[sent_sort_ind]  # (n_sentences, word_pad_len, emb_size)\n",
    "\n",
    "        # Get word embeddings, apply dropout\n",
    "        sentences = self.dropout(self.embeddings(sentences))  # (n_sentences, word_pad_len, emb_size)\n",
    "\n",
    "        # Re-arrange as words by removing pad-words (SENTENCES -> WORDS)\n",
    "        words, bw = pack_padded_sequence(sentences,\n",
    "                                         lengths=words_per_sentence.tolist(),\n",
    "                                         batch_first=True)\n",
    "        # (n_words, emb_size), bw is the effective batch size at each word-timestep\n",
    "\n",
    "        (words, _), _ = self.word_rnn(PackedSequence(words, bw))  # (n_words, 2 * word_rnn_size), (max(sent_lens))\n",
    "\n",
    "        # Find attention vectors by applying the attention linear layer\n",
    "        att_w = self.word_attention(words)  # (n_words, att_size)\n",
    "        att_w = F.tanh(att_w)  # (n_words, att_size)\n",
    "\n",
    "        # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n",
    "        att_w = self.word_context_vector(att_w).squeeze(1)  # (n_words)\n",
    "\n",
    "        max_value = att_w.max()  \n",
    "        att_w = torch.exp(att_w - max_value)  # (n_words)\n",
    "\n",
    "        # Re-arrange as sentences by re-padding with 0s (WORDS -> SENTENCES)\n",
    "        att_w, _ = pad_packed_sequence(PackedSequence(att_w, bw), batch_first=True)\n",
    "        # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        # Calculate softmax values\n",
    "        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)\n",
    "\n",
    "        # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        # Similarly re-arrange word-level RNN outputs as sentences by re-padding with 0s (WORDS -> SENTENCES)\n",
    "        sentences, _ = pad_packed_sequence(PackedSequence(words, bw), batch_first=True)\n",
    "        # (n_sentences, max_sent_len_in_batch, 2 * word_rnn_size)\n",
    "\n",
    "        # Find sentence embeddings\n",
    "        sentences = sentences * word_alphas.unsqueeze(2)  # (n_sentences, max_sent_len_in_batch, 2 * word_rnn_size)\n",
    "        sentences = sentences.sum(dim=1)  # (n_sentences, 2 * word_rnn_size)\n",
    "\n",
    "        # Unsort sentences into the original order (INVERSE OF SORTING #2)\n",
    "        _, sent_unsort_ind = sent_sort_ind.sort(dim=0, descending=False)  # (n_sentences)\n",
    "        sentences = sentences[sent_unsort_ind]  # (n_sentences, 2 * word_rnn_size)\n",
    "        word_alphas = word_alphas[sent_unsort_ind]  # (n_sentences, max_sent_len_in_batch)\n",
    "\n",
    "        return sentences, word_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 句子级别的 attention\n",
    "\n",
    "和 word level 的 attention 类似，对于 document，也有一个句子级别的上下文向量 [公式]，来衡量一个句子相对于 document 的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The sentence-level attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers, sentence_rnn_layers,\n",
    "                 word_att_size, sentence_att_size, dropout):\n",
    "        \"\"\"\n",
    "        :param vocab_size: number of words in the vocabulary of the model\n",
    "        :param emb_size: size of word embeddings\n",
    "        :param word_rnn_size: size of (bidirectional) word-level RNN\n",
    "        :param sentence_rnn_size: size of (bidirectional) sentence-level RNN\n",
    "        :param word_rnn_layers: number of layers in word-level RNN\n",
    "        :param sentence_rnn_layers: number of layers in sentence-level RNN\n",
    "        :param word_att_size: size of word-level attention layer\n",
    "        :param sentence_att_size: size of sentence-level attention layer\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(SentenceAttention, self).__init__()\n",
    "\n",
    "        # Word-level attention module\n",
    "        self.word_attention = WordAttention(vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size,\n",
    "                                            dropout)\n",
    "\n",
    "        # Bidirectional sentence-level RNN\n",
    "        self.sentence_rnn = nn.GRU(2 * word_rnn_size, sentence_rnn_size, num_layers=sentence_rnn_layers,\n",
    "                                   bidirectional=True, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Sentence-level attention network\n",
    "        self.sentence_attention = nn.Linear(2 * sentence_rnn_size, sentence_att_size)\n",
    "\n",
    "        # Sentence context vector to take dot-product with\n",
    "        self.sentence_context_vector = nn.Linear(sentence_att_size, 1, bias=False)\n",
    "        # this performs a dot product with the linear layer's 1D parameter vector, which is the sentence context vector\n",
    "\n",
    "        # You could also do this with:\n",
    "        # self.sentence_context_vector = nn.Parameter(torch.FloatTensor(1, sentence_att_size))\n",
    "        # self.sentence_context_vector.data.uniform_(-0.1, 0.1)\n",
    "        # And then take the dot-product\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param documents: encoded document-level data, a tensor of dimensions (n_documents, sent_pad_len, word_pad_len)\n",
    "        :param sentences_per_document: document lengths, a tensor of dimensions (n_documents)\n",
    "        :param words_per_sentence: sentence lengths, a tensor of dimensions (n_documents, sent_pad_len)\n",
    "        :return: document embeddings, attention weights of words, attention weights of sentences\n",
    "        \"\"\"\n",
    "        # Sort documents by decreasing document lengths (SORTING #1)\n",
    "        sentences_per_document, doc_sort_ind = sentences_per_document.sort(dim=0, descending=True)\n",
    "        documents = documents[doc_sort_ind]  # (n_documents, sent_pad_len, word_pad_len)\n",
    "        words_per_sentence = words_per_sentence[doc_sort_ind]  # (n_documents, sent_pad_len)\n",
    "\n",
    "        # Re-arrange as sentences by removing pad-sentences (DOCUMENTS -> SENTENCES)\n",
    "        sentences, bs = pack_padded_sequence(documents,\n",
    "                                             lengths=sentences_per_document.tolist(),\n",
    "                                             batch_first=True)\n",
    "        # (n_sentences, word_pad_len), bs is the effective batch size at each sentence-timestep\n",
    "\n",
    "        # Re-arrange sentence lengths in the same way (DOCUMENTS -> SENTENCES)\n",
    "        words_per_sentence, _ = pack_padded_sequence(words_per_sentence,\n",
    "                                                     lengths=sentences_per_document.tolist(),\n",
    "                                                     batch_first=True)\n",
    "        # (n_sentences), '_' is the same as 'bs' in the earlier step\n",
    "\n",
    "        # Find sentence embeddings by applying the word-level attention module\n",
    "        sentences, word_alphas = self.word_attention(sentences, words_per_sentence)\n",
    "\n",
    "        # (n_sentences, 2 * word_rnn_size), (n_sentences, max_sent_len_in_batch)\n",
    "        sentences = self.dropout(sentences)\n",
    "\n",
    "        (sentences, _), _ = self.sentence_rnn(\n",
    "            PackedSequence(sentences, bs))  # (n_sentences, 2 * sentence_rnn_size), (max(sent_lens))\n",
    "\n",
    "        # Find attention vectors by applying the attention linear layer\n",
    "        att_s = self.sentence_attention(sentences)  # (n_sentences, att_size)\n",
    "        att_s = F.tanh(att_s)  # (n_sentences, att_size)\n",
    "        \n",
    "        # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n",
    "        att_s = self.sentence_context_vector(att_s).squeeze(1)  # (n_sentences)\n",
    "\n",
    "        max_value = att_s.max()  # scalar, for numerical stability during exponent calculation\n",
    "        att_s = torch.exp(att_s - max_value)  # (n_sentences)\n",
    "\n",
    "        # Re-arrange as documents by re-padding with 0s (SENTENCES -> DOCUMENTS)\n",
    "        att_s, _ = pad_packed_sequence(PackedSequence(att_s, bs),\n",
    "                                       batch_first=True)  # (n_documents, max_doc_len_in_batch)\n",
    "\n",
    "        # Calculate softmax values\n",
    "        sentence_alphas = att_s / torch.sum(att_s, dim=1, keepdim=True)  # (n_documents, max_doc_len_in_batch)\n",
    "\n",
    "        # Similarly re-arrange sentence-level RNN outputs as documents by re-padding with 0s (SENTENCES -> DOCUMENTS)\n",
    "        documents, _ = pad_packed_sequence(PackedSequence(sentences, bs),\n",
    "                                           batch_first=True)\n",
    "        # (n_documents, max_doc_len_in_batch, 2 * sentence_rnn_size)\n",
    "\n",
    "        # Find document embeddings\n",
    "        documents = documents * sentence_alphas.unsqueeze(\n",
    "            2)  # (n_documents, max_doc_len_in_batch, 2 * sentence_rnn_size)\n",
    "        documents = documents.sum(dim=1)  # (n_documents, 2 * sentence_rnn_size)\n",
    "\n",
    "        # Also re-arrange word_alphas (SENTENCES -> DOCUMENTS)\n",
    "        word_alphas, _ = pad_packed_sequence(PackedSequence(word_alphas, bs),\n",
    "                                             batch_first=True)\n",
    "        # (n_documents, max_doc_len_in_batch, max_sent_len_in_batch)\n",
    "\n",
    "        # Unsort documents into the original order (INVERSE OF SORTING #1)\n",
    "        _, doc_unsort_ind = doc_sort_ind.sort(dim=0, descending=False)  # (n_documents)\n",
    "        documents = documents[doc_unsort_ind]  # (n_documents, 2 * sentence_rnn_size)\n",
    "        sentence_alphas = sentence_alphas[doc_unsort_ind]  # (n_documents, max_doc_len_in_batch)\n",
    "        word_alphas = word_alphas[doc_unsort_ind]  # (n_documents, max_doc_len_in_batch, max_sent_len_in_batch)\n",
    "\n",
    "        return documents, word_alphas, sentence_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAN\n",
    "使用全连接的 softmax 层进行分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers,\n",
    "                 sentence_rnn_layers, word_att_size, sentence_att_size, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param n_classes: number of classes\n",
    "        :param vocab_size: number of words in the vocabulary of the model\n",
    "        :param emb_size: size of word embeddings\n",
    "        :param word_rnn_size: size of (bidirectional) word-level RNN\n",
    "        :param sentence_rnn_size: size of (bidirectional) sentence-level RNN\n",
    "        :param word_rnn_layers: number of layers in word-level RNN\n",
    "        :param sentence_rnn_layers: number of layers in sentence-level RNN\n",
    "        :param word_att_size: size of word-level attention layer\n",
    "        :param sentence_att_size: size of sentence-level attention layer\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.sentence_attention = SentenceAttention(vocab_size, emb_size, word_rnn_size, sentence_rnn_size,\n",
    "                                                    word_rnn_layers, sentence_rnn_layers, word_att_size,\n",
    "                                                    sentence_att_size, dropout)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(2 * sentence_rnn_size, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, documents, sentences_per_document, words_per_sentence):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param documents: encoded document-level data, a tensor of dimensions (n_documents, sent_pad_len, word_pad_len)\n",
    "        :param sentences_per_document: document lengths, a tensor of dimensions (n_documents)\n",
    "        :param words_per_sentence: sentence lengths, a tensor of dimensions (n_documents, sent_pad_len)\n",
    "        :return: class scores, attention weights of words, attention weights of sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # (n_documents, 2 * sentence_rnn_size)\n",
    "        # (n_documents, max_doc_len_in_batch, max_sent_len_in_batch)\n",
    "        # (n_documents, max_doc_len_in_batch)\n",
    "        document_embeddings, word_alphas, sentence_alphas = self.sentence_attention(documents, sentences_per_document, words_per_sentence)  \n",
    "        \n",
    "        # (n_documents, n_classes)\n",
    "        scores = self.fc(self.dropout(document_embeddings))  \n",
    "\n",
    "        return scores, word_alphas, sentence_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/54165155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HAN->SentenceAttention->WordAttention->GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
