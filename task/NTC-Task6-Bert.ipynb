{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\learn\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: numpy in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: six in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 08:35:37,329 INFO: Use cuda: True, gpu id: 0.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import BasicTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to 10 fold\n",
    "\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        self.word_counter = Counter()\n",
    "\n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        self.label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "\n",
    "    def load_pretrained_embs(self, embfile):\n",
    "        with open(embfile, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            items = lines[0].split()\n",
    "            word_count, embedding_dim = int(items[0]), int(items[1])\n",
    "\n",
    "        index = len(self._id2extword)\n",
    "        embeddings = np.zeros((word_count + index, embedding_dim))\n",
    "        for line in lines[1:]:\n",
    "            values = line.split()\n",
    "            self._id2extword.append(values[0])\n",
    "            vector = np.array(values[1:], dtype='float64')\n",
    "            embeddings[self.unk] += vector\n",
    "            embeddings[index] = vector\n",
    "            index += 1\n",
    "\n",
    "        embeddings[self.unk] = embeddings[self.unk] / word_count\n",
    "        embeddings = embeddings / np.std(embeddings)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._extword2id = reverse(self._id2extword)\n",
    "\n",
    "        assert len(set(self._id2extword)) == len(self._id2extword)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "\n",
    "    def extword2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._extword2id.get(x, self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs, self.unk)\n",
    "\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b = np.zeros(hidden_size, dtype=np.float32)\n",
    "        self.bias.data.copy_(torch.from_numpy(b))\n",
    "\n",
    "        self.query = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.query.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "    def forward(self, batch_hidden, batch_masks):\n",
    "        # batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)\n",
    "        # batch_masks:  b x len\n",
    "\n",
    "        # linear\n",
    "        key = torch.matmul(batch_hidden, self.weight) + self.bias  # b x len x hidden\n",
    "\n",
    "        # compute attention\n",
    "        outputs = torch.matmul(key, self.query)  # b x len\n",
    "\n",
    "        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n",
    "\n",
    "        attn_scores = F.softmax(masked_outputs, dim=1)  # b x len\n",
    "\n",
    "        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n",
    "        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n",
    "\n",
    "        # sum weighted sources\n",
    "        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  # b x hidden\n",
    "\n",
    "        return batch_outputs, attn_scores\n",
    "\n",
    "\n",
    "class WordBertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordBertEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.tokenizer = WhitespaceTokenizer()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)\n",
    "\n",
    "        self.pooled = False\n",
    "        logging.info('Build Bert encoder with pooled {}.'.format(self.pooled))\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        tokens = self.tokenizer.tokenize(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def get_bert_parameters(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    #微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        # input_ids: sen_num x bert_len\n",
    "        # token_type_ids: sen_num  x bert_len\n",
    "\n",
    "        # sen_num x bert_len x 256, sen_num x 256\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        if self.pooled:\n",
    "            reps = pooled_output\n",
    "        else:\n",
    "            reps = sequence_output[:, 0, :]  # sen_num x 256\n",
    "\n",
    "        if self.training:\n",
    "            reps = self.dropout(reps)\n",
    "\n",
    "        return reps\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer():\n",
    "    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        vocab_file = bert_path + 'vocab.txt'\n",
    "        self._token2id = self.load_vocab(vocab_file)\n",
    "        self._id2token = {v: k for k, v in self._token2id.items()}\n",
    "        self.max_len = 256\n",
    "        self.unk = 1\n",
    "\n",
    "        logging.info(\"Build Bert vocab with size %d.\" % (self.vocab_size))\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        lines = list(map(lambda x: x.strip(), lines))\n",
    "        vocab = dict(zip(lines, range(len(lines))))\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, tokens):\n",
    "        assert len(tokens) <= self.max_len - 2\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        output_tokens = self.token2id(tokens)\n",
    "        return output_tokens\n",
    "\n",
    "    def token2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._token2id.get(x, self.unk) for x in xs]\n",
    "        return self._token2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._id2token)\n",
    "\n",
    "\n",
    "class SentEncoder(nn.Module):\n",
    "    def __init__(self, sent_rep_size):\n",
    "        super(SentEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sent_lstm = nn.LSTM(\n",
    "            input_size=sent_rep_size,\n",
    "            hidden_size=sent_hidden_size,\n",
    "            num_layers=sent_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, sent_reps, sent_masks):\n",
    "        # sent_reps:  b x doc_len x sent_rep_size\n",
    "        # sent_masks: b x doc_len\n",
    "\n",
    "        sent_hiddens, _ = self.sent_lstm(sent_reps)  # b x doc_len x hidden*2\n",
    "        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            sent_hiddens = self.dropout(sent_hiddens)\n",
    "\n",
    "        return sent_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.sent_rep_size = 256\n",
    "        self.doc_rep_size = sent_hidden_size * 2\n",
    "        self.all_parameters = {}\n",
    "        parameters = []\n",
    "        self.word_encoder = WordBertEncoder()\n",
    "        bert_parameters = self.word_encoder.get_bert_parameters()\n",
    "\n",
    "        self.sent_encoder = SentEncoder(self.sent_rep_size)\n",
    "        self.sent_attention = Attention(self.doc_rep_size)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n",
    "\n",
    "        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n",
    "\n",
    "        if use_cuda:\n",
    "            self.to(device)\n",
    "\n",
    "        if len(parameters) > 0:\n",
    "            self.all_parameters[\"basic_parameters\"] = parameters\n",
    "        self.all_parameters[\"bert_parameters\"] = bert_parameters\n",
    "\n",
    "        logging.info('Build model with bert word encoder, lstm sent encoder.')\n",
    "\n",
    "        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n",
    "        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        # batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len\n",
    "        # batch_masks : b x doc_len x sent_len\n",
    "        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n",
    "        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n",
    "        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "\n",
    "        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  # sen_num x sent_rep_size\n",
    "\n",
    "        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  # b x doc_len x sent_rep_size\n",
    "        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  # b x doc_len x max_sent_len\n",
    "        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n",
    "\n",
    "        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  # b x doc_len x doc_rep_size\n",
    "        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  # b x doc_rep_size\n",
    "\n",
    "        batch_outputs = self.out(doc_reps)  # b x num_labels\n",
    "\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, model_parameters, steps):\n",
    "        self.all_params = []\n",
    "        self.optims = []\n",
    "        self.schedulers = []\n",
    "\n",
    "        for name, parameters in model_parameters.items():\n",
    "            if name.startswith(\"basic\"):\n",
    "                optim = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "                self.optims.append(optim)\n",
    "\n",
    "                l = lambda step: decay ** (step // decay_step)\n",
    "                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n",
    "                self.schedulers.append(scheduler)\n",
    "                self.all_params.extend(parameters)\n",
    "            elif name.startswith(\"bert\"):\n",
    "                optim_bert = AdamW(parameters, bert_lr, eps=1e-8)\n",
    "                self.optims.append(optim_bert)\n",
    "\n",
    "                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, 0, steps)\n",
    "                self.schedulers.append(scheduler_bert)\n",
    "\n",
    "                for group in parameters:\n",
    "                    for p in group['params']:\n",
    "                        self.all_params.append(p)\n",
    "            else:\n",
    "                Exception(\"no nameed parameters.\")\n",
    "\n",
    "        self.num = len(self.optims)\n",
    "\n",
    "    def step(self):\n",
    "        for optim, scheduler in zip(self.optims, self.schedulers):\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for optim in self.optims:\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n",
    "        lr = ' %.5f' * self.num\n",
    "        res = lr % lrs\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "def sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n",
    "    words = text.strip().split()\n",
    "    document_len = len(words)\n",
    "\n",
    "    index = list(range(0, document_len, max_sent_len))\n",
    "    index.append(document_len)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(index) - 1):\n",
    "        segment = words[index[i]: index[i + 1]]\n",
    "        assert len(segment) > 0\n",
    "        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n",
    "        segments.append([len(segment), segment])\n",
    "\n",
    "    assert len(segments) > 0\n",
    "    if len(segments) > max_segment:\n",
    "        segment_ = int(max_segment / 2)\n",
    "        return segments[:segment_] + segments[-segment_:]\n",
    "    else:\n",
    "        return segments\n",
    "\n",
    "\n",
    "def get_examples(data, word_encoder, vocab, max_sent_len=256, max_segment=8):\n",
    "    label2id = vocab.label2id\n",
    "    examples = []\n",
    "\n",
    "    for text, label in zip(data['text'], data['label']):\n",
    "        # label\n",
    "        id = label2id(label)\n",
    "\n",
    "        # words\n",
    "        sents_words = sentence_split(text, vocab, max_sent_len-2, max_segment)\n",
    "        doc = []\n",
    "        for sent_len, sent_words in sents_words:\n",
    "            token_ids = word_encoder.encode(sent_words)\n",
    "            sent_len = len(token_ids)\n",
    "            token_type_ids = [0] * sent_len\n",
    "            doc.append([sent_len, token_ids, token_type_ids])\n",
    "        examples.append([id, len(doc), doc])\n",
    "\n",
    "    logging.info('Total %d docs.' % len(examples))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loader\n",
    "\n",
    "def batch_slice(data, batch_size):\n",
    "    batch_num = int(np.ceil(len(data) / float(batch_size)))\n",
    "    for i in range(batch_num):\n",
    "        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n",
    "        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "\n",
    "        yield docs\n",
    "\n",
    "\n",
    "def data_iter(data, batch_size, shuffle=True, noise=1.0):\n",
    "    \"\"\"\n",
    "    randomly permute data, then sort by source length, and partition into batches\n",
    "    ensure that the length of  sentences in each batch\n",
    "    \"\"\"\n",
    "\n",
    "    batched_data = []\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        lengths = [example[1] for example in data]\n",
    "        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n",
    "        sorted_indices = np.argsort(noisy_lengths).tolist()\n",
    "        sorted_data = [data[i] for i in sorted_indices]\n",
    "    else:\n",
    "        sorted_data =data\n",
    "        \n",
    "    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(batched_data)\n",
    "\n",
    "    for batch in batched_data:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function\n",
    "\n",
    "def get_score(y_ture, y_pred):\n",
    "    y_ture = np.array(y_ture)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n",
    "    p = precision_score(y_ture, y_pred, average='macro') * 100\n",
    "    r = recall_score(y_ture, y_pred, average='macro') * 100\n",
    "\n",
    "    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n",
    "\n",
    "\n",
    "def reformat(num, n):\n",
    "    return float(format(num, '0.' + str(n) + 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build trainer\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.report = True\n",
    "        \n",
    "        self.train_data = get_examples(train_data, model.word_encoder, vocab)\n",
    "        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n",
    "        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n",
    "        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # label name\n",
    "        self.target_names = vocab.target_names\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n",
    "\n",
    "        # count\n",
    "        self.step = 0\n",
    "        self.early_stop = -1\n",
    "        self.best_train_f1, self.best_dev_f1 = 0, 0\n",
    "        self.last_epoch = epochs\n",
    "\n",
    "    def train(self):\n",
    "        logging.info('Start training...')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_f1 = self._train(epoch)\n",
    "\n",
    "            dev_f1 = self._eval(epoch)\n",
    "\n",
    "            if self.best_dev_f1 <= dev_f1:\n",
    "                logging.info(\n",
    "                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n",
    "                torch.save(self.model.state_dict(), save_model)\n",
    "\n",
    "                self.best_train_f1 = train_f1\n",
    "                self.best_dev_f1 = dev_f1\n",
    "                self.early_stop = 0\n",
    "            else:\n",
    "                self.early_stop += 1\n",
    "                if self.early_stop == early_stops:\n",
    "                    logging.info(\n",
    "                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n",
    "                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n",
    "                    self.last_epoch = epoch\n",
    "                    break\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load(save_model))\n",
    "        self._eval(self.last_epoch + 1, test=True)\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "        overall_losses = 0\n",
    "        losses = 0\n",
    "        batch_idx = 1\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "            batch_outputs = self.model(batch_inputs)\n",
    "            loss = self.criterion(batch_outputs, batch_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "\n",
    "            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n",
    "            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                lrs = self.optimizer.get_lr()\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, self.step, batch_idx, self.batch_num, lrs,\n",
    "                        losses / log_interval,\n",
    "                        elapsed / log_interval))\n",
    "\n",
    "                losses = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        overall_losses /= self.batch_num\n",
    "        during_time = time.time() - epoch_start_time\n",
    "\n",
    "        # reformat\n",
    "        overall_losses = reformat(overall_losses, 4)\n",
    "        score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "        logging.info(\n",
    "            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  overall_losses,\n",
    "                                                                                  during_time))\n",
    "        if set(y_true) == set(y_pred) and self.report:\n",
    "            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "            logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def _eval(self, epoch, test=False):\n",
    "        self.model.eval()\n",
    "        start_time = time.time()\n",
    "        data = self.test_data if test else self.dev_data\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "                batch_outputs = self.model(batch_inputs)\n",
    "                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "                y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "            during_time = time.time() - start_time\n",
    "            \n",
    "            if test:\n",
    "                df = pd.DataFrame({'label': y_pred})\n",
    "                df.to_csv(save_test, index=False, sep=',')\n",
    "            else:\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                              during_time))\n",
    "                if set(y_true) == set(y_pred) and self.report:\n",
    "                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "                    logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def batch2tensor(self, batch_data):\n",
    "        '''\n",
    "            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n",
    "        '''\n",
    "        batch_size = len(batch_data)\n",
    "        doc_labels = []\n",
    "        doc_lens = []\n",
    "        doc_max_sent_len = []\n",
    "        for doc_data in batch_data:\n",
    "            doc_labels.append(doc_data[0])\n",
    "            doc_lens.append(doc_data[1])\n",
    "            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n",
    "            max_sent_len = max(sent_lens)\n",
    "            doc_max_sent_len.append(max_sent_len)\n",
    "\n",
    "        max_doc_len = max(doc_lens)\n",
    "        max_sent_len = max(doc_max_sent_len)\n",
    "\n",
    "        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n",
    "        batch_labels = torch.LongTensor(doc_labels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for sent_idx in range(doc_lens[b]):\n",
    "                sent_data = batch_data[b][2][sent_idx]\n",
    "                for word_idx in range(sent_data[0]):\n",
    "                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n",
    "                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n",
    "                    batch_masks[b, sent_idx, word_idx] = 1\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_inputs1 = batch_inputs1.to(device)\n",
    "            batch_inputs2 = batch_inputs2.to(device)\n",
    "            batch_masks = batch_masks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_id = 9\n",
    "fold_num = 10\n",
    "data_file = '../input/train_set.csv'\n",
    "test_data_file = '../input/test_a.csv'\n",
    "\n",
    "save_model = '../output/bert20200806.bin'\n",
    "save_test = '../output/bertNext.csv'\n",
    "\n",
    "# build word encoder\n",
    "bert_path = '../emb/bert-mini/'\n",
    "\n",
    "# build sent encoder\n",
    "sent_hidden_size = 256\n",
    "sent_num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 08:36:24,352 INFO: Fold lens [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n"
     ]
    }
   ],
   "source": [
    "fold_data = all_data2fold(10,200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build train, dev, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# train\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "train_data = {'label': train_labels, 'text': train_texts}\n",
    "\n",
    "# test\n",
    "f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\n",
    "texts = f['text'].tolist()\n",
    "test_data = {'label': [0] * len(texts), 'text': texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 08:37:17,457 INFO: Build vocab: words 5983, labels 14.\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 08:37:17,473 INFO: Build Bert vocab with size 5981.\n",
      "2020-08-08 08:37:17,474 INFO: loading configuration file ../emb/bert-mini/config.json\n",
      "2020-08-08 08:37:17,481 INFO: Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 5981\n",
      "}\n",
      "\n",
      "2020-08-08 08:37:17,482 INFO: loading weights file ../emb/bert-mini/pytorch_model.bin\n",
      "2020-08-08 08:37:17,570 INFO: All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "2020-08-08 08:37:17,571 INFO: All the weights of BertModel were initialized from the model checkpoint at ../emb/bert-mini/.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "2020-08-08 08:37:17,573 INFO: Build Bert encoder with pooled False.\n",
      "2020-08-08 08:37:17,637 INFO: Build model with bert word encoder, lstm sent encoder.\n",
      "2020-08-08 08:37:17,641 INFO: Model param num: 7.72 M.\n"
     ]
    }
   ],
   "source": [
    "model = Model(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "learning_rate = 2e-4\n",
    "dropout = 0.15\n",
    "bert_lr = 5e-5\n",
    "decay = .75\n",
    "decay_step = 1000\n",
    "clip = 5.0\n",
    "epochs = 4\n",
    "early_stops = 3\n",
    "log_interval = 50\n",
    "test_batch_size = 16\n",
    "train_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 08:53:56,859 INFO: Total 180000 docs.\n",
      "2020-08-08 08:55:41,583 INFO: Total 20000 docs.\n",
      "2020-08-08 09:00:04,540 INFO: Total 50000 docs.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer = Trainer(model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标使loss尽量最小之后，再进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 09:19:00,651 INFO: Start training...\n",
      "2020-08-08 09:19:36,874 INFO: | epoch   1 | step  50 | batch  50/11250 | lr 0.00020 0.00005 | loss 2.2175 | s/batch 0.72\n",
      "2020-08-08 09:20:11,199 INFO: | epoch   1 | step 100 | batch 100/11250 | lr 0.00020 0.00005 | loss 1.4541 | s/batch 0.69\n",
      "2020-08-08 09:20:48,323 INFO: | epoch   1 | step 150 | batch 150/11250 | lr 0.00020 0.00005 | loss 0.9852 | s/batch 0.74\n",
      "2020-08-08 09:21:19,674 INFO: | epoch   1 | step 200 | batch 200/11250 | lr 0.00020 0.00005 | loss 0.8033 | s/batch 0.63\n",
      "2020-08-08 09:21:54,750 INFO: | epoch   1 | step 250 | batch 250/11250 | lr 0.00020 0.00005 | loss 0.7683 | s/batch 0.70\n",
      "2020-08-08 09:22:29,364 INFO: | epoch   1 | step 300 | batch 300/11250 | lr 0.00020 0.00005 | loss 0.6919 | s/batch 0.69\n",
      "2020-08-08 09:23:01,724 INFO: | epoch   1 | step 350 | batch 350/11250 | lr 0.00020 0.00005 | loss 0.6007 | s/batch 0.65\n",
      "2020-08-08 09:23:34,797 INFO: | epoch   1 | step 400 | batch 400/11250 | lr 0.00020 0.00005 | loss 0.5645 | s/batch 0.66\n",
      "2020-08-08 09:24:05,353 INFO: | epoch   1 | step 450 | batch 450/11250 | lr 0.00020 0.00005 | loss 0.5397 | s/batch 0.61\n",
      "2020-08-08 09:24:35,433 INFO: | epoch   1 | step 500 | batch 500/11250 | lr 0.00020 0.00005 | loss 0.5562 | s/batch 0.60\n",
      "2020-08-08 09:25:09,517 INFO: | epoch   1 | step 550 | batch 550/11250 | lr 0.00020 0.00005 | loss 0.5611 | s/batch 0.68\n",
      "2020-08-08 09:25:43,284 INFO: | epoch   1 | step 600 | batch 600/11250 | lr 0.00020 0.00005 | loss 0.4912 | s/batch 0.68\n",
      "2020-08-08 09:26:11,698 INFO: | epoch   1 | step 650 | batch 650/11250 | lr 0.00020 0.00005 | loss 0.4331 | s/batch 0.57\n",
      "2020-08-08 09:26:44,445 INFO: | epoch   1 | step 700 | batch 700/11250 | lr 0.00020 0.00005 | loss 0.4469 | s/batch 0.65\n",
      "2020-08-08 09:27:16,075 INFO: | epoch   1 | step 750 | batch 750/11250 | lr 0.00020 0.00005 | loss 0.5146 | s/batch 0.63\n",
      "2020-08-08 09:27:48,127 INFO: | epoch   1 | step 800 | batch 800/11250 | lr 0.00020 0.00005 | loss 0.3929 | s/batch 0.64\n",
      "2020-08-08 09:28:26,247 INFO: | epoch   1 | step 850 | batch 850/11250 | lr 0.00020 0.00005 | loss 0.4967 | s/batch 0.76\n",
      "2020-08-08 09:28:58,241 INFO: | epoch   1 | step 900 | batch 900/11250 | lr 0.00020 0.00005 | loss 0.4073 | s/batch 0.64\n",
      "2020-08-08 09:29:28,967 INFO: | epoch   1 | step 950 | batch 950/11250 | lr 0.00020 0.00005 | loss 0.4167 | s/batch 0.61\n",
      "2020-08-08 09:30:00,068 INFO: | epoch   1 | step 1000 | batch 1000/11250 | lr 0.00015 0.00005 | loss 0.4367 | s/batch 0.62\n",
      "2020-08-08 09:30:38,646 INFO: | epoch   1 | step 1050 | batch 1050/11250 | lr 0.00015 0.00005 | loss 0.4326 | s/batch 0.77\n",
      "2020-08-08 09:31:09,070 INFO: | epoch   1 | step 1100 | batch 1100/11250 | lr 0.00015 0.00005 | loss 0.3787 | s/batch 0.61\n",
      "2020-08-08 09:31:38,336 INFO: | epoch   1 | step 1150 | batch 1150/11250 | lr 0.00015 0.00005 | loss 0.3497 | s/batch 0.59\n",
      "2020-08-08 09:32:14,316 INFO: | epoch   1 | step 1200 | batch 1200/11250 | lr 0.00015 0.00005 | loss 0.3579 | s/batch 0.72\n",
      "2020-08-08 09:32:49,752 INFO: | epoch   1 | step 1250 | batch 1250/11250 | lr 0.00015 0.00005 | loss 0.3929 | s/batch 0.71\n",
      "2020-08-08 09:33:22,896 INFO: | epoch   1 | step 1300 | batch 1300/11250 | lr 0.00015 0.00005 | loss 0.3863 | s/batch 0.66\n",
      "2020-08-08 09:33:56,215 INFO: | epoch   1 | step 1350 | batch 1350/11250 | lr 0.00015 0.00005 | loss 0.3319 | s/batch 0.67\n",
      "2020-08-08 09:34:29,930 INFO: | epoch   1 | step 1400 | batch 1400/11250 | lr 0.00015 0.00005 | loss 0.3540 | s/batch 0.67\n",
      "2020-08-08 09:35:00,577 INFO: | epoch   1 | step 1450 | batch 1450/11250 | lr 0.00015 0.00005 | loss 0.3798 | s/batch 0.61\n",
      "2020-08-08 09:35:34,478 INFO: | epoch   1 | step 1500 | batch 1500/11250 | lr 0.00015 0.00005 | loss 0.3673 | s/batch 0.68\n",
      "2020-08-08 09:36:11,050 INFO: | epoch   1 | step 1550 | batch 1550/11250 | lr 0.00015 0.00005 | loss 0.3530 | s/batch 0.73\n",
      "2020-08-08 09:36:44,425 INFO: | epoch   1 | step 1600 | batch 1600/11250 | lr 0.00015 0.00005 | loss 0.3418 | s/batch 0.67\n",
      "2020-08-08 09:37:18,834 INFO: | epoch   1 | step 1650 | batch 1650/11250 | lr 0.00015 0.00005 | loss 0.3620 | s/batch 0.69\n",
      "2020-08-08 09:37:55,031 INFO: | epoch   1 | step 1700 | batch 1700/11250 | lr 0.00015 0.00005 | loss 0.3297 | s/batch 0.72\n",
      "2020-08-08 09:38:24,988 INFO: | epoch   1 | step 1750 | batch 1750/11250 | lr 0.00015 0.00005 | loss 0.2784 | s/batch 0.60\n",
      "2020-08-08 09:38:56,602 INFO: | epoch   1 | step 1800 | batch 1800/11250 | lr 0.00015 0.00005 | loss 0.3847 | s/batch 0.63\n",
      "2020-08-08 09:39:27,197 INFO: | epoch   1 | step 1850 | batch 1850/11250 | lr 0.00015 0.00005 | loss 0.3259 | s/batch 0.61\n",
      "2020-08-08 09:39:55,363 INFO: | epoch   1 | step 1900 | batch 1900/11250 | lr 0.00015 0.00005 | loss 0.3311 | s/batch 0.56\n",
      "2020-08-08 09:40:29,510 INFO: | epoch   1 | step 1950 | batch 1950/11250 | lr 0.00015 0.00005 | loss 0.3226 | s/batch 0.68\n",
      "2020-08-08 09:41:01,101 INFO: | epoch   1 | step 2000 | batch 2000/11250 | lr 0.00011 0.00005 | loss 0.3464 | s/batch 0.63\n",
      "2020-08-08 09:41:36,126 INFO: | epoch   1 | step 2050 | batch 2050/11250 | lr 0.00011 0.00005 | loss 0.3708 | s/batch 0.70\n",
      "2020-08-08 09:42:12,363 INFO: | epoch   1 | step 2100 | batch 2100/11250 | lr 0.00011 0.00005 | loss 0.3097 | s/batch 0.72\n",
      "2020-08-08 09:42:44,781 INFO: | epoch   1 | step 2150 | batch 2150/11250 | lr 0.00011 0.00005 | loss 0.3428 | s/batch 0.65\n",
      "2020-08-08 09:43:16,669 INFO: | epoch   1 | step 2200 | batch 2200/11250 | lr 0.00011 0.00005 | loss 0.3647 | s/batch 0.64\n",
      "2020-08-08 09:43:45,909 INFO: | epoch   1 | step 2250 | batch 2250/11250 | lr 0.00011 0.00005 | loss 0.3612 | s/batch 0.58\n",
      "2020-08-08 09:44:23,017 INFO: | epoch   1 | step 2300 | batch 2300/11250 | lr 0.00011 0.00005 | loss 0.3342 | s/batch 0.74\n",
      "2020-08-08 09:44:54,322 INFO: | epoch   1 | step 2350 | batch 2350/11250 | lr 0.00011 0.00005 | loss 0.3581 | s/batch 0.63\n",
      "2020-08-08 09:45:26,457 INFO: | epoch   1 | step 2400 | batch 2400/11250 | lr 0.00011 0.00005 | loss 0.3810 | s/batch 0.64\n",
      "2020-08-08 09:46:01,454 INFO: | epoch   1 | step 2450 | batch 2450/11250 | lr 0.00011 0.00005 | loss 0.2718 | s/batch 0.70\n",
      "2020-08-08 09:46:35,192 INFO: | epoch   1 | step 2500 | batch 2500/11250 | lr 0.00011 0.00005 | loss 0.3312 | s/batch 0.67\n",
      "2020-08-08 09:47:07,699 INFO: | epoch   1 | step 2550 | batch 2550/11250 | lr 0.00011 0.00005 | loss 0.3365 | s/batch 0.65\n",
      "2020-08-08 09:47:39,324 INFO: | epoch   1 | step 2600 | batch 2600/11250 | lr 0.00011 0.00005 | loss 0.3355 | s/batch 0.63\n",
      "2020-08-08 09:48:13,277 INFO: | epoch   1 | step 2650 | batch 2650/11250 | lr 0.00011 0.00005 | loss 0.2907 | s/batch 0.68\n",
      "2020-08-08 09:48:45,425 INFO: | epoch   1 | step 2700 | batch 2700/11250 | lr 0.00011 0.00005 | loss 0.2814 | s/batch 0.64\n",
      "2020-08-08 09:49:19,727 INFO: | epoch   1 | step 2750 | batch 2750/11250 | lr 0.00011 0.00005 | loss 0.3414 | s/batch 0.69\n",
      "2020-08-08 09:49:51,975 INFO: | epoch   1 | step 2800 | batch 2800/11250 | lr 0.00011 0.00005 | loss 0.2664 | s/batch 0.64\n",
      "2020-08-08 09:50:28,208 INFO: | epoch   1 | step 2850 | batch 2850/11250 | lr 0.00011 0.00005 | loss 0.3256 | s/batch 0.72\n",
      "2020-08-08 09:51:05,449 INFO: | epoch   1 | step 2900 | batch 2900/11250 | lr 0.00011 0.00005 | loss 0.3511 | s/batch 0.74\n",
      "2020-08-08 09:51:38,153 INFO: | epoch   1 | step 2950 | batch 2950/11250 | lr 0.00011 0.00005 | loss 0.2558 | s/batch 0.65\n",
      "2020-08-08 09:52:13,031 INFO: | epoch   1 | step 3000 | batch 3000/11250 | lr 0.00008 0.00005 | loss 0.3107 | s/batch 0.70\n",
      "2020-08-08 09:52:48,259 INFO: | epoch   1 | step 3050 | batch 3050/11250 | lr 0.00008 0.00005 | loss 0.3619 | s/batch 0.70\n",
      "2020-08-08 09:53:19,931 INFO: | epoch   1 | step 3100 | batch 3100/11250 | lr 0.00008 0.00005 | loss 0.2635 | s/batch 0.63\n",
      "2020-08-08 09:53:53,514 INFO: | epoch   1 | step 3150 | batch 3150/11250 | lr 0.00008 0.00005 | loss 0.2676 | s/batch 0.67\n",
      "2020-08-08 09:54:24,247 INFO: | epoch   1 | step 3200 | batch 3200/11250 | lr 0.00008 0.00005 | loss 0.3303 | s/batch 0.61\n",
      "2020-08-08 09:55:03,372 INFO: | epoch   1 | step 3250 | batch 3250/11250 | lr 0.00008 0.00005 | loss 0.3183 | s/batch 0.78\n",
      "2020-08-08 09:55:38,293 INFO: | epoch   1 | step 3300 | batch 3300/11250 | lr 0.00008 0.00005 | loss 0.2431 | s/batch 0.70\n",
      "2020-08-08 09:56:15,024 INFO: | epoch   1 | step 3350 | batch 3350/11250 | lr 0.00008 0.00005 | loss 0.2665 | s/batch 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 09:56:51,800 INFO: | epoch   1 | step 3400 | batch 3400/11250 | lr 0.00008 0.00005 | loss 0.3035 | s/batch 0.74\n",
      "2020-08-08 09:57:22,568 INFO: | epoch   1 | step 3450 | batch 3450/11250 | lr 0.00008 0.00005 | loss 0.3224 | s/batch 0.62\n",
      "2020-08-08 09:57:56,169 INFO: | epoch   1 | step 3500 | batch 3500/11250 | lr 0.00008 0.00005 | loss 0.3223 | s/batch 0.67\n",
      "2020-08-08 09:58:29,362 INFO: | epoch   1 | step 3550 | batch 3550/11250 | lr 0.00008 0.00005 | loss 0.2747 | s/batch 0.66\n",
      "2020-08-08 09:59:02,281 INFO: | epoch   1 | step 3600 | batch 3600/11250 | lr 0.00008 0.00005 | loss 0.2617 | s/batch 0.66\n",
      "2020-08-08 09:59:39,095 INFO: | epoch   1 | step 3650 | batch 3650/11250 | lr 0.00008 0.00005 | loss 0.3114 | s/batch 0.74\n",
      "2020-08-08 10:00:11,406 INFO: | epoch   1 | step 3700 | batch 3700/11250 | lr 0.00008 0.00005 | loss 0.3060 | s/batch 0.65\n",
      "2020-08-08 10:00:48,248 INFO: | epoch   1 | step 3750 | batch 3750/11250 | lr 0.00008 0.00005 | loss 0.2061 | s/batch 0.74\n",
      "2020-08-08 10:01:24,895 INFO: | epoch   1 | step 3800 | batch 3800/11250 | lr 0.00008 0.00005 | loss 0.2787 | s/batch 0.73\n",
      "2020-08-08 10:02:01,696 INFO: | epoch   1 | step 3850 | batch 3850/11250 | lr 0.00008 0.00005 | loss 0.2895 | s/batch 0.74\n",
      "2020-08-08 10:02:32,091 INFO: | epoch   1 | step 3900 | batch 3900/11250 | lr 0.00008 0.00005 | loss 0.2752 | s/batch 0.61\n",
      "2020-08-08 10:03:09,521 INFO: | epoch   1 | step 3950 | batch 3950/11250 | lr 0.00008 0.00005 | loss 0.2088 | s/batch 0.75\n",
      "2020-08-08 10:03:46,104 INFO: | epoch   1 | step 4000 | batch 4000/11250 | lr 0.00006 0.00005 | loss 0.3431 | s/batch 0.73\n",
      "2020-08-08 10:04:19,590 INFO: | epoch   1 | step 4050 | batch 4050/11250 | lr 0.00006 0.00005 | loss 0.2957 | s/batch 0.67\n",
      "2020-08-08 10:04:54,527 INFO: | epoch   1 | step 4100 | batch 4100/11250 | lr 0.00006 0.00005 | loss 0.2633 | s/batch 0.70\n",
      "2020-08-08 10:05:27,056 INFO: | epoch   1 | step 4150 | batch 4150/11250 | lr 0.00006 0.00005 | loss 0.2248 | s/batch 0.65\n",
      "2020-08-08 10:06:02,258 INFO: | epoch   1 | step 4200 | batch 4200/11250 | lr 0.00006 0.00005 | loss 0.2238 | s/batch 0.70\n",
      "2020-08-08 10:06:37,110 INFO: | epoch   1 | step 4250 | batch 4250/11250 | lr 0.00006 0.00005 | loss 0.2802 | s/batch 0.70\n",
      "2020-08-08 10:07:05,851 INFO: | epoch   1 | step 4300 | batch 4300/11250 | lr 0.00006 0.00005 | loss 0.2811 | s/batch 0.57\n",
      "2020-08-08 10:07:39,449 INFO: | epoch   1 | step 4350 | batch 4350/11250 | lr 0.00006 0.00005 | loss 0.2372 | s/batch 0.67\n",
      "2020-08-08 10:08:14,165 INFO: | epoch   1 | step 4400 | batch 4400/11250 | lr 0.00006 0.00005 | loss 0.2584 | s/batch 0.69\n",
      "2020-08-08 10:08:49,098 INFO: | epoch   1 | step 4450 | batch 4450/11250 | lr 0.00006 0.00005 | loss 0.2349 | s/batch 0.70\n",
      "2020-08-08 10:09:26,416 INFO: | epoch   1 | step 4500 | batch 4500/11250 | lr 0.00006 0.00005 | loss 0.2596 | s/batch 0.75\n",
      "2020-08-08 10:10:00,734 INFO: | epoch   1 | step 4550 | batch 4550/11250 | lr 0.00006 0.00004 | loss 0.2539 | s/batch 0.69\n",
      "2020-08-08 10:10:38,317 INFO: | epoch   1 | step 4600 | batch 4600/11250 | lr 0.00006 0.00004 | loss 0.2682 | s/batch 0.75\n",
      "2020-08-08 10:11:11,086 INFO: | epoch   1 | step 4650 | batch 4650/11250 | lr 0.00006 0.00004 | loss 0.2882 | s/batch 0.66\n",
      "2020-08-08 10:11:41,043 INFO: | epoch   1 | step 4700 | batch 4700/11250 | lr 0.00006 0.00004 | loss 0.2581 | s/batch 0.60\n",
      "2020-08-08 10:12:14,925 INFO: | epoch   1 | step 4750 | batch 4750/11250 | lr 0.00006 0.00004 | loss 0.2594 | s/batch 0.68\n",
      "2020-08-08 10:12:47,826 INFO: | epoch   1 | step 4800 | batch 4800/11250 | lr 0.00006 0.00004 | loss 0.2307 | s/batch 0.66\n",
      "2020-08-08 10:13:21,519 INFO: | epoch   1 | step 4850 | batch 4850/11250 | lr 0.00006 0.00004 | loss 0.2392 | s/batch 0.67\n",
      "2020-08-08 10:13:54,312 INFO: | epoch   1 | step 4900 | batch 4900/11250 | lr 0.00006 0.00004 | loss 0.2138 | s/batch 0.66\n",
      "2020-08-08 10:14:28,315 INFO: | epoch   1 | step 4950 | batch 4950/11250 | lr 0.00006 0.00004 | loss 0.2536 | s/batch 0.68\n",
      "2020-08-08 10:15:03,874 INFO: | epoch   1 | step 5000 | batch 5000/11250 | lr 0.00005 0.00004 | loss 0.2506 | s/batch 0.71\n",
      "2020-08-08 10:15:38,874 INFO: | epoch   1 | step 5050 | batch 5050/11250 | lr 0.00005 0.00004 | loss 0.2067 | s/batch 0.70\n",
      "2020-08-08 10:16:16,329 INFO: | epoch   1 | step 5100 | batch 5100/11250 | lr 0.00005 0.00004 | loss 0.2286 | s/batch 0.75\n",
      "2020-08-08 10:16:53,241 INFO: | epoch   1 | step 5150 | batch 5150/11250 | lr 0.00005 0.00004 | loss 0.2603 | s/batch 0.74\n",
      "2020-08-08 10:17:29,280 INFO: | epoch   1 | step 5200 | batch 5200/11250 | lr 0.00005 0.00004 | loss 0.2564 | s/batch 0.72\n",
      "2020-08-08 10:18:02,169 INFO: | epoch   1 | step 5250 | batch 5250/11250 | lr 0.00005 0.00004 | loss 0.2153 | s/batch 0.66\n",
      "2020-08-08 10:18:40,585 INFO: | epoch   1 | step 5300 | batch 5300/11250 | lr 0.00005 0.00004 | loss 0.2870 | s/batch 0.77\n",
      "2020-08-08 10:19:15,219 INFO: | epoch   1 | step 5350 | batch 5350/11250 | lr 0.00005 0.00004 | loss 0.2621 | s/batch 0.69\n",
      "2020-08-08 10:19:47,380 INFO: | epoch   1 | step 5400 | batch 5400/11250 | lr 0.00005 0.00004 | loss 0.2211 | s/batch 0.64\n",
      "2020-08-08 10:20:22,329 INFO: | epoch   1 | step 5450 | batch 5450/11250 | lr 0.00005 0.00004 | loss 0.2542 | s/batch 0.70\n",
      "2020-08-08 10:20:58,888 INFO: | epoch   1 | step 5500 | batch 5500/11250 | lr 0.00005 0.00004 | loss 0.2148 | s/batch 0.73\n",
      "2020-08-08 10:21:32,291 INFO: | epoch   1 | step 5550 | batch 5550/11250 | lr 0.00005 0.00004 | loss 0.1909 | s/batch 0.67\n",
      "2020-08-08 10:22:08,503 INFO: | epoch   1 | step 5600 | batch 5600/11250 | lr 0.00005 0.00004 | loss 0.2811 | s/batch 0.72\n",
      "2020-08-08 10:22:40,678 INFO: | epoch   1 | step 5650 | batch 5650/11250 | lr 0.00005 0.00004 | loss 0.2784 | s/batch 0.64\n",
      "2020-08-08 10:23:15,068 INFO: | epoch   1 | step 5700 | batch 5700/11250 | lr 0.00005 0.00004 | loss 0.2604 | s/batch 0.69\n",
      "2020-08-08 10:23:50,961 INFO: | epoch   1 | step 5750 | batch 5750/11250 | lr 0.00005 0.00004 | loss 0.2600 | s/batch 0.72\n",
      "2020-08-08 10:24:26,827 INFO: | epoch   1 | step 5800 | batch 5800/11250 | lr 0.00005 0.00004 | loss 0.2117 | s/batch 0.72\n",
      "2020-08-08 10:25:01,854 INFO: | epoch   1 | step 5850 | batch 5850/11250 | lr 0.00005 0.00004 | loss 0.2750 | s/batch 0.70\n",
      "2020-08-08 10:25:31,371 INFO: | epoch   1 | step 5900 | batch 5900/11250 | lr 0.00005 0.00004 | loss 0.2283 | s/batch 0.59\n",
      "2020-08-08 10:26:04,974 INFO: | epoch   1 | step 5950 | batch 5950/11250 | lr 0.00005 0.00004 | loss 0.1751 | s/batch 0.67\n",
      "2020-08-08 10:26:37,292 INFO: | epoch   1 | step 6000 | batch 6000/11250 | lr 0.00004 0.00004 | loss 0.2495 | s/batch 0.65\n",
      "2020-08-08 10:27:10,829 INFO: | epoch   1 | step 6050 | batch 6050/11250 | lr 0.00004 0.00004 | loss 0.2037 | s/batch 0.67\n",
      "2020-08-08 10:27:42,275 INFO: | epoch   1 | step 6100 | batch 6100/11250 | lr 0.00004 0.00004 | loss 0.2370 | s/batch 0.63\n",
      "2020-08-08 10:28:10,911 INFO: | epoch   1 | step 6150 | batch 6150/11250 | lr 0.00004 0.00004 | loss 0.2583 | s/batch 0.57\n",
      "2020-08-08 10:28:44,397 INFO: | epoch   1 | step 6200 | batch 6200/11250 | lr 0.00004 0.00004 | loss 0.2344 | s/batch 0.67\n",
      "2020-08-08 10:29:15,040 INFO: | epoch   1 | step 6250 | batch 6250/11250 | lr 0.00004 0.00004 | loss 0.2529 | s/batch 0.61\n",
      "2020-08-08 10:29:48,553 INFO: | epoch   1 | step 6300 | batch 6300/11250 | lr 0.00004 0.00004 | loss 0.2369 | s/batch 0.67\n",
      "2020-08-08 10:30:28,118 INFO: | epoch   1 | step 6350 | batch 6350/11250 | lr 0.00004 0.00004 | loss 0.2224 | s/batch 0.79\n",
      "2020-08-08 10:31:07,868 INFO: | epoch   1 | step 6400 | batch 6400/11250 | lr 0.00004 0.00004 | loss 0.2723 | s/batch 0.79\n",
      "2020-08-08 10:31:40,224 INFO: | epoch   1 | step 6450 | batch 6450/11250 | lr 0.00004 0.00004 | loss 0.2274 | s/batch 0.65\n",
      "2020-08-08 10:32:15,720 INFO: | epoch   1 | step 6500 | batch 6500/11250 | lr 0.00004 0.00004 | loss 0.1852 | s/batch 0.71\n",
      "2020-08-08 10:32:47,120 INFO: | epoch   1 | step 6550 | batch 6550/11250 | lr 0.00004 0.00004 | loss 0.1876 | s/batch 0.63\n",
      "2020-08-08 10:33:18,531 INFO: | epoch   1 | step 6600 | batch 6600/11250 | lr 0.00004 0.00004 | loss 0.2643 | s/batch 0.63\n",
      "2020-08-08 10:33:52,140 INFO: | epoch   1 | step 6650 | batch 6650/11250 | lr 0.00004 0.00004 | loss 0.1960 | s/batch 0.67\n",
      "2020-08-08 10:34:23,853 INFO: | epoch   1 | step 6700 | batch 6700/11250 | lr 0.00004 0.00004 | loss 0.2668 | s/batch 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 10:35:00,479 INFO: | epoch   1 | step 6750 | batch 6750/11250 | lr 0.00004 0.00004 | loss 0.2656 | s/batch 0.73\n",
      "2020-08-08 10:35:36,138 INFO: | epoch   1 | step 6800 | batch 6800/11250 | lr 0.00004 0.00004 | loss 0.2303 | s/batch 0.71\n",
      "2020-08-08 10:36:06,908 INFO: | epoch   1 | step 6850 | batch 6850/11250 | lr 0.00004 0.00004 | loss 0.2332 | s/batch 0.62\n",
      "2020-08-08 10:36:41,873 INFO: | epoch   1 | step 6900 | batch 6900/11250 | lr 0.00004 0.00004 | loss 0.2591 | s/batch 0.70\n",
      "2020-08-08 10:37:16,577 INFO: | epoch   1 | step 6950 | batch 6950/11250 | lr 0.00004 0.00004 | loss 0.2011 | s/batch 0.69\n",
      "2020-08-08 10:37:52,017 INFO: | epoch   1 | step 7000 | batch 7000/11250 | lr 0.00003 0.00004 | loss 0.2738 | s/batch 0.71\n",
      "2020-08-08 10:38:29,650 INFO: | epoch   1 | step 7050 | batch 7050/11250 | lr 0.00003 0.00004 | loss 0.3265 | s/batch 0.75\n",
      "2020-08-08 10:39:04,229 INFO: | epoch   1 | step 7100 | batch 7100/11250 | lr 0.00003 0.00004 | loss 0.2068 | s/batch 0.69\n",
      "2020-08-08 10:39:37,896 INFO: | epoch   1 | step 7150 | batch 7150/11250 | lr 0.00003 0.00004 | loss 0.2073 | s/batch 0.67\n",
      "2020-08-08 10:40:15,066 INFO: | epoch   1 | step 7200 | batch 7200/11250 | lr 0.00003 0.00004 | loss 0.2463 | s/batch 0.74\n",
      "2020-08-08 10:40:57,370 INFO: | epoch   1 | step 7250 | batch 7250/11250 | lr 0.00003 0.00004 | loss 0.2087 | s/batch 0.85\n",
      "2020-08-08 10:41:34,702 INFO: | epoch   1 | step 7300 | batch 7300/11250 | lr 0.00003 0.00004 | loss 0.2688 | s/batch 0.75\n",
      "2020-08-08 10:42:09,912 INFO: | epoch   1 | step 7350 | batch 7350/11250 | lr 0.00003 0.00004 | loss 0.2143 | s/batch 0.70\n",
      "2020-08-08 10:42:46,462 INFO: | epoch   1 | step 7400 | batch 7400/11250 | lr 0.00003 0.00004 | loss 0.2519 | s/batch 0.73\n",
      "2020-08-08 10:43:17,068 INFO: | epoch   1 | step 7450 | batch 7450/11250 | lr 0.00003 0.00004 | loss 0.2056 | s/batch 0.61\n",
      "2020-08-08 10:43:54,149 INFO: | epoch   1 | step 7500 | batch 7500/11250 | lr 0.00003 0.00004 | loss 0.2224 | s/batch 0.74\n",
      "2020-08-08 10:44:30,234 INFO: | epoch   1 | step 7550 | batch 7550/11250 | lr 0.00003 0.00004 | loss 0.2629 | s/batch 0.72\n",
      "2020-08-08 10:45:04,381 INFO: | epoch   1 | step 7600 | batch 7600/11250 | lr 0.00003 0.00004 | loss 0.1942 | s/batch 0.68\n",
      "2020-08-08 10:45:36,097 INFO: | epoch   1 | step 7650 | batch 7650/11250 | lr 0.00003 0.00004 | loss 0.2196 | s/batch 0.63\n",
      "2020-08-08 10:46:10,289 INFO: | epoch   1 | step 7700 | batch 7700/11250 | lr 0.00003 0.00004 | loss 0.1947 | s/batch 0.68\n",
      "2020-08-08 10:46:43,138 INFO: | epoch   1 | step 7750 | batch 7750/11250 | lr 0.00003 0.00004 | loss 0.2340 | s/batch 0.66\n",
      "2020-08-08 10:47:16,184 INFO: | epoch   1 | step 7800 | batch 7800/11250 | lr 0.00003 0.00004 | loss 0.2203 | s/batch 0.66\n",
      "2020-08-08 10:47:54,522 INFO: | epoch   1 | step 7850 | batch 7850/11250 | lr 0.00003 0.00004 | loss 0.1814 | s/batch 0.77\n",
      "2020-08-08 10:48:28,161 INFO: | epoch   1 | step 7900 | batch 7900/11250 | lr 0.00003 0.00004 | loss 0.1682 | s/batch 0.67\n",
      "2020-08-08 10:49:01,422 INFO: | epoch   1 | step 7950 | batch 7950/11250 | lr 0.00003 0.00004 | loss 0.2527 | s/batch 0.67\n",
      "2020-08-08 10:49:31,464 INFO: | epoch   1 | step 8000 | batch 8000/11250 | lr 0.00002 0.00004 | loss 0.2358 | s/batch 0.60\n",
      "2020-08-08 10:50:06,299 INFO: | epoch   1 | step 8050 | batch 8050/11250 | lr 0.00002 0.00004 | loss 0.2461 | s/batch 0.70\n",
      "2020-08-08 10:50:38,264 INFO: | epoch   1 | step 8100 | batch 8100/11250 | lr 0.00002 0.00004 | loss 0.2256 | s/batch 0.64\n",
      "2020-08-08 10:51:11,332 INFO: | epoch   1 | step 8150 | batch 8150/11250 | lr 0.00002 0.00004 | loss 0.2160 | s/batch 0.66\n",
      "2020-08-08 10:51:47,245 INFO: | epoch   1 | step 8200 | batch 8200/11250 | lr 0.00002 0.00004 | loss 0.2338 | s/batch 0.72\n",
      "2020-08-08 10:52:20,721 INFO: | epoch   1 | step 8250 | batch 8250/11250 | lr 0.00002 0.00004 | loss 0.2133 | s/batch 0.67\n",
      "2020-08-08 10:52:55,133 INFO: | epoch   1 | step 8300 | batch 8300/11250 | lr 0.00002 0.00004 | loss 0.1837 | s/batch 0.69\n",
      "2020-08-08 10:53:28,746 INFO: | epoch   1 | step 8350 | batch 8350/11250 | lr 0.00002 0.00004 | loss 0.2004 | s/batch 0.67\n",
      "2020-08-08 10:54:02,300 INFO: | epoch   1 | step 8400 | batch 8400/11250 | lr 0.00002 0.00004 | loss 0.2199 | s/batch 0.67\n",
      "2020-08-08 10:54:37,359 INFO: | epoch   1 | step 8450 | batch 8450/11250 | lr 0.00002 0.00004 | loss 0.2222 | s/batch 0.70\n",
      "2020-08-08 10:55:12,991 INFO: | epoch   1 | step 8500 | batch 8500/11250 | lr 0.00002 0.00004 | loss 0.2044 | s/batch 0.71\n",
      "2020-08-08 10:55:48,026 INFO: | epoch   1 | step 8550 | batch 8550/11250 | lr 0.00002 0.00004 | loss 0.1991 | s/batch 0.70\n",
      "2020-08-08 10:56:26,449 INFO: | epoch   1 | step 8600 | batch 8600/11250 | lr 0.00002 0.00004 | loss 0.2339 | s/batch 0.77\n",
      "2020-08-08 10:57:01,414 INFO: | epoch   1 | step 8650 | batch 8650/11250 | lr 0.00002 0.00004 | loss 0.2110 | s/batch 0.70\n",
      "2020-08-08 10:57:34,450 INFO: | epoch   1 | step 8700 | batch 8700/11250 | lr 0.00002 0.00004 | loss 0.1965 | s/batch 0.66\n",
      "2020-08-08 10:58:05,437 INFO: | epoch   1 | step 8750 | batch 8750/11250 | lr 0.00002 0.00004 | loss 0.1541 | s/batch 0.62\n",
      "2020-08-08 10:58:42,582 INFO: | epoch   1 | step 8800 | batch 8800/11250 | lr 0.00002 0.00004 | loss 0.2119 | s/batch 0.74\n",
      "2020-08-08 10:59:24,659 INFO: | epoch   1 | step 8850 | batch 8850/11250 | lr 0.00002 0.00004 | loss 0.2421 | s/batch 0.84\n",
      "2020-08-08 11:00:01,994 INFO: | epoch   1 | step 8900 | batch 8900/11250 | lr 0.00002 0.00004 | loss 0.2471 | s/batch 0.75\n",
      "2020-08-08 11:00:42,979 INFO: | epoch   1 | step 8950 | batch 8950/11250 | lr 0.00002 0.00004 | loss 0.2523 | s/batch 0.82\n",
      "2020-08-08 11:01:19,040 INFO: | epoch   1 | step 9000 | batch 9000/11250 | lr 0.00002 0.00004 | loss 0.1650 | s/batch 0.72\n",
      "2020-08-08 11:01:54,749 INFO: | epoch   1 | step 9050 | batch 9050/11250 | lr 0.00002 0.00004 | loss 0.1885 | s/batch 0.71\n",
      "2020-08-08 11:02:30,374 INFO: | epoch   1 | step 9100 | batch 9100/11250 | lr 0.00002 0.00004 | loss 0.2293 | s/batch 0.71\n",
      "2020-08-08 11:03:00,778 INFO: | epoch   1 | step 9150 | batch 9150/11250 | lr 0.00002 0.00004 | loss 0.1952 | s/batch 0.61\n",
      "2020-08-08 11:03:41,737 INFO: | epoch   1 | step 9200 | batch 9200/11250 | lr 0.00002 0.00004 | loss 0.2064 | s/batch 0.82\n",
      "2020-08-08 11:04:10,793 INFO: | epoch   1 | step 9250 | batch 9250/11250 | lr 0.00002 0.00004 | loss 0.1629 | s/batch 0.58\n",
      "2020-08-08 11:04:46,426 INFO: | epoch   1 | step 9300 | batch 9300/11250 | lr 0.00002 0.00004 | loss 0.1774 | s/batch 0.71\n",
      "2020-08-08 11:05:25,112 INFO: | epoch   1 | step 9350 | batch 9350/11250 | lr 0.00002 0.00004 | loss 0.2038 | s/batch 0.77\n",
      "2020-08-08 11:05:57,987 INFO: | epoch   1 | step 9400 | batch 9400/11250 | lr 0.00002 0.00004 | loss 0.1915 | s/batch 0.66\n",
      "2020-08-08 11:06:37,278 INFO: | epoch   1 | step 9450 | batch 9450/11250 | lr 0.00002 0.00004 | loss 0.1608 | s/batch 0.79\n",
      "2020-08-08 11:07:11,426 INFO: | epoch   1 | step 9500 | batch 9500/11250 | lr 0.00002 0.00004 | loss 0.2187 | s/batch 0.68\n",
      "2020-08-08 11:07:41,773 INFO: | epoch   1 | step 9550 | batch 9550/11250 | lr 0.00002 0.00004 | loss 0.1490 | s/batch 0.61\n",
      "2020-08-08 11:08:15,810 INFO: | epoch   1 | step 9600 | batch 9600/11250 | lr 0.00002 0.00004 | loss 0.2130 | s/batch 0.68\n",
      "2020-08-08 11:08:48,819 INFO: | epoch   1 | step 9650 | batch 9650/11250 | lr 0.00002 0.00004 | loss 0.1800 | s/batch 0.66\n",
      "2020-08-08 11:09:23,275 INFO: | epoch   1 | step 9700 | batch 9700/11250 | lr 0.00002 0.00004 | loss 0.1934 | s/batch 0.69\n",
      "2020-08-08 11:09:59,300 INFO: | epoch   1 | step 9750 | batch 9750/11250 | lr 0.00002 0.00004 | loss 0.1960 | s/batch 0.72\n",
      "2020-08-08 11:10:34,889 INFO: | epoch   1 | step 9800 | batch 9800/11250 | lr 0.00002 0.00004 | loss 0.1968 | s/batch 0.71\n",
      "2020-08-08 11:11:11,566 INFO: | epoch   1 | step 9850 | batch 9850/11250 | lr 0.00002 0.00004 | loss 0.2412 | s/batch 0.73\n",
      "2020-08-08 11:11:48,576 INFO: | epoch   1 | step 9900 | batch 9900/11250 | lr 0.00002 0.00004 | loss 0.2032 | s/batch 0.74\n",
      "2020-08-08 11:12:29,096 INFO: | epoch   1 | step 9950 | batch 9950/11250 | lr 0.00002 0.00004 | loss 0.1812 | s/batch 0.81\n",
      "2020-08-08 11:13:07,384 INFO: | epoch   1 | step 10000 | batch 10000/11250 | lr 0.00001 0.00004 | loss 0.2347 | s/batch 0.77\n",
      "2020-08-08 11:13:42,424 INFO: | epoch   1 | step 10050 | batch 10050/11250 | lr 0.00001 0.00004 | loss 0.2169 | s/batch 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 11:14:18,045 INFO: | epoch   1 | step 10100 | batch 10100/11250 | lr 0.00001 0.00004 | loss 0.1716 | s/batch 0.71\n",
      "2020-08-08 11:15:00,032 INFO: | epoch   1 | step 10150 | batch 10150/11250 | lr 0.00001 0.00004 | loss 0.1905 | s/batch 0.84\n",
      "2020-08-08 11:15:37,163 INFO: | epoch   1 | step 10200 | batch 10200/11250 | lr 0.00001 0.00004 | loss 0.2151 | s/batch 0.74\n",
      "2020-08-08 11:16:11,075 INFO: | epoch   1 | step 10250 | batch 10250/11250 | lr 0.00001 0.00004 | loss 0.2177 | s/batch 0.68\n",
      "2020-08-08 11:16:42,211 INFO: | epoch   1 | step 10300 | batch 10300/11250 | lr 0.00001 0.00004 | loss 0.1829 | s/batch 0.62\n",
      "2020-08-08 11:17:17,519 INFO: | epoch   1 | step 10350 | batch 10350/11250 | lr 0.00001 0.00004 | loss 0.1948 | s/batch 0.71\n",
      "2020-08-08 11:17:53,327 INFO: | epoch   1 | step 10400 | batch 10400/11250 | lr 0.00001 0.00004 | loss 0.1886 | s/batch 0.72\n",
      "2020-08-08 11:18:30,465 INFO: | epoch   1 | step 10450 | batch 10450/11250 | lr 0.00001 0.00004 | loss 0.1998 | s/batch 0.74\n",
      "2020-08-08 11:19:03,255 INFO: | epoch   1 | step 10500 | batch 10500/11250 | lr 0.00001 0.00004 | loss 0.2738 | s/batch 0.66\n",
      "2020-08-08 11:19:39,465 INFO: | epoch   1 | step 10550 | batch 10550/11250 | lr 0.00001 0.00004 | loss 0.1784 | s/batch 0.72\n",
      "2020-08-08 11:20:16,238 INFO: | epoch   1 | step 10600 | batch 10600/11250 | lr 0.00001 0.00004 | loss 0.2504 | s/batch 0.74\n",
      "2020-08-08 11:20:48,190 INFO: | epoch   1 | step 10650 | batch 10650/11250 | lr 0.00001 0.00004 | loss 0.2382 | s/batch 0.64\n",
      "2020-08-08 11:21:20,518 INFO: | epoch   1 | step 10700 | batch 10700/11250 | lr 0.00001 0.00004 | loss 0.1761 | s/batch 0.65\n",
      "2020-08-08 11:21:54,916 INFO: | epoch   1 | step 10750 | batch 10750/11250 | lr 0.00001 0.00004 | loss 0.1918 | s/batch 0.69\n",
      "2020-08-08 11:22:28,296 INFO: | epoch   1 | step 10800 | batch 10800/11250 | lr 0.00001 0.00004 | loss 0.1753 | s/batch 0.67\n",
      "2020-08-08 11:23:04,904 INFO: | epoch   1 | step 10850 | batch 10850/11250 | lr 0.00001 0.00004 | loss 0.2063 | s/batch 0.73\n",
      "2020-08-08 11:23:39,509 INFO: | epoch   1 | step 10900 | batch 10900/11250 | lr 0.00001 0.00004 | loss 0.2106 | s/batch 0.69\n",
      "2020-08-08 11:24:13,175 INFO: | epoch   1 | step 10950 | batch 10950/11250 | lr 0.00001 0.00004 | loss 0.1977 | s/batch 0.67\n",
      "2020-08-08 11:24:50,589 INFO: | epoch   1 | step 11000 | batch 11000/11250 | lr 0.00001 0.00004 | loss 0.2166 | s/batch 0.75\n",
      "2020-08-08 11:25:25,022 INFO: | epoch   1 | step 11050 | batch 11050/11250 | lr 0.00001 0.00004 | loss 0.1713 | s/batch 0.69\n",
      "2020-08-08 11:26:01,772 INFO: | epoch   1 | step 11100 | batch 11100/11250 | lr 0.00001 0.00004 | loss 0.2125 | s/batch 0.73\n",
      "2020-08-08 11:26:36,960 INFO: | epoch   1 | step 11150 | batch 11150/11250 | lr 0.00001 0.00004 | loss 0.2011 | s/batch 0.70\n",
      "2020-08-08 11:27:10,362 INFO: | epoch   1 | step 11200 | batch 11200/11250 | lr 0.00001 0.00004 | loss 0.1924 | s/batch 0.67\n",
      "2020-08-08 11:27:43,721 INFO: | epoch   1 | step 11250 | batch 11250/11250 | lr 0.00001 0.00004 | loss 0.1904 | s/batch 0.67\n",
      "2020-08-08 11:27:43,991 INFO: | epoch   1 | score (89.74, 87.22, 88.41) | f1 88.41 | loss 0.2913 | time 7723.11\n",
      "2020-08-08 11:27:44,353 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9187    0.9248    0.9218     35027\n",
      "          股票     0.9125    0.9349    0.9236     33251\n",
      "          体育     0.9742    0.9798    0.9770     28283\n",
      "          娱乐     0.9311    0.9464    0.9387     19920\n",
      "          时政     0.8720    0.8846    0.8783     13515\n",
      "          社会     0.8529    0.8513    0.8521     11009\n",
      "          教育     0.9228    0.9153    0.9191      8987\n",
      "          财经     0.8610    0.7864    0.8220      7957\n",
      "          家居     0.8814    0.8705    0.8759      7063\n",
      "          游戏     0.8929    0.8560    0.8741      5291\n",
      "          房产     0.9063    0.8537    0.8792      4428\n",
      "          时尚     0.8583    0.8165    0.8369      2818\n",
      "          彩票     0.9199    0.8334    0.8745      1639\n",
      "          星座     0.8589    0.7574    0.8050       812\n",
      "\n",
      "    accuracy                         0.9144    180000\n",
      "   macro avg     0.8974    0.8722    0.8841    180000\n",
      "weighted avg     0.9140    0.9144    0.9140    180000\n",
      "\n",
      "2020-08-08 11:38:30,197 INFO: | epoch   1 | dev | score (94.3, 93.81, 94.01) | f1 94.01 | time 645.84\n",
      "2020-08-08 11:38:30,259 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9367    0.9584    0.9474      3891\n",
      "          股票     0.9463    0.9594    0.9528      3694\n",
      "          体育     0.9910    0.9835    0.9872      3142\n",
      "          娱乐     0.9468    0.9738    0.9601      2213\n",
      "          时政     0.9272    0.9081    0.9175      1501\n",
      "          社会     0.9478    0.8913    0.9187      1223\n",
      "          教育     0.9502    0.9569    0.9536       998\n",
      "          财经     0.8882    0.8631    0.8755       884\n",
      "          家居     0.9587    0.9184    0.9381       784\n",
      "          游戏     0.9477    0.8961    0.9212       587\n",
      "          房产     0.9412    0.9756    0.9581       492\n",
      "          时尚     0.9692    0.9042    0.9355       313\n",
      "          彩票     0.9451    0.9451    0.9451       182\n",
      "          星座     0.9057    1.0000    0.9505        96\n",
      "\n",
      "    accuracy                         0.9483     20000\n",
      "   macro avg     0.9430    0.9381    0.9401     20000\n",
      "weighted avg     0.9484    0.9483    0.9481     20000\n",
      "\n",
      "2020-08-08 11:38:30,263 INFO: Exceed history dev = 0.00, current dev = 94.01\n",
      "2020-08-08 11:39:07,401 INFO: | epoch   2 | step 11300 | batch  50/11250 | lr 0.00001 0.00004 | loss 0.2053 | s/batch 0.74\n",
      "2020-08-08 11:39:43,338 INFO: | epoch   2 | step 11350 | batch 100/11250 | lr 0.00001 0.00004 | loss 0.1640 | s/batch 0.72\n",
      "2020-08-08 11:40:20,286 INFO: | epoch   2 | step 11400 | batch 150/11250 | lr 0.00001 0.00004 | loss 0.1489 | s/batch 0.74\n",
      "2020-08-08 11:40:55,003 INFO: | epoch   2 | step 11450 | batch 200/11250 | lr 0.00001 0.00004 | loss 0.1795 | s/batch 0.69\n",
      "2020-08-08 11:41:29,469 INFO: | epoch   2 | step 11500 | batch 250/11250 | lr 0.00001 0.00004 | loss 0.1568 | s/batch 0.69\n",
      "2020-08-08 11:42:05,014 INFO: | epoch   2 | step 11550 | batch 300/11250 | lr 0.00001 0.00004 | loss 0.1628 | s/batch 0.71\n",
      "2020-08-08 11:42:40,856 INFO: | epoch   2 | step 11600 | batch 350/11250 | lr 0.00001 0.00004 | loss 0.1778 | s/batch 0.72\n",
      "2020-08-08 11:43:17,246 INFO: | epoch   2 | step 11650 | batch 400/11250 | lr 0.00001 0.00004 | loss 0.1998 | s/batch 0.73\n",
      "2020-08-08 11:43:53,332 INFO: | epoch   2 | step 11700 | batch 450/11250 | lr 0.00001 0.00004 | loss 0.1910 | s/batch 0.72\n",
      "2020-08-08 11:44:29,470 INFO: | epoch   2 | step 11750 | batch 500/11250 | lr 0.00001 0.00004 | loss 0.1948 | s/batch 0.72\n",
      "2020-08-08 11:44:59,965 INFO: | epoch   2 | step 11800 | batch 550/11250 | lr 0.00001 0.00004 | loss 0.1455 | s/batch 0.61\n",
      "2020-08-08 11:45:36,072 INFO: | epoch   2 | step 11850 | batch 600/11250 | lr 0.00001 0.00004 | loss 0.2040 | s/batch 0.72\n",
      "2020-08-08 11:46:19,931 INFO: | epoch   2 | step 11900 | batch 650/11250 | lr 0.00001 0.00004 | loss 0.1769 | s/batch 0.88\n",
      "2020-08-08 11:46:53,034 INFO: | epoch   2 | step 11950 | batch 700/11250 | lr 0.00001 0.00004 | loss 0.1828 | s/batch 0.66\n",
      "2020-08-08 11:47:26,041 INFO: | epoch   2 | step 12000 | batch 750/11250 | lr 0.00001 0.00004 | loss 0.1753 | s/batch 0.66\n",
      "2020-08-08 11:48:03,689 INFO: | epoch   2 | step 12050 | batch 800/11250 | lr 0.00001 0.00004 | loss 0.2197 | s/batch 0.75\n",
      "2020-08-08 11:48:38,239 INFO: | epoch   2 | step 12100 | batch 850/11250 | lr 0.00001 0.00004 | loss 0.1314 | s/batch 0.69\n",
      "2020-08-08 11:49:17,562 INFO: | epoch   2 | step 12150 | batch 900/11250 | lr 0.00001 0.00004 | loss 0.1691 | s/batch 0.79\n",
      "2020-08-08 11:49:53,418 INFO: | epoch   2 | step 12200 | batch 950/11250 | lr 0.00001 0.00004 | loss 0.1914 | s/batch 0.72\n",
      "2020-08-08 11:50:30,410 INFO: | epoch   2 | step 12250 | batch 1000/11250 | lr 0.00001 0.00004 | loss 0.2436 | s/batch 0.74\n",
      "2020-08-08 11:51:05,910 INFO: | epoch   2 | step 12300 | batch 1050/11250 | lr 0.00001 0.00004 | loss 0.1714 | s/batch 0.71\n",
      "2020-08-08 11:51:47,259 INFO: | epoch   2 | step 12350 | batch 1100/11250 | lr 0.00001 0.00004 | loss 0.1751 | s/batch 0.83\n",
      "2020-08-08 11:52:24,960 INFO: | epoch   2 | step 12400 | batch 1150/11250 | lr 0.00001 0.00004 | loss 0.2008 | s/batch 0.75\n",
      "2020-08-08 11:53:05,671 INFO: | epoch   2 | step 12450 | batch 1200/11250 | lr 0.00001 0.00004 | loss 0.2061 | s/batch 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 11:53:41,052 INFO: | epoch   2 | step 12500 | batch 1250/11250 | lr 0.00001 0.00004 | loss 0.1173 | s/batch 0.71\n",
      "2020-08-08 11:54:12,962 INFO: | epoch   2 | step 12550 | batch 1300/11250 | lr 0.00001 0.00004 | loss 0.1595 | s/batch 0.64\n",
      "2020-08-08 11:54:43,536 INFO: | epoch   2 | step 12600 | batch 1350/11250 | lr 0.00001 0.00004 | loss 0.1838 | s/batch 0.61\n",
      "2020-08-08 11:55:16,738 INFO: | epoch   2 | step 12650 | batch 1400/11250 | lr 0.00001 0.00004 | loss 0.1821 | s/batch 0.66\n",
      "2020-08-08 11:55:49,980 INFO: | epoch   2 | step 12700 | batch 1450/11250 | lr 0.00001 0.00004 | loss 0.1403 | s/batch 0.66\n",
      "2020-08-08 11:56:23,231 INFO: | epoch   2 | step 12750 | batch 1500/11250 | lr 0.00001 0.00004 | loss 0.2048 | s/batch 0.66\n",
      "2020-08-08 11:56:55,639 INFO: | epoch   2 | step 12800 | batch 1550/11250 | lr 0.00001 0.00004 | loss 0.1490 | s/batch 0.65\n",
      "2020-08-08 11:57:28,270 INFO: | epoch   2 | step 12850 | batch 1600/11250 | lr 0.00001 0.00004 | loss 0.1677 | s/batch 0.65\n",
      "2020-08-08 11:58:04,289 INFO: | epoch   2 | step 12900 | batch 1650/11250 | lr 0.00001 0.00004 | loss 0.1763 | s/batch 0.72\n",
      "2020-08-08 11:58:32,792 INFO: | epoch   2 | step 12950 | batch 1700/11250 | lr 0.00001 0.00004 | loss 0.1432 | s/batch 0.57\n",
      "2020-08-08 11:59:04,268 INFO: | epoch   2 | step 13000 | batch 1750/11250 | lr 0.00000 0.00004 | loss 0.1832 | s/batch 0.63\n",
      "2020-08-08 11:59:39,974 INFO: | epoch   2 | step 13050 | batch 1800/11250 | lr 0.00000 0.00004 | loss 0.1642 | s/batch 0.71\n",
      "2020-08-08 12:00:14,707 INFO: | epoch   2 | step 13100 | batch 1850/11250 | lr 0.00000 0.00004 | loss 0.1630 | s/batch 0.69\n",
      "2020-08-08 12:00:49,259 INFO: | epoch   2 | step 13150 | batch 1900/11250 | lr 0.00000 0.00004 | loss 0.1416 | s/batch 0.69\n",
      "2020-08-08 12:01:24,622 INFO: | epoch   2 | step 13200 | batch 1950/11250 | lr 0.00000 0.00004 | loss 0.1680 | s/batch 0.71\n",
      "2020-08-08 12:01:54,026 INFO: | epoch   2 | step 13250 | batch 2000/11250 | lr 0.00000 0.00004 | loss 0.1282 | s/batch 0.59\n",
      "2020-08-08 12:02:21,110 INFO: | epoch   2 | step 13300 | batch 2050/11250 | lr 0.00000 0.00004 | loss 0.1515 | s/batch 0.54\n",
      "2020-08-08 12:02:56,685 INFO: | epoch   2 | step 13350 | batch 2100/11250 | lr 0.00000 0.00004 | loss 0.2094 | s/batch 0.71\n",
      "2020-08-08 12:03:30,912 INFO: | epoch   2 | step 13400 | batch 2150/11250 | lr 0.00000 0.00004 | loss 0.1603 | s/batch 0.68\n",
      "2020-08-08 12:04:05,755 INFO: | epoch   2 | step 13450 | batch 2200/11250 | lr 0.00000 0.00004 | loss 0.1422 | s/batch 0.70\n",
      "2020-08-08 12:04:33,381 INFO: | epoch   2 | step 13500 | batch 2250/11250 | lr 0.00000 0.00003 | loss 0.1088 | s/batch 0.55\n",
      "2020-08-08 12:05:06,814 INFO: | epoch   2 | step 13550 | batch 2300/11250 | lr 0.00000 0.00003 | loss 0.1369 | s/batch 0.67\n",
      "2020-08-08 12:05:39,789 INFO: | epoch   2 | step 13600 | batch 2350/11250 | lr 0.00000 0.00003 | loss 0.1463 | s/batch 0.66\n",
      "2020-08-08 12:06:12,645 INFO: | epoch   2 | step 13650 | batch 2400/11250 | lr 0.00000 0.00003 | loss 0.1571 | s/batch 0.66\n",
      "2020-08-08 12:06:43,356 INFO: | epoch   2 | step 13700 | batch 2450/11250 | lr 0.00000 0.00003 | loss 0.1678 | s/batch 0.61\n",
      "2020-08-08 12:07:19,879 INFO: | epoch   2 | step 13750 | batch 2500/11250 | lr 0.00000 0.00003 | loss 0.2300 | s/batch 0.73\n",
      "2020-08-08 12:07:51,488 INFO: | epoch   2 | step 13800 | batch 2550/11250 | lr 0.00000 0.00003 | loss 0.1574 | s/batch 0.63\n",
      "2020-08-08 12:08:29,099 INFO: | epoch   2 | step 13850 | batch 2600/11250 | lr 0.00000 0.00003 | loss 0.1345 | s/batch 0.75\n",
      "2020-08-08 12:08:56,248 INFO: | epoch   2 | step 13900 | batch 2650/11250 | lr 0.00000 0.00003 | loss 0.1559 | s/batch 0.54\n",
      "2020-08-08 12:09:34,113 INFO: | epoch   2 | step 13950 | batch 2700/11250 | lr 0.00000 0.00003 | loss 0.1971 | s/batch 0.76\n",
      "2020-08-08 12:10:09,916 INFO: | epoch   2 | step 14000 | batch 2750/11250 | lr 0.00000 0.00003 | loss 0.2440 | s/batch 0.72\n",
      "2020-08-08 12:10:44,493 INFO: | epoch   2 | step 14050 | batch 2800/11250 | lr 0.00000 0.00003 | loss 0.2097 | s/batch 0.69\n",
      "2020-08-08 12:11:17,105 INFO: | epoch   2 | step 14100 | batch 2850/11250 | lr 0.00000 0.00003 | loss 0.1851 | s/batch 0.65\n",
      "2020-08-08 12:11:52,177 INFO: | epoch   2 | step 14150 | batch 2900/11250 | lr 0.00000 0.00003 | loss 0.2079 | s/batch 0.70\n",
      "2020-08-08 12:12:28,552 INFO: | epoch   2 | step 14200 | batch 2950/11250 | lr 0.00000 0.00003 | loss 0.1357 | s/batch 0.73\n",
      "2020-08-08 12:13:02,712 INFO: | epoch   2 | step 14250 | batch 3000/11250 | lr 0.00000 0.00003 | loss 0.1603 | s/batch 0.68\n",
      "2020-08-08 12:13:38,219 INFO: | epoch   2 | step 14300 | batch 3050/11250 | lr 0.00000 0.00003 | loss 0.1709 | s/batch 0.71\n",
      "2020-08-08 12:14:12,127 INFO: | epoch   2 | step 14350 | batch 3100/11250 | lr 0.00000 0.00003 | loss 0.1826 | s/batch 0.68\n",
      "2020-08-08 12:14:43,955 INFO: | epoch   2 | step 14400 | batch 3150/11250 | lr 0.00000 0.00003 | loss 0.1220 | s/batch 0.64\n",
      "2020-08-08 12:15:16,892 INFO: | epoch   2 | step 14450 | batch 3200/11250 | lr 0.00000 0.00003 | loss 0.1404 | s/batch 0.66\n",
      "2020-08-08 12:15:52,533 INFO: | epoch   2 | step 14500 | batch 3250/11250 | lr 0.00000 0.00003 | loss 0.1910 | s/batch 0.71\n",
      "2020-08-08 12:16:32,916 INFO: | epoch   2 | step 14550 | batch 3300/11250 | lr 0.00000 0.00003 | loss 0.1815 | s/batch 0.81\n",
      "2020-08-08 12:17:15,735 INFO: | epoch   2 | step 14600 | batch 3350/11250 | lr 0.00000 0.00003 | loss 0.1831 | s/batch 0.86\n",
      "2020-08-08 12:17:50,351 INFO: | epoch   2 | step 14650 | batch 3400/11250 | lr 0.00000 0.00003 | loss 0.1250 | s/batch 0.69\n",
      "2020-08-08 12:18:33,053 INFO: | epoch   2 | step 14700 | batch 3450/11250 | lr 0.00000 0.00003 | loss 0.1463 | s/batch 0.85\n",
      "2020-08-08 12:19:10,089 INFO: | epoch   2 | step 14750 | batch 3500/11250 | lr 0.00000 0.00003 | loss 0.1435 | s/batch 0.74\n",
      "2020-08-08 12:19:48,575 INFO: | epoch   2 | step 14800 | batch 3550/11250 | lr 0.00000 0.00003 | loss 0.1982 | s/batch 0.77\n",
      "2020-08-08 12:20:25,878 INFO: | epoch   2 | step 14850 | batch 3600/11250 | lr 0.00000 0.00003 | loss 0.1977 | s/batch 0.75\n",
      "2020-08-08 12:21:03,245 INFO: | epoch   2 | step 14900 | batch 3650/11250 | lr 0.00000 0.00003 | loss 0.1816 | s/batch 0.75\n",
      "2020-08-08 12:21:39,222 INFO: | epoch   2 | step 14950 | batch 3700/11250 | lr 0.00000 0.00003 | loss 0.1857 | s/batch 0.72\n",
      "2020-08-08 12:22:17,421 INFO: | epoch   2 | step 15000 | batch 3750/11250 | lr 0.00000 0.00003 | loss 0.2175 | s/batch 0.76\n",
      "2020-08-08 12:22:53,352 INFO: | epoch   2 | step 15050 | batch 3800/11250 | lr 0.00000 0.00003 | loss 0.2041 | s/batch 0.72\n",
      "2020-08-08 12:23:29,485 INFO: | epoch   2 | step 15100 | batch 3850/11250 | lr 0.00000 0.00003 | loss 0.1697 | s/batch 0.72\n",
      "2020-08-08 12:24:03,279 INFO: | epoch   2 | step 15150 | batch 3900/11250 | lr 0.00000 0.00003 | loss 0.1651 | s/batch 0.68\n",
      "2020-08-08 12:24:38,472 INFO: | epoch   2 | step 15200 | batch 3950/11250 | lr 0.00000 0.00003 | loss 0.1465 | s/batch 0.70\n",
      "2020-08-08 12:25:08,917 INFO: | epoch   2 | step 15250 | batch 4000/11250 | lr 0.00000 0.00003 | loss 0.1924 | s/batch 0.61\n",
      "2020-08-08 12:25:45,058 INFO: | epoch   2 | step 15300 | batch 4050/11250 | lr 0.00000 0.00003 | loss 0.1720 | s/batch 0.72\n",
      "2020-08-08 12:26:22,756 INFO: | epoch   2 | step 15350 | batch 4100/11250 | lr 0.00000 0.00003 | loss 0.1929 | s/batch 0.75\n",
      "2020-08-08 12:26:56,689 INFO: | epoch   2 | step 15400 | batch 4150/11250 | lr 0.00000 0.00003 | loss 0.1567 | s/batch 0.68\n",
      "2020-08-08 12:27:30,419 INFO: | epoch   2 | step 15450 | batch 4200/11250 | lr 0.00000 0.00003 | loss 0.1630 | s/batch 0.67\n",
      "2020-08-08 12:28:01,004 INFO: | epoch   2 | step 15500 | batch 4250/11250 | lr 0.00000 0.00003 | loss 0.1576 | s/batch 0.61\n",
      "2020-08-08 12:28:33,862 INFO: | epoch   2 | step 15550 | batch 4300/11250 | lr 0.00000 0.00003 | loss 0.1556 | s/batch 0.66\n",
      "2020-08-08 12:29:09,514 INFO: | epoch   2 | step 15600 | batch 4350/11250 | lr 0.00000 0.00003 | loss 0.1747 | s/batch 0.71\n",
      "2020-08-08 12:29:44,474 INFO: | epoch   2 | step 15650 | batch 4400/11250 | lr 0.00000 0.00003 | loss 0.1738 | s/batch 0.70\n",
      "2020-08-08 12:30:19,399 INFO: | epoch   2 | step 15700 | batch 4450/11250 | lr 0.00000 0.00003 | loss 0.1778 | s/batch 0.70\n",
      "2020-08-08 12:31:00,700 INFO: | epoch   2 | step 15750 | batch 4500/11250 | lr 0.00000 0.00003 | loss 0.1724 | s/batch 0.83\n",
      "2020-08-08 12:31:37,336 INFO: | epoch   2 | step 15800 | batch 4550/11250 | lr 0.00000 0.00003 | loss 0.1982 | s/batch 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 12:32:05,499 INFO: | epoch   2 | step 15850 | batch 4600/11250 | lr 0.00000 0.00003 | loss 0.1531 | s/batch 0.56\n",
      "2020-08-08 12:32:41,147 INFO: | epoch   2 | step 15900 | batch 4650/11250 | lr 0.00000 0.00003 | loss 0.1365 | s/batch 0.71\n",
      "2020-08-08 12:33:14,478 INFO: | epoch   2 | step 15950 | batch 4700/11250 | lr 0.00000 0.00003 | loss 0.1458 | s/batch 0.67\n",
      "2020-08-08 12:33:47,215 INFO: | epoch   2 | step 16000 | batch 4750/11250 | lr 0.00000 0.00003 | loss 0.1730 | s/batch 0.65\n",
      "2020-08-08 12:34:22,477 INFO: | epoch   2 | step 16050 | batch 4800/11250 | lr 0.00000 0.00003 | loss 0.2174 | s/batch 0.71\n",
      "2020-08-08 12:34:59,335 INFO: | epoch   2 | step 16100 | batch 4850/11250 | lr 0.00000 0.00003 | loss 0.1621 | s/batch 0.74\n",
      "2020-08-08 12:35:35,361 INFO: | epoch   2 | step 16150 | batch 4900/11250 | lr 0.00000 0.00003 | loss 0.1914 | s/batch 0.72\n",
      "2020-08-08 12:36:08,901 INFO: | epoch   2 | step 16200 | batch 4950/11250 | lr 0.00000 0.00003 | loss 0.1322 | s/batch 0.67\n",
      "2020-08-08 12:36:45,763 INFO: | epoch   2 | step 16250 | batch 5000/11250 | lr 0.00000 0.00003 | loss 0.2177 | s/batch 0.74\n",
      "2020-08-08 12:37:21,296 INFO: | epoch   2 | step 16300 | batch 5050/11250 | lr 0.00000 0.00003 | loss 0.1575 | s/batch 0.71\n",
      "2020-08-08 12:37:57,787 INFO: | epoch   2 | step 16350 | batch 5100/11250 | lr 0.00000 0.00003 | loss 0.2098 | s/batch 0.73\n",
      "2020-08-08 12:38:31,075 INFO: | epoch   2 | step 16400 | batch 5150/11250 | lr 0.00000 0.00003 | loss 0.1432 | s/batch 0.67\n",
      "2020-08-08 12:39:02,386 INFO: | epoch   2 | step 16450 | batch 5200/11250 | lr 0.00000 0.00003 | loss 0.1884 | s/batch 0.63\n",
      "2020-08-08 12:39:33,495 INFO: | epoch   2 | step 16500 | batch 5250/11250 | lr 0.00000 0.00003 | loss 0.1754 | s/batch 0.62\n",
      "2020-08-08 12:40:08,757 INFO: | epoch   2 | step 16550 | batch 5300/11250 | lr 0.00000 0.00003 | loss 0.1705 | s/batch 0.71\n",
      "2020-08-08 12:40:41,094 INFO: | epoch   2 | step 16600 | batch 5350/11250 | lr 0.00000 0.00003 | loss 0.1281 | s/batch 0.65\n",
      "2020-08-08 12:41:17,640 INFO: | epoch   2 | step 16650 | batch 5400/11250 | lr 0.00000 0.00003 | loss 0.1860 | s/batch 0.73\n",
      "2020-08-08 12:41:53,863 INFO: | epoch   2 | step 16700 | batch 5450/11250 | lr 0.00000 0.00003 | loss 0.1254 | s/batch 0.72\n",
      "2020-08-08 12:42:24,727 INFO: | epoch   2 | step 16750 | batch 5500/11250 | lr 0.00000 0.00003 | loss 0.1824 | s/batch 0.62\n",
      "2020-08-08 12:42:56,749 INFO: | epoch   2 | step 16800 | batch 5550/11250 | lr 0.00000 0.00003 | loss 0.2021 | s/batch 0.64\n",
      "2020-08-08 12:43:33,116 INFO: | epoch   2 | step 16850 | batch 5600/11250 | lr 0.00000 0.00003 | loss 0.1881 | s/batch 0.73\n",
      "2020-08-08 12:44:09,914 INFO: | epoch   2 | step 16900 | batch 5650/11250 | lr 0.00000 0.00003 | loss 0.1599 | s/batch 0.74\n",
      "2020-08-08 12:44:44,298 INFO: | epoch   2 | step 16950 | batch 5700/11250 | lr 0.00000 0.00003 | loss 0.1722 | s/batch 0.69\n",
      "2020-08-08 12:45:21,036 INFO: | epoch   2 | step 17000 | batch 5750/11250 | lr 0.00000 0.00003 | loss 0.1970 | s/batch 0.73\n",
      "2020-08-08 12:45:56,764 INFO: | epoch   2 | step 17050 | batch 5800/11250 | lr 0.00000 0.00003 | loss 0.1527 | s/batch 0.71\n",
      "2020-08-08 12:46:28,354 INFO: | epoch   2 | step 17100 | batch 5850/11250 | lr 0.00000 0.00003 | loss 0.1660 | s/batch 0.63\n",
      "2020-08-08 12:47:00,614 INFO: | epoch   2 | step 17150 | batch 5900/11250 | lr 0.00000 0.00003 | loss 0.1644 | s/batch 0.65\n",
      "2020-08-08 12:47:33,607 INFO: | epoch   2 | step 17200 | batch 5950/11250 | lr 0.00000 0.00003 | loss 0.1896 | s/batch 0.66\n",
      "2020-08-08 12:48:05,224 INFO: | epoch   2 | step 17250 | batch 6000/11250 | lr 0.00000 0.00003 | loss 0.1482 | s/batch 0.63\n",
      "2020-08-08 12:48:39,211 INFO: | epoch   2 | step 17300 | batch 6050/11250 | lr 0.00000 0.00003 | loss 0.1952 | s/batch 0.68\n",
      "2020-08-08 12:49:13,870 INFO: | epoch   2 | step 17350 | batch 6100/11250 | lr 0.00000 0.00003 | loss 0.1430 | s/batch 0.69\n",
      "2020-08-08 12:49:48,503 INFO: | epoch   2 | step 17400 | batch 6150/11250 | lr 0.00000 0.00003 | loss 0.1386 | s/batch 0.69\n",
      "2020-08-08 12:50:18,085 INFO: | epoch   2 | step 17450 | batch 6200/11250 | lr 0.00000 0.00003 | loss 0.1441 | s/batch 0.59\n",
      "2020-08-08 12:50:50,865 INFO: | epoch   2 | step 17500 | batch 6250/11250 | lr 0.00000 0.00003 | loss 0.1599 | s/batch 0.66\n",
      "2020-08-08 12:51:24,715 INFO: | epoch   2 | step 17550 | batch 6300/11250 | lr 0.00000 0.00003 | loss 0.1749 | s/batch 0.68\n",
      "2020-08-08 12:52:02,024 INFO: | epoch   2 | step 17600 | batch 6350/11250 | lr 0.00000 0.00003 | loss 0.1965 | s/batch 0.75\n",
      "2020-08-08 12:52:35,646 INFO: | epoch   2 | step 17650 | batch 6400/11250 | lr 0.00000 0.00003 | loss 0.1562 | s/batch 0.67\n",
      "2020-08-08 12:53:08,710 INFO: | epoch   2 | step 17700 | batch 6450/11250 | lr 0.00000 0.00003 | loss 0.1692 | s/batch 0.66\n",
      "2020-08-08 12:53:40,507 INFO: | epoch   2 | step 17750 | batch 6500/11250 | lr 0.00000 0.00003 | loss 0.2188 | s/batch 0.64\n",
      "2020-08-08 12:54:16,488 INFO: | epoch   2 | step 17800 | batch 6550/11250 | lr 0.00000 0.00003 | loss 0.1095 | s/batch 0.72\n",
      "2020-08-08 12:54:47,598 INFO: | epoch   2 | step 17850 | batch 6600/11250 | lr 0.00000 0.00003 | loss 0.1425 | s/batch 0.62\n",
      "2020-08-08 12:55:26,820 INFO: | epoch   2 | step 17900 | batch 6650/11250 | lr 0.00000 0.00003 | loss 0.2349 | s/batch 0.78\n",
      "2020-08-08 12:56:04,434 INFO: | epoch   2 | step 17950 | batch 6700/11250 | lr 0.00000 0.00003 | loss 0.2002 | s/batch 0.75\n",
      "2020-08-08 12:56:36,684 INFO: | epoch   2 | step 18000 | batch 6750/11250 | lr 0.00000 0.00003 | loss 0.1604 | s/batch 0.64\n",
      "2020-08-08 12:57:07,529 INFO: | epoch   2 | step 18050 | batch 6800/11250 | lr 0.00000 0.00003 | loss 0.1733 | s/batch 0.62\n",
      "2020-08-08 12:57:42,263 INFO: | epoch   2 | step 18100 | batch 6850/11250 | lr 0.00000 0.00003 | loss 0.1593 | s/batch 0.69\n",
      "2020-08-08 12:58:17,274 INFO: | epoch   2 | step 18150 | batch 6900/11250 | lr 0.00000 0.00003 | loss 0.1821 | s/batch 0.70\n",
      "2020-08-08 12:58:50,909 INFO: | epoch   2 | step 18200 | batch 6950/11250 | lr 0.00000 0.00003 | loss 0.1780 | s/batch 0.67\n",
      "2020-08-08 12:59:25,824 INFO: | epoch   2 | step 18250 | batch 7000/11250 | lr 0.00000 0.00003 | loss 0.1499 | s/batch 0.70\n",
      "2020-08-08 13:00:05,109 INFO: | epoch   2 | step 18300 | batch 7050/11250 | lr 0.00000 0.00003 | loss 0.1980 | s/batch 0.79\n",
      "2020-08-08 13:00:37,184 INFO: | epoch   2 | step 18350 | batch 7100/11250 | lr 0.00000 0.00003 | loss 0.1635 | s/batch 0.64\n",
      "2020-08-08 13:01:09,797 INFO: | epoch   2 | step 18400 | batch 7150/11250 | lr 0.00000 0.00003 | loss 0.1840 | s/batch 0.65\n",
      "2020-08-08 13:01:43,300 INFO: | epoch   2 | step 18450 | batch 7200/11250 | lr 0.00000 0.00003 | loss 0.1600 | s/batch 0.67\n",
      "2020-08-08 13:02:15,627 INFO: | epoch   2 | step 18500 | batch 7250/11250 | lr 0.00000 0.00003 | loss 0.1579 | s/batch 0.65\n",
      "2020-08-08 13:02:45,879 INFO: | epoch   2 | step 18550 | batch 7300/11250 | lr 0.00000 0.00003 | loss 0.1575 | s/batch 0.61\n",
      "2020-08-08 13:03:16,354 INFO: | epoch   2 | step 18600 | batch 7350/11250 | lr 0.00000 0.00003 | loss 0.1791 | s/batch 0.61\n",
      "2020-08-08 13:03:46,503 INFO: | epoch   2 | step 18650 | batch 7400/11250 | lr 0.00000 0.00003 | loss 0.1550 | s/batch 0.60\n",
      "2020-08-08 13:04:18,047 INFO: | epoch   2 | step 18700 | batch 7450/11250 | lr 0.00000 0.00003 | loss 0.1790 | s/batch 0.63\n",
      "2020-08-08 13:04:48,322 INFO: | epoch   2 | step 18750 | batch 7500/11250 | lr 0.00000 0.00003 | loss 0.1341 | s/batch 0.61\n",
      "2020-08-08 13:05:22,146 INFO: | epoch   2 | step 18800 | batch 7550/11250 | lr 0.00000 0.00003 | loss 0.1487 | s/batch 0.68\n",
      "2020-08-08 13:05:55,930 INFO: | epoch   2 | step 18850 | batch 7600/11250 | lr 0.00000 0.00003 | loss 0.1212 | s/batch 0.68\n",
      "2020-08-08 13:06:25,878 INFO: | epoch   2 | step 18900 | batch 7650/11250 | lr 0.00000 0.00003 | loss 0.1837 | s/batch 0.60\n",
      "2020-08-08 13:06:57,683 INFO: | epoch   2 | step 18950 | batch 7700/11250 | lr 0.00000 0.00003 | loss 0.2018 | s/batch 0.64\n",
      "2020-08-08 13:07:32,707 INFO: | epoch   2 | step 19000 | batch 7750/11250 | lr 0.00000 0.00003 | loss 0.1741 | s/batch 0.70\n",
      "2020-08-08 13:08:09,226 INFO: | epoch   2 | step 19050 | batch 7800/11250 | lr 0.00000 0.00003 | loss 0.1601 | s/batch 0.73\n",
      "2020-08-08 13:08:40,172 INFO: | epoch   2 | step 19100 | batch 7850/11250 | lr 0.00000 0.00003 | loss 0.1126 | s/batch 0.62\n",
      "2020-08-08 13:09:13,261 INFO: | epoch   2 | step 19150 | batch 7900/11250 | lr 0.00000 0.00003 | loss 0.1600 | s/batch 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 13:09:44,508 INFO: | epoch   2 | step 19200 | batch 7950/11250 | lr 0.00000 0.00003 | loss 0.1627 | s/batch 0.62\n",
      "2020-08-08 13:10:16,253 INFO: | epoch   2 | step 19250 | batch 8000/11250 | lr 0.00000 0.00003 | loss 0.1674 | s/batch 0.63\n",
      "2020-08-08 13:10:46,054 INFO: | epoch   2 | step 19300 | batch 8050/11250 | lr 0.00000 0.00003 | loss 0.2009 | s/batch 0.60\n",
      "2020-08-08 13:11:16,993 INFO: | epoch   2 | step 19350 | batch 8100/11250 | lr 0.00000 0.00003 | loss 0.1391 | s/batch 0.62\n",
      "2020-08-08 13:11:48,850 INFO: | epoch   2 | step 19400 | batch 8150/11250 | lr 0.00000 0.00003 | loss 0.1686 | s/batch 0.64\n",
      "2020-08-08 13:12:21,127 INFO: | epoch   2 | step 19450 | batch 8200/11250 | lr 0.00000 0.00003 | loss 0.1661 | s/batch 0.65\n",
      "2020-08-08 13:12:53,007 INFO: | epoch   2 | step 19500 | batch 8250/11250 | lr 0.00000 0.00003 | loss 0.1213 | s/batch 0.64\n",
      "2020-08-08 13:13:27,165 INFO: | epoch   2 | step 19550 | batch 8300/11250 | lr 0.00000 0.00003 | loss 0.1853 | s/batch 0.68\n",
      "2020-08-08 13:13:58,571 INFO: | epoch   2 | step 19600 | batch 8350/11250 | lr 0.00000 0.00003 | loss 0.1618 | s/batch 0.63\n",
      "2020-08-08 13:14:29,622 INFO: | epoch   2 | step 19650 | batch 8400/11250 | lr 0.00000 0.00003 | loss 0.1536 | s/batch 0.62\n",
      "2020-08-08 13:14:59,952 INFO: | epoch   2 | step 19700 | batch 8450/11250 | lr 0.00000 0.00003 | loss 0.1380 | s/batch 0.61\n",
      "2020-08-08 13:15:40,944 INFO: | epoch   2 | step 19750 | batch 8500/11250 | lr 0.00000 0.00003 | loss 0.1528 | s/batch 0.82\n",
      "2020-08-08 13:16:22,342 INFO: | epoch   2 | step 19800 | batch 8550/11250 | lr 0.00000 0.00003 | loss 0.1359 | s/batch 0.83\n",
      "2020-08-08 13:17:06,576 INFO: | epoch   2 | step 19850 | batch 8600/11250 | lr 0.00000 0.00003 | loss 0.1440 | s/batch 0.88\n",
      "2020-08-08 13:17:42,989 INFO: | epoch   2 | step 19900 | batch 8650/11250 | lr 0.00000 0.00003 | loss 0.1685 | s/batch 0.73\n",
      "2020-08-08 13:18:26,206 INFO: | epoch   2 | step 19950 | batch 8700/11250 | lr 0.00000 0.00003 | loss 0.1801 | s/batch 0.86\n",
      "2020-08-08 13:19:02,041 INFO: | epoch   2 | step 20000 | batch 8750/11250 | lr 0.00000 0.00003 | loss 0.1201 | s/batch 0.72\n",
      "2020-08-08 13:19:40,176 INFO: | epoch   2 | step 20050 | batch 8800/11250 | lr 0.00000 0.00003 | loss 0.1459 | s/batch 0.76\n",
      "2020-08-08 13:20:16,181 INFO: | epoch   2 | step 20100 | batch 8850/11250 | lr 0.00000 0.00003 | loss 0.1964 | s/batch 0.72\n",
      "2020-08-08 13:20:45,971 INFO: | epoch   2 | step 20150 | batch 8900/11250 | lr 0.00000 0.00003 | loss 0.1523 | s/batch 0.60\n",
      "2020-08-08 13:21:20,827 INFO: | epoch   2 | step 20200 | batch 8950/11250 | lr 0.00000 0.00003 | loss 0.1666 | s/batch 0.70\n",
      "2020-08-08 13:21:53,704 INFO: | epoch   2 | step 20250 | batch 9000/11250 | lr 0.00000 0.00003 | loss 0.1590 | s/batch 0.66\n",
      "2020-08-08 13:22:30,612 INFO: | epoch   2 | step 20300 | batch 9050/11250 | lr 0.00000 0.00003 | loss 0.1920 | s/batch 0.74\n",
      "2020-08-08 13:23:03,668 INFO: | epoch   2 | step 20350 | batch 9100/11250 | lr 0.00000 0.00003 | loss 0.1384 | s/batch 0.66\n",
      "2020-08-08 13:23:37,146 INFO: | epoch   2 | step 20400 | batch 9150/11250 | lr 0.00000 0.00003 | loss 0.1473 | s/batch 0.67\n",
      "2020-08-08 13:24:07,816 INFO: | epoch   2 | step 20450 | batch 9200/11250 | lr 0.00000 0.00003 | loss 0.1239 | s/batch 0.61\n",
      "2020-08-08 13:24:42,826 INFO: | epoch   2 | step 20500 | batch 9250/11250 | lr 0.00000 0.00003 | loss 0.1340 | s/batch 0.70\n",
      "2020-08-08 13:25:16,959 INFO: | epoch   2 | step 20550 | batch 9300/11250 | lr 0.00000 0.00003 | loss 0.1586 | s/batch 0.68\n",
      "2020-08-08 13:25:49,248 INFO: | epoch   2 | step 20600 | batch 9350/11250 | lr 0.00000 0.00003 | loss 0.1261 | s/batch 0.65\n",
      "2020-08-08 13:26:26,827 INFO: | epoch   2 | step 20650 | batch 9400/11250 | lr 0.00000 0.00003 | loss 0.1804 | s/batch 0.75\n",
      "2020-08-08 13:26:56,166 INFO: | epoch   2 | step 20700 | batch 9450/11250 | lr 0.00000 0.00003 | loss 0.1530 | s/batch 0.59\n",
      "2020-08-08 13:27:28,879 INFO: | epoch   2 | step 20750 | batch 9500/11250 | lr 0.00000 0.00003 | loss 0.1537 | s/batch 0.65\n",
      "2020-08-08 13:28:01,548 INFO: | epoch   2 | step 20800 | batch 9550/11250 | lr 0.00000 0.00003 | loss 0.1508 | s/batch 0.65\n",
      "2020-08-08 13:28:30,657 INFO: | epoch   2 | step 20850 | batch 9600/11250 | lr 0.00000 0.00003 | loss 0.1488 | s/batch 0.58\n",
      "2020-08-08 13:29:07,460 INFO: | epoch   2 | step 20900 | batch 9650/11250 | lr 0.00000 0.00003 | loss 0.1814 | s/batch 0.74\n",
      "2020-08-08 13:29:39,805 INFO: | epoch   2 | step 20950 | batch 9700/11250 | lr 0.00000 0.00003 | loss 0.1418 | s/batch 0.65\n",
      "2020-08-08 13:30:13,628 INFO: | epoch   2 | step 21000 | batch 9750/11250 | lr 0.00000 0.00003 | loss 0.1434 | s/batch 0.68\n",
      "2020-08-08 13:30:47,050 INFO: | epoch   2 | step 21050 | batch 9800/11250 | lr 0.00000 0.00003 | loss 0.1385 | s/batch 0.67\n",
      "2020-08-08 13:31:24,983 INFO: | epoch   2 | step 21100 | batch 9850/11250 | lr 0.00000 0.00003 | loss 0.1692 | s/batch 0.76\n",
      "2020-08-08 13:31:58,364 INFO: | epoch   2 | step 21150 | batch 9900/11250 | lr 0.00000 0.00003 | loss 0.1482 | s/batch 0.67\n",
      "2020-08-08 13:32:32,432 INFO: | epoch   2 | step 21200 | batch 9950/11250 | lr 0.00000 0.00003 | loss 0.1624 | s/batch 0.68\n",
      "2020-08-08 13:33:09,620 INFO: | epoch   2 | step 21250 | batch 10000/11250 | lr 0.00000 0.00003 | loss 0.1211 | s/batch 0.74\n",
      "2020-08-08 13:33:40,277 INFO: | epoch   2 | step 21300 | batch 10050/11250 | lr 0.00000 0.00003 | loss 0.1349 | s/batch 0.61\n",
      "2020-08-08 13:34:13,926 INFO: | epoch   2 | step 21350 | batch 10100/11250 | lr 0.00000 0.00003 | loss 0.1834 | s/batch 0.67\n",
      "2020-08-08 13:34:45,772 INFO: | epoch   2 | step 21400 | batch 10150/11250 | lr 0.00000 0.00003 | loss 0.1502 | s/batch 0.64\n",
      "2020-08-08 13:35:19,730 INFO: | epoch   2 | step 21450 | batch 10200/11250 | lr 0.00000 0.00003 | loss 0.1960 | s/batch 0.68\n",
      "2020-08-08 13:35:52,870 INFO: | epoch   2 | step 21500 | batch 10250/11250 | lr 0.00000 0.00003 | loss 0.1688 | s/batch 0.66\n",
      "2020-08-08 13:36:28,570 INFO: | epoch   2 | step 21550 | batch 10300/11250 | lr 0.00000 0.00003 | loss 0.1518 | s/batch 0.71\n",
      "2020-08-08 13:37:01,146 INFO: | epoch   2 | step 21600 | batch 10350/11250 | lr 0.00000 0.00003 | loss 0.1514 | s/batch 0.65\n",
      "2020-08-08 13:37:32,080 INFO: | epoch   2 | step 21650 | batch 10400/11250 | lr 0.00000 0.00003 | loss 0.1492 | s/batch 0.62\n",
      "2020-08-08 13:38:08,621 INFO: | epoch   2 | step 21700 | batch 10450/11250 | lr 0.00000 0.00003 | loss 0.1549 | s/batch 0.73\n",
      "2020-08-08 13:38:43,730 INFO: | epoch   2 | step 21750 | batch 10500/11250 | lr 0.00000 0.00003 | loss 0.1506 | s/batch 0.70\n",
      "2020-08-08 13:39:18,996 INFO: | epoch   2 | step 21800 | batch 10550/11250 | lr 0.00000 0.00003 | loss 0.1223 | s/batch 0.71\n",
      "2020-08-08 13:39:54,358 INFO: | epoch   2 | step 21850 | batch 10600/11250 | lr 0.00000 0.00003 | loss 0.1459 | s/batch 0.71\n",
      "2020-08-08 13:40:32,495 INFO: | epoch   2 | step 21900 | batch 10650/11250 | lr 0.00000 0.00003 | loss 0.0989 | s/batch 0.76\n",
      "2020-08-08 13:41:05,759 INFO: | epoch   2 | step 21950 | batch 10700/11250 | lr 0.00000 0.00003 | loss 0.1626 | s/batch 0.67\n",
      "2020-08-08 13:41:42,300 INFO: | epoch   2 | step 22000 | batch 10750/11250 | lr 0.00000 0.00003 | loss 0.2061 | s/batch 0.73\n",
      "2020-08-08 13:42:20,381 INFO: | epoch   2 | step 22050 | batch 10800/11250 | lr 0.00000 0.00003 | loss 0.1713 | s/batch 0.76\n",
      "2020-08-08 13:42:54,578 INFO: | epoch   2 | step 22100 | batch 10850/11250 | lr 0.00000 0.00003 | loss 0.1521 | s/batch 0.68\n",
      "2020-08-08 13:43:27,064 INFO: | epoch   2 | step 22150 | batch 10900/11250 | lr 0.00000 0.00003 | loss 0.1297 | s/batch 0.65\n",
      "2020-08-08 13:44:02,000 INFO: | epoch   2 | step 22200 | batch 10950/11250 | lr 0.00000 0.00003 | loss 0.1387 | s/batch 0.70\n",
      "2020-08-08 13:44:40,841 INFO: | epoch   2 | step 22250 | batch 11000/11250 | lr 0.00000 0.00003 | loss 0.1848 | s/batch 0.78\n",
      "2020-08-08 13:45:14,477 INFO: | epoch   2 | step 22300 | batch 11050/11250 | lr 0.00000 0.00003 | loss 0.1419 | s/batch 0.67\n",
      "2020-08-08 13:45:51,172 INFO: | epoch   2 | step 22350 | batch 11100/11250 | lr 0.00000 0.00003 | loss 0.1426 | s/batch 0.73\n",
      "2020-08-08 13:46:23,334 INFO: | epoch   2 | step 22400 | batch 11150/11250 | lr 0.00000 0.00003 | loss 0.1834 | s/batch 0.64\n",
      "2020-08-08 13:46:57,938 INFO: | epoch   2 | step 22450 | batch 11200/11250 | lr 0.00000 0.00003 | loss 0.1351 | s/batch 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 13:47:36,257 INFO: | epoch   2 | step 22500 | batch 11250/11250 | lr 0.00000 0.00003 | loss 0.1674 | s/batch 0.77\n",
      "2020-08-08 13:47:36,542 INFO: | epoch   2 | score (94.13, 93.72, 93.92) | f1 93.92 | loss 0.1658 | time 7745.99\n",
      "2020-08-08 13:47:36,924 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9500    0.9501    0.9501     35027\n",
      "          股票     0.9528    0.9559    0.9544     33251\n",
      "          体育     0.9879    0.9882    0.9880     28283\n",
      "          娱乐     0.9624    0.9720    0.9672     19920\n",
      "          时政     0.9132    0.9290    0.9210     13515\n",
      "          社会     0.9118    0.9079    0.9098     11009\n",
      "          教育     0.9530    0.9500    0.9515      8987\n",
      "          财经     0.9015    0.8727    0.8868      7957\n",
      "          家居     0.9392    0.9378    0.9385      7063\n",
      "          游戏     0.9334    0.9170    0.9252      5291\n",
      "          房产     0.9712    0.9591    0.9651      4428\n",
      "          时尚     0.9339    0.9223    0.9280      2818\n",
      "          彩票     0.9480    0.9225    0.9351      1639\n",
      "          星座     0.9201    0.9360    0.9280       812\n",
      "\n",
      "    accuracy                         0.9500    180000\n",
      "   macro avg     0.9413    0.9372    0.9392    180000\n",
      "weighted avg     0.9500    0.9500    0.9500    180000\n",
      "\n",
      "2020-08-08 13:58:01,287 INFO: | epoch   2 | dev | score (94.75, 94.79, 94.72) | f1 94.72 | time 624.36\n",
      "2020-08-08 13:58:01,334 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9598    0.9519    0.9559      3891\n",
      "          股票     0.9569    0.9667    0.9618      3694\n",
      "          体育     0.9836    0.9933    0.9884      3142\n",
      "          娱乐     0.9671    0.9684    0.9677      2213\n",
      "          时政     0.9227    0.9380    0.9303      1501\n",
      "          社会     0.9572    0.8953    0.9252      1223\n",
      "          教育     0.9455    0.9739    0.9595       998\n",
      "          财经     0.9026    0.9016    0.9021       884\n",
      "          家居     0.9554    0.9554    0.9554       784\n",
      "          游戏     0.9319    0.9319    0.9319       587\n",
      "          房产     0.9917    0.9756    0.9836       492\n",
      "          时尚     0.9574    0.9329    0.9450       313\n",
      "          彩票     0.9706    0.9066    0.9375       182\n",
      "          星座     0.8624    0.9792    0.9171        96\n",
      "\n",
      "    accuracy                         0.9569     20000\n",
      "   macro avg     0.9475    0.9479    0.9472     20000\n",
      "weighted avg     0.9570    0.9569    0.9568     20000\n",
      "\n",
      "2020-08-08 13:58:01,335 INFO: Exceed history dev = 94.01, current dev = 94.72\n",
      "2020-08-08 13:58:38,231 INFO: | epoch   3 | step 22550 | batch  50/11250 | lr 0.00000 0.00002 | loss 0.1336 | s/batch 0.74\n",
      "2020-08-08 13:59:14,719 INFO: | epoch   3 | step 22600 | batch 100/11250 | lr 0.00000 0.00002 | loss 0.1688 | s/batch 0.73\n",
      "2020-08-08 13:59:45,368 INFO: | epoch   3 | step 22650 | batch 150/11250 | lr 0.00000 0.00002 | loss 0.1086 | s/batch 0.61\n",
      "2020-08-08 14:00:20,016 INFO: | epoch   3 | step 22700 | batch 200/11250 | lr 0.00000 0.00002 | loss 0.1320 | s/batch 0.69\n",
      "2020-08-08 14:00:52,471 INFO: | epoch   3 | step 22750 | batch 250/11250 | lr 0.00000 0.00002 | loss 0.1966 | s/batch 0.65\n",
      "2020-08-08 14:01:27,352 INFO: | epoch   3 | step 22800 | batch 300/11250 | lr 0.00000 0.00002 | loss 0.1596 | s/batch 0.70\n",
      "2020-08-08 14:01:58,381 INFO: | epoch   3 | step 22850 | batch 350/11250 | lr 0.00000 0.00002 | loss 0.1368 | s/batch 0.62\n",
      "2020-08-08 14:02:33,742 INFO: | epoch   3 | step 22900 | batch 400/11250 | lr 0.00000 0.00002 | loss 0.1199 | s/batch 0.71\n",
      "2020-08-08 14:03:03,532 INFO: | epoch   3 | step 22950 | batch 450/11250 | lr 0.00000 0.00002 | loss 0.1369 | s/batch 0.60\n",
      "2020-08-08 14:03:35,121 INFO: | epoch   3 | step 23000 | batch 500/11250 | lr 0.00000 0.00002 | loss 0.1371 | s/batch 0.63\n",
      "2020-08-08 14:04:11,327 INFO: | epoch   3 | step 23050 | batch 550/11250 | lr 0.00000 0.00002 | loss 0.1680 | s/batch 0.72\n",
      "2020-08-08 14:04:44,032 INFO: | epoch   3 | step 23100 | batch 600/11250 | lr 0.00000 0.00002 | loss 0.1108 | s/batch 0.65\n",
      "2020-08-08 14:05:17,914 INFO: | epoch   3 | step 23150 | batch 650/11250 | lr 0.00000 0.00002 | loss 0.1665 | s/batch 0.68\n",
      "2020-08-08 14:05:53,299 INFO: | epoch   3 | step 23200 | batch 700/11250 | lr 0.00000 0.00002 | loss 0.1578 | s/batch 0.71\n",
      "2020-08-08 14:06:27,236 INFO: | epoch   3 | step 23250 | batch 750/11250 | lr 0.00000 0.00002 | loss 0.1451 | s/batch 0.68\n",
      "2020-08-08 14:07:06,911 INFO: | epoch   3 | step 23300 | batch 800/11250 | lr 0.00000 0.00002 | loss 0.1359 | s/batch 0.79\n",
      "2020-08-08 14:07:44,051 INFO: | epoch   3 | step 23350 | batch 850/11250 | lr 0.00000 0.00002 | loss 0.1123 | s/batch 0.74\n",
      "2020-08-08 14:08:18,664 INFO: | epoch   3 | step 23400 | batch 900/11250 | lr 0.00000 0.00002 | loss 0.1233 | s/batch 0.69\n",
      "2020-08-08 14:08:52,517 INFO: | epoch   3 | step 23450 | batch 950/11250 | lr 0.00000 0.00002 | loss 0.2005 | s/batch 0.68\n",
      "2020-08-08 14:09:27,743 INFO: | epoch   3 | step 23500 | batch 1000/11250 | lr 0.00000 0.00002 | loss 0.1278 | s/batch 0.70\n",
      "2020-08-08 14:10:02,358 INFO: | epoch   3 | step 23550 | batch 1050/11250 | lr 0.00000 0.00002 | loss 0.1209 | s/batch 0.69\n",
      "2020-08-08 14:10:34,668 INFO: | epoch   3 | step 23600 | batch 1100/11250 | lr 0.00000 0.00002 | loss 0.0982 | s/batch 0.65\n",
      "2020-08-08 14:11:08,336 INFO: | epoch   3 | step 23650 | batch 1150/11250 | lr 0.00000 0.00002 | loss 0.1437 | s/batch 0.67\n",
      "2020-08-08 14:11:45,884 INFO: | epoch   3 | step 23700 | batch 1200/11250 | lr 0.00000 0.00002 | loss 0.1081 | s/batch 0.75\n",
      "2020-08-08 14:12:20,099 INFO: | epoch   3 | step 23750 | batch 1250/11250 | lr 0.00000 0.00002 | loss 0.1167 | s/batch 0.68\n",
      "2020-08-08 14:12:50,697 INFO: | epoch   3 | step 23800 | batch 1300/11250 | lr 0.00000 0.00002 | loss 0.1010 | s/batch 0.61\n",
      "2020-08-08 14:13:19,634 INFO: | epoch   3 | step 23850 | batch 1350/11250 | lr 0.00000 0.00002 | loss 0.1222 | s/batch 0.58\n",
      "2020-08-08 14:13:52,135 INFO: | epoch   3 | step 23900 | batch 1400/11250 | lr 0.00000 0.00002 | loss 0.1498 | s/batch 0.65\n",
      "2020-08-08 14:14:29,221 INFO: | epoch   3 | step 23950 | batch 1450/11250 | lr 0.00000 0.00002 | loss 0.1259 | s/batch 0.74\n",
      "2020-08-08 14:15:03,255 INFO: | epoch   3 | step 24000 | batch 1500/11250 | lr 0.00000 0.00002 | loss 0.1333 | s/batch 0.68\n",
      "2020-08-08 14:15:34,918 INFO: | epoch   3 | step 24050 | batch 1550/11250 | lr 0.00000 0.00002 | loss 0.1233 | s/batch 0.63\n",
      "2020-08-08 14:16:08,475 INFO: | epoch   3 | step 24100 | batch 1600/11250 | lr 0.00000 0.00002 | loss 0.1739 | s/batch 0.67\n",
      "2020-08-08 14:16:46,349 INFO: | epoch   3 | step 24150 | batch 1650/11250 | lr 0.00000 0.00002 | loss 0.1404 | s/batch 0.76\n",
      "2020-08-08 14:17:19,354 INFO: | epoch   3 | step 24200 | batch 1700/11250 | lr 0.00000 0.00002 | loss 0.0943 | s/batch 0.66\n",
      "2020-08-08 14:17:49,796 INFO: | epoch   3 | step 24250 | batch 1750/11250 | lr 0.00000 0.00002 | loss 0.1146 | s/batch 0.61\n",
      "2020-08-08 14:18:26,074 INFO: | epoch   3 | step 24300 | batch 1800/11250 | lr 0.00000 0.00002 | loss 0.1279 | s/batch 0.73\n",
      "2020-08-08 14:19:00,456 INFO: | epoch   3 | step 24350 | batch 1850/11250 | lr 0.00000 0.00002 | loss 0.1366 | s/batch 0.69\n",
      "2020-08-08 14:19:35,706 INFO: | epoch   3 | step 24400 | batch 1900/11250 | lr 0.00000 0.00002 | loss 0.1452 | s/batch 0.70\n",
      "2020-08-08 14:20:08,678 INFO: | epoch   3 | step 24450 | batch 1950/11250 | lr 0.00000 0.00002 | loss 0.1055 | s/batch 0.66\n",
      "2020-08-08 14:20:40,737 INFO: | epoch   3 | step 24500 | batch 2000/11250 | lr 0.00000 0.00002 | loss 0.1302 | s/batch 0.64\n",
      "2020-08-08 14:21:09,633 INFO: | epoch   3 | step 24550 | batch 2050/11250 | lr 0.00000 0.00002 | loss 0.1403 | s/batch 0.58\n",
      "2020-08-08 14:21:40,217 INFO: | epoch   3 | step 24600 | batch 2100/11250 | lr 0.00000 0.00002 | loss 0.0940 | s/batch 0.61\n",
      "2020-08-08 14:22:19,265 INFO: | epoch   3 | step 24650 | batch 2150/11250 | lr 0.00000 0.00002 | loss 0.1315 | s/batch 0.78\n",
      "2020-08-08 14:22:53,124 INFO: | epoch   3 | step 24700 | batch 2200/11250 | lr 0.00000 0.00002 | loss 0.1544 | s/batch 0.68\n",
      "2020-08-08 14:23:32,365 INFO: | epoch   3 | step 24750 | batch 2250/11250 | lr 0.00000 0.00002 | loss 0.1464 | s/batch 0.78\n",
      "2020-08-08 14:24:06,927 INFO: | epoch   3 | step 24800 | batch 2300/11250 | lr 0.00000 0.00002 | loss 0.1068 | s/batch 0.69\n",
      "2020-08-08 14:24:41,463 INFO: | epoch   3 | step 24850 | batch 2350/11250 | lr 0.00000 0.00002 | loss 0.1167 | s/batch 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 14:25:09,888 INFO: | epoch   3 | step 24900 | batch 2400/11250 | lr 0.00000 0.00002 | loss 0.1549 | s/batch 0.57\n",
      "2020-08-08 14:25:43,021 INFO: | epoch   3 | step 24950 | batch 2450/11250 | lr 0.00000 0.00002 | loss 0.1185 | s/batch 0.66\n",
      "2020-08-08 14:26:16,789 INFO: | epoch   3 | step 25000 | batch 2500/11250 | lr 0.00000 0.00002 | loss 0.1916 | s/batch 0.68\n",
      "2020-08-08 14:26:52,456 INFO: | epoch   3 | step 25050 | batch 2550/11250 | lr 0.00000 0.00002 | loss 0.1315 | s/batch 0.71\n",
      "2020-08-08 14:27:26,952 INFO: | epoch   3 | step 25100 | batch 2600/11250 | lr 0.00000 0.00002 | loss 0.1208 | s/batch 0.69\n",
      "2020-08-08 14:28:01,346 INFO: | epoch   3 | step 25150 | batch 2650/11250 | lr 0.00000 0.00002 | loss 0.1504 | s/batch 0.69\n",
      "2020-08-08 14:28:38,035 INFO: | epoch   3 | step 25200 | batch 2700/11250 | lr 0.00000 0.00002 | loss 0.1240 | s/batch 0.73\n",
      "2020-08-08 14:29:14,250 INFO: | epoch   3 | step 25250 | batch 2750/11250 | lr 0.00000 0.00002 | loss 0.1421 | s/batch 0.72\n",
      "2020-08-08 14:29:46,389 INFO: | epoch   3 | step 25300 | batch 2800/11250 | lr 0.00000 0.00002 | loss 0.1564 | s/batch 0.64\n",
      "2020-08-08 14:30:18,225 INFO: | epoch   3 | step 25350 | batch 2850/11250 | lr 0.00000 0.00002 | loss 0.1535 | s/batch 0.64\n",
      "2020-08-08 14:30:48,445 INFO: | epoch   3 | step 25400 | batch 2900/11250 | lr 0.00000 0.00002 | loss 0.1198 | s/batch 0.60\n",
      "2020-08-08 14:31:19,777 INFO: | epoch   3 | step 25450 | batch 2950/11250 | lr 0.00000 0.00002 | loss 0.1119 | s/batch 0.63\n",
      "2020-08-08 14:31:59,535 INFO: | epoch   3 | step 25500 | batch 3000/11250 | lr 0.00000 0.00002 | loss 0.1480 | s/batch 0.80\n",
      "2020-08-08 14:32:32,264 INFO: | epoch   3 | step 25550 | batch 3050/11250 | lr 0.00000 0.00002 | loss 0.1132 | s/batch 0.65\n",
      "2020-08-08 14:33:05,114 INFO: | epoch   3 | step 25600 | batch 3100/11250 | lr 0.00000 0.00002 | loss 0.1471 | s/batch 0.66\n",
      "2020-08-08 14:33:38,752 INFO: | epoch   3 | step 25650 | batch 3150/11250 | lr 0.00000 0.00002 | loss 0.1381 | s/batch 0.67\n",
      "2020-08-08 14:34:12,608 INFO: | epoch   3 | step 25700 | batch 3200/11250 | lr 0.00000 0.00002 | loss 0.1127 | s/batch 0.68\n",
      "2020-08-08 14:34:48,168 INFO: | epoch   3 | step 25750 | batch 3250/11250 | lr 0.00000 0.00002 | loss 0.1369 | s/batch 0.71\n",
      "2020-08-08 14:35:17,650 INFO: | epoch   3 | step 25800 | batch 3300/11250 | lr 0.00000 0.00002 | loss 0.0957 | s/batch 0.59\n",
      "2020-08-08 14:35:52,932 INFO: | epoch   3 | step 25850 | batch 3350/11250 | lr 0.00000 0.00002 | loss 0.1309 | s/batch 0.71\n",
      "2020-08-08 14:36:23,823 INFO: | epoch   3 | step 25900 | batch 3400/11250 | lr 0.00000 0.00002 | loss 0.1250 | s/batch 0.62\n",
      "2020-08-08 14:36:55,246 INFO: | epoch   3 | step 25950 | batch 3450/11250 | lr 0.00000 0.00002 | loss 0.1104 | s/batch 0.63\n",
      "2020-08-08 14:37:29,745 INFO: | epoch   3 | step 26000 | batch 3500/11250 | lr 0.00000 0.00002 | loss 0.1145 | s/batch 0.69\n",
      "2020-08-08 14:37:59,172 INFO: | epoch   3 | step 26050 | batch 3550/11250 | lr 0.00000 0.00002 | loss 0.1079 | s/batch 0.59\n",
      "2020-08-08 14:38:32,630 INFO: | epoch   3 | step 26100 | batch 3600/11250 | lr 0.00000 0.00002 | loss 0.1449 | s/batch 0.67\n",
      "2020-08-08 14:39:06,875 INFO: | epoch   3 | step 26150 | batch 3650/11250 | lr 0.00000 0.00002 | loss 0.1765 | s/batch 0.68\n",
      "2020-08-08 14:39:40,508 INFO: | epoch   3 | step 26200 | batch 3700/11250 | lr 0.00000 0.00002 | loss 0.1713 | s/batch 0.67\n",
      "2020-08-08 14:40:14,035 INFO: | epoch   3 | step 26250 | batch 3750/11250 | lr 0.00000 0.00002 | loss 0.1097 | s/batch 0.67\n",
      "2020-08-08 14:40:48,704 INFO: | epoch   3 | step 26300 | batch 3800/11250 | lr 0.00000 0.00002 | loss 0.1222 | s/batch 0.69\n",
      "2020-08-08 14:41:21,307 INFO: | epoch   3 | step 26350 | batch 3850/11250 | lr 0.00000 0.00002 | loss 0.1333 | s/batch 0.65\n",
      "2020-08-08 14:41:58,099 INFO: | epoch   3 | step 26400 | batch 3900/11250 | lr 0.00000 0.00002 | loss 0.1607 | s/batch 0.74\n",
      "2020-08-08 14:42:33,489 INFO: | epoch   3 | step 26450 | batch 3950/11250 | lr 0.00000 0.00002 | loss 0.1512 | s/batch 0.71\n",
      "2020-08-08 14:43:00,621 INFO: | epoch   3 | step 26500 | batch 4000/11250 | lr 0.00000 0.00002 | loss 0.1122 | s/batch 0.54\n",
      "2020-08-08 14:43:34,887 INFO: | epoch   3 | step 26550 | batch 4050/11250 | lr 0.00000 0.00002 | loss 0.1271 | s/batch 0.69\n",
      "2020-08-08 14:44:03,968 INFO: | epoch   3 | step 26600 | batch 4100/11250 | lr 0.00000 0.00002 | loss 0.1139 | s/batch 0.58\n",
      "2020-08-08 14:44:33,059 INFO: | epoch   3 | step 26650 | batch 4150/11250 | lr 0.00000 0.00002 | loss 0.1402 | s/batch 0.58\n",
      "2020-08-08 14:45:05,876 INFO: | epoch   3 | step 26700 | batch 4200/11250 | lr 0.00000 0.00002 | loss 0.1281 | s/batch 0.66\n",
      "2020-08-08 14:45:37,914 INFO: | epoch   3 | step 26750 | batch 4250/11250 | lr 0.00000 0.00002 | loss 0.1632 | s/batch 0.64\n",
      "2020-08-08 14:46:13,344 INFO: | epoch   3 | step 26800 | batch 4300/11250 | lr 0.00000 0.00002 | loss 0.1077 | s/batch 0.71\n",
      "2020-08-08 14:46:46,434 INFO: | epoch   3 | step 26850 | batch 4350/11250 | lr 0.00000 0.00002 | loss 0.1530 | s/batch 0.66\n",
      "2020-08-08 14:47:19,086 INFO: | epoch   3 | step 26900 | batch 4400/11250 | lr 0.00000 0.00002 | loss 0.1556 | s/batch 0.65\n",
      "2020-08-08 14:47:53,477 INFO: | epoch   3 | step 26950 | batch 4450/11250 | lr 0.00000 0.00002 | loss 0.1556 | s/batch 0.69\n",
      "2020-08-08 14:48:29,922 INFO: | epoch   3 | step 27000 | batch 4500/11250 | lr 0.00000 0.00002 | loss 0.1272 | s/batch 0.73\n",
      "2020-08-08 14:49:05,036 INFO: | epoch   3 | step 27050 | batch 4550/11250 | lr 0.00000 0.00002 | loss 0.1362 | s/batch 0.70\n",
      "2020-08-08 14:49:33,724 INFO: | epoch   3 | step 27100 | batch 4600/11250 | lr 0.00000 0.00002 | loss 0.1426 | s/batch 0.57\n",
      "2020-08-08 14:50:06,776 INFO: | epoch   3 | step 27150 | batch 4650/11250 | lr 0.00000 0.00002 | loss 0.1309 | s/batch 0.66\n",
      "2020-08-08 14:50:42,534 INFO: | epoch   3 | step 27200 | batch 4700/11250 | lr 0.00000 0.00002 | loss 0.1034 | s/batch 0.72\n",
      "2020-08-08 14:51:18,115 INFO: | epoch   3 | step 27250 | batch 4750/11250 | lr 0.00000 0.00002 | loss 0.1571 | s/batch 0.71\n",
      "2020-08-08 14:51:49,262 INFO: | epoch   3 | step 27300 | batch 4800/11250 | lr 0.00000 0.00002 | loss 0.0907 | s/batch 0.62\n",
      "2020-08-08 14:52:27,257 INFO: | epoch   3 | step 27350 | batch 4850/11250 | lr 0.00000 0.00002 | loss 0.1273 | s/batch 0.76\n",
      "2020-08-08 14:53:05,866 INFO: | epoch   3 | step 27400 | batch 4900/11250 | lr 0.00000 0.00002 | loss 0.1484 | s/batch 0.77\n",
      "2020-08-08 14:53:42,120 INFO: | epoch   3 | step 27450 | batch 4950/11250 | lr 0.00000 0.00002 | loss 0.1556 | s/batch 0.73\n",
      "2020-08-08 14:54:17,742 INFO: | epoch   3 | step 27500 | batch 5000/11250 | lr 0.00000 0.00002 | loss 0.1412 | s/batch 0.71\n",
      "2020-08-08 14:54:52,608 INFO: | epoch   3 | step 27550 | batch 5050/11250 | lr 0.00000 0.00002 | loss 0.1050 | s/batch 0.70\n",
      "2020-08-08 14:55:25,073 INFO: | epoch   3 | step 27600 | batch 5100/11250 | lr 0.00000 0.00002 | loss 0.1421 | s/batch 0.65\n",
      "2020-08-08 14:56:00,127 INFO: | epoch   3 | step 27650 | batch 5150/11250 | lr 0.00000 0.00002 | loss 0.0835 | s/batch 0.70\n",
      "2020-08-08 14:56:29,946 INFO: | epoch   3 | step 27700 | batch 5200/11250 | lr 0.00000 0.00002 | loss 0.1180 | s/batch 0.60\n",
      "2020-08-08 14:57:03,847 INFO: | epoch   3 | step 27750 | batch 5250/11250 | lr 0.00000 0.00002 | loss 0.1318 | s/batch 0.68\n",
      "2020-08-08 14:57:36,539 INFO: | epoch   3 | step 27800 | batch 5300/11250 | lr 0.00000 0.00002 | loss 0.1081 | s/batch 0.65\n",
      "2020-08-08 14:58:11,551 INFO: | epoch   3 | step 27850 | batch 5350/11250 | lr 0.00000 0.00002 | loss 0.1232 | s/batch 0.70\n",
      "2020-08-08 14:58:42,826 INFO: | epoch   3 | step 27900 | batch 5400/11250 | lr 0.00000 0.00002 | loss 0.1221 | s/batch 0.63\n",
      "2020-08-08 14:59:50,260 INFO: | epoch   3 | step 27950 | batch 5450/11250 | lr 0.00000 0.00002 | loss 0.1435 | s/batch 1.35\n",
      "2020-08-08 15:00:22,873 INFO: | epoch   3 | step 28000 | batch 5500/11250 | lr 0.00000 0.00002 | loss 0.1109 | s/batch 0.65\n",
      "2020-08-08 15:00:52,214 INFO: | epoch   3 | step 28050 | batch 5550/11250 | lr 0.00000 0.00002 | loss 0.1769 | s/batch 0.59\n",
      "2020-08-08 15:01:26,489 INFO: | epoch   3 | step 28100 | batch 5600/11250 | lr 0.00000 0.00002 | loss 0.1319 | s/batch 0.69\n",
      "2020-08-08 15:01:57,458 INFO: | epoch   3 | step 28150 | batch 5650/11250 | lr 0.00000 0.00002 | loss 0.1129 | s/batch 0.62\n",
      "2020-08-08 15:02:32,331 INFO: | epoch   3 | step 28200 | batch 5700/11250 | lr 0.00000 0.00002 | loss 0.1579 | s/batch 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 15:03:03,401 INFO: | epoch   3 | step 28250 | batch 5750/11250 | lr 0.00000 0.00002 | loss 0.0946 | s/batch 0.62\n",
      "2020-08-08 15:03:40,931 INFO: | epoch   3 | step 28300 | batch 5800/11250 | lr 0.00000 0.00002 | loss 0.1564 | s/batch 0.75\n",
      "2020-08-08 15:04:16,555 INFO: | epoch   3 | step 28350 | batch 5850/11250 | lr 0.00000 0.00002 | loss 0.1019 | s/batch 0.71\n",
      "2020-08-08 15:04:48,075 INFO: | epoch   3 | step 28400 | batch 5900/11250 | lr 0.00000 0.00002 | loss 0.1353 | s/batch 0.63\n",
      "2020-08-08 15:05:23,221 INFO: | epoch   3 | step 28450 | batch 5950/11250 | lr 0.00000 0.00002 | loss 0.1081 | s/batch 0.70\n",
      "2020-08-08 15:05:57,864 INFO: | epoch   3 | step 28500 | batch 6000/11250 | lr 0.00000 0.00002 | loss 0.1346 | s/batch 0.69\n",
      "2020-08-08 15:06:32,380 INFO: | epoch   3 | step 28550 | batch 6050/11250 | lr 0.00000 0.00002 | loss 0.1664 | s/batch 0.69\n",
      "2020-08-08 15:07:05,112 INFO: | epoch   3 | step 28600 | batch 6100/11250 | lr 0.00000 0.00002 | loss 0.1484 | s/batch 0.65\n",
      "2020-08-08 15:07:38,278 INFO: | epoch   3 | step 28650 | batch 6150/11250 | lr 0.00000 0.00002 | loss 0.1254 | s/batch 0.66\n",
      "2020-08-08 15:08:14,281 INFO: | epoch   3 | step 28700 | batch 6200/11250 | lr 0.00000 0.00002 | loss 0.1775 | s/batch 0.72\n",
      "2020-08-08 15:08:50,526 INFO: | epoch   3 | step 28750 | batch 6250/11250 | lr 0.00000 0.00002 | loss 0.1257 | s/batch 0.72\n",
      "2020-08-08 15:09:21,989 INFO: | epoch   3 | step 28800 | batch 6300/11250 | lr 0.00000 0.00002 | loss 0.1340 | s/batch 0.63\n",
      "2020-08-08 15:09:56,593 INFO: | epoch   3 | step 28850 | batch 6350/11250 | lr 0.00000 0.00002 | loss 0.0919 | s/batch 0.69\n",
      "2020-08-08 15:10:26,831 INFO: | epoch   3 | step 28900 | batch 6400/11250 | lr 0.00000 0.00002 | loss 0.1265 | s/batch 0.60\n",
      "2020-08-08 15:10:58,762 INFO: | epoch   3 | step 28950 | batch 6450/11250 | lr 0.00000 0.00002 | loss 0.1366 | s/batch 0.64\n",
      "2020-08-08 15:11:32,712 INFO: | epoch   3 | step 29000 | batch 6500/11250 | lr 0.00000 0.00002 | loss 0.1116 | s/batch 0.68\n",
      "2020-08-08 15:12:04,302 INFO: | epoch   3 | step 29050 | batch 6550/11250 | lr 0.00000 0.00002 | loss 0.1338 | s/batch 0.63\n",
      "2020-08-08 15:12:38,791 INFO: | epoch   3 | step 29100 | batch 6600/11250 | lr 0.00000 0.00002 | loss 0.1036 | s/batch 0.69\n",
      "2020-08-08 15:13:16,426 INFO: | epoch   3 | step 29150 | batch 6650/11250 | lr 0.00000 0.00002 | loss 0.1409 | s/batch 0.75\n",
      "2020-08-08 15:13:48,302 INFO: | epoch   3 | step 29200 | batch 6700/11250 | lr 0.00000 0.00002 | loss 0.1237 | s/batch 0.64\n",
      "2020-08-08 15:14:22,215 INFO: | epoch   3 | step 29250 | batch 6750/11250 | lr 0.00000 0.00002 | loss 0.1144 | s/batch 0.68\n",
      "2020-08-08 15:15:00,111 INFO: | epoch   3 | step 29300 | batch 6800/11250 | lr 0.00000 0.00002 | loss 0.1492 | s/batch 0.76\n",
      "2020-08-08 15:15:34,553 INFO: | epoch   3 | step 29350 | batch 6850/11250 | lr 0.00000 0.00002 | loss 0.1411 | s/batch 0.69\n",
      "2020-08-08 15:16:06,441 INFO: | epoch   3 | step 29400 | batch 6900/11250 | lr 0.00000 0.00002 | loss 0.1370 | s/batch 0.64\n",
      "2020-08-08 15:16:42,194 INFO: | epoch   3 | step 29450 | batch 6950/11250 | lr 0.00000 0.00002 | loss 0.1174 | s/batch 0.72\n",
      "2020-08-08 15:17:14,494 INFO: | epoch   3 | step 29500 | batch 7000/11250 | lr 0.00000 0.00002 | loss 0.1722 | s/batch 0.65\n",
      "2020-08-08 15:17:53,776 INFO: | epoch   3 | step 29550 | batch 7050/11250 | lr 0.00000 0.00002 | loss 0.1363 | s/batch 0.79\n",
      "2020-08-08 15:18:23,286 INFO: | epoch   3 | step 29600 | batch 7100/11250 | lr 0.00000 0.00002 | loss 0.1199 | s/batch 0.59\n",
      "2020-08-08 15:18:51,873 INFO: | epoch   3 | step 29650 | batch 7150/11250 | lr 0.00000 0.00002 | loss 0.1195 | s/batch 0.57\n",
      "2020-08-08 15:19:27,828 INFO: | epoch   3 | step 29700 | batch 7200/11250 | lr 0.00000 0.00002 | loss 0.1407 | s/batch 0.72\n",
      "2020-08-08 15:20:03,908 INFO: | epoch   3 | step 29750 | batch 7250/11250 | lr 0.00000 0.00002 | loss 0.1570 | s/batch 0.72\n",
      "2020-08-08 15:20:36,277 INFO: | epoch   3 | step 29800 | batch 7300/11250 | lr 0.00000 0.00002 | loss 0.1065 | s/batch 0.65\n",
      "2020-08-08 15:21:11,924 INFO: | epoch   3 | step 29850 | batch 7350/11250 | lr 0.00000 0.00002 | loss 0.1294 | s/batch 0.71\n",
      "2020-08-08 15:21:43,975 INFO: | epoch   3 | step 29900 | batch 7400/11250 | lr 0.00000 0.00002 | loss 0.1340 | s/batch 0.64\n",
      "2020-08-08 15:22:20,684 INFO: | epoch   3 | step 29950 | batch 7450/11250 | lr 0.00000 0.00002 | loss 0.1242 | s/batch 0.73\n",
      "2020-08-08 15:22:55,851 INFO: | epoch   3 | step 30000 | batch 7500/11250 | lr 0.00000 0.00002 | loss 0.1170 | s/batch 0.70\n",
      "2020-08-08 15:23:25,216 INFO: | epoch   3 | step 30050 | batch 7550/11250 | lr 0.00000 0.00002 | loss 0.0892 | s/batch 0.59\n",
      "2020-08-08 15:24:01,913 INFO: | epoch   3 | step 30100 | batch 7600/11250 | lr 0.00000 0.00002 | loss 0.1444 | s/batch 0.73\n",
      "2020-08-08 15:24:34,946 INFO: | epoch   3 | step 30150 | batch 7650/11250 | lr 0.00000 0.00002 | loss 0.1060 | s/batch 0.66\n",
      "2020-08-08 15:25:06,987 INFO: | epoch   3 | step 30200 | batch 7700/11250 | lr 0.00000 0.00002 | loss 0.1422 | s/batch 0.64\n",
      "2020-08-08 15:25:42,205 INFO: | epoch   3 | step 30250 | batch 7750/11250 | lr 0.00000 0.00002 | loss 0.1529 | s/batch 0.70\n",
      "2020-08-08 15:26:15,636 INFO: | epoch   3 | step 30300 | batch 7800/11250 | lr 0.00000 0.00002 | loss 0.1478 | s/batch 0.67\n",
      "2020-08-08 15:26:51,170 INFO: | epoch   3 | step 30350 | batch 7850/11250 | lr 0.00000 0.00002 | loss 0.1009 | s/batch 0.71\n",
      "2020-08-08 15:27:27,993 INFO: | epoch   3 | step 30400 | batch 7900/11250 | lr 0.00000 0.00002 | loss 0.1253 | s/batch 0.74\n",
      "2020-08-08 15:28:06,855 INFO: | epoch   3 | step 30450 | batch 7950/11250 | lr 0.00000 0.00002 | loss 0.0863 | s/batch 0.78\n",
      "2020-08-08 15:28:41,466 INFO: | epoch   3 | step 30500 | batch 8000/11250 | lr 0.00000 0.00002 | loss 0.1347 | s/batch 0.69\n",
      "2020-08-08 15:29:18,261 INFO: | epoch   3 | step 30550 | batch 8050/11250 | lr 0.00000 0.00002 | loss 0.1464 | s/batch 0.74\n",
      "2020-08-08 15:29:49,150 INFO: | epoch   3 | step 30600 | batch 8100/11250 | lr 0.00000 0.00002 | loss 0.1097 | s/batch 0.62\n",
      "2020-08-08 15:30:23,592 INFO: | epoch   3 | step 30650 | batch 8150/11250 | lr 0.00000 0.00002 | loss 0.1441 | s/batch 0.69\n",
      "2020-08-08 15:30:55,592 INFO: | epoch   3 | step 30700 | batch 8200/11250 | lr 0.00000 0.00002 | loss 0.1445 | s/batch 0.64\n",
      "2020-08-08 15:31:27,734 INFO: | epoch   3 | step 30750 | batch 8250/11250 | lr 0.00000 0.00002 | loss 0.1241 | s/batch 0.64\n",
      "2020-08-08 15:32:06,300 INFO: | epoch   3 | step 30800 | batch 8300/11250 | lr 0.00000 0.00002 | loss 0.1456 | s/batch 0.77\n",
      "2020-08-08 15:32:40,299 INFO: | epoch   3 | step 30850 | batch 8350/11250 | lr 0.00000 0.00002 | loss 0.1456 | s/batch 0.68\n",
      "2020-08-08 15:33:15,399 INFO: | epoch   3 | step 30900 | batch 8400/11250 | lr 0.00000 0.00002 | loss 0.0951 | s/batch 0.70\n",
      "2020-08-08 15:33:49,546 INFO: | epoch   3 | step 30950 | batch 8450/11250 | lr 0.00000 0.00002 | loss 0.1211 | s/batch 0.68\n",
      "2020-08-08 15:34:26,637 INFO: | epoch   3 | step 31000 | batch 8500/11250 | lr 0.00000 0.00002 | loss 0.1380 | s/batch 0.74\n",
      "2020-08-08 15:35:01,107 INFO: | epoch   3 | step 31050 | batch 8550/11250 | lr 0.00000 0.00002 | loss 0.1127 | s/batch 0.69\n",
      "2020-08-08 15:35:37,176 INFO: | epoch   3 | step 31100 | batch 8600/11250 | lr 0.00000 0.00002 | loss 0.1363 | s/batch 0.72\n",
      "2020-08-08 15:36:11,558 INFO: | epoch   3 | step 31150 | batch 8650/11250 | lr 0.00000 0.00002 | loss 0.1384 | s/batch 0.69\n",
      "2020-08-08 15:36:43,496 INFO: | epoch   3 | step 31200 | batch 8700/11250 | lr 0.00000 0.00002 | loss 0.1402 | s/batch 0.64\n",
      "2020-08-08 15:37:17,697 INFO: | epoch   3 | step 31250 | batch 8750/11250 | lr 0.00000 0.00002 | loss 0.0994 | s/batch 0.68\n",
      "2020-08-08 15:37:55,860 INFO: | epoch   3 | step 31300 | batch 8800/11250 | lr 0.00000 0.00002 | loss 0.1809 | s/batch 0.76\n",
      "2020-08-08 15:38:28,243 INFO: | epoch   3 | step 31350 | batch 8850/11250 | lr 0.00000 0.00002 | loss 0.1023 | s/batch 0.65\n",
      "2020-08-08 15:39:02,178 INFO: | epoch   3 | step 31400 | batch 8900/11250 | lr 0.00000 0.00002 | loss 0.1540 | s/batch 0.68\n",
      "2020-08-08 15:39:32,660 INFO: | epoch   3 | step 31450 | batch 8950/11250 | lr 0.00000 0.00002 | loss 0.1398 | s/batch 0.61\n",
      "2020-08-08 15:40:07,888 INFO: | epoch   3 | step 31500 | batch 9000/11250 | lr 0.00000 0.00002 | loss 0.1015 | s/batch 0.70\n",
      "2020-08-08 15:40:44,531 INFO: | epoch   3 | step 31550 | batch 9050/11250 | lr 0.00000 0.00001 | loss 0.1343 | s/batch 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 15:41:20,955 INFO: | epoch   3 | step 31600 | batch 9100/11250 | lr 0.00000 0.00001 | loss 0.1692 | s/batch 0.73\n",
      "2020-08-08 15:41:53,075 INFO: | epoch   3 | step 31650 | batch 9150/11250 | lr 0.00000 0.00001 | loss 0.1400 | s/batch 0.64\n",
      "2020-08-08 15:42:27,456 INFO: | epoch   3 | step 31700 | batch 9200/11250 | lr 0.00000 0.00001 | loss 0.1644 | s/batch 0.69\n",
      "2020-08-08 15:42:58,999 INFO: | epoch   3 | step 31750 | batch 9250/11250 | lr 0.00000 0.00001 | loss 0.1653 | s/batch 0.63\n",
      "2020-08-08 15:43:30,598 INFO: | epoch   3 | step 31800 | batch 9300/11250 | lr 0.00000 0.00001 | loss 0.1194 | s/batch 0.63\n",
      "2020-08-08 15:44:03,072 INFO: | epoch   3 | step 31850 | batch 9350/11250 | lr 0.00000 0.00001 | loss 0.1500 | s/batch 0.65\n",
      "2020-08-08 15:44:32,103 INFO: | epoch   3 | step 31900 | batch 9400/11250 | lr 0.00000 0.00001 | loss 0.1190 | s/batch 0.58\n",
      "2020-08-08 15:45:10,009 INFO: | epoch   3 | step 31950 | batch 9450/11250 | lr 0.00000 0.00001 | loss 0.1252 | s/batch 0.76\n",
      "2020-08-08 15:45:43,363 INFO: | epoch   3 | step 32000 | batch 9500/11250 | lr 0.00000 0.00001 | loss 0.0983 | s/batch 0.67\n",
      "2020-08-08 15:46:16,477 INFO: | epoch   3 | step 32050 | batch 9550/11250 | lr 0.00000 0.00001 | loss 0.1064 | s/batch 0.66\n",
      "2020-08-08 15:46:49,516 INFO: | epoch   3 | step 32100 | batch 9600/11250 | lr 0.00000 0.00001 | loss 0.1209 | s/batch 0.66\n",
      "2020-08-08 15:47:24,477 INFO: | epoch   3 | step 32150 | batch 9650/11250 | lr 0.00000 0.00001 | loss 0.1232 | s/batch 0.70\n",
      "2020-08-08 15:47:58,584 INFO: | epoch   3 | step 32200 | batch 9700/11250 | lr 0.00000 0.00001 | loss 0.1465 | s/batch 0.68\n",
      "2020-08-08 15:48:32,011 INFO: | epoch   3 | step 32250 | batch 9750/11250 | lr 0.00000 0.00001 | loss 0.1476 | s/batch 0.67\n",
      "2020-08-08 15:49:06,802 INFO: | epoch   3 | step 32300 | batch 9800/11250 | lr 0.00000 0.00001 | loss 0.1444 | s/batch 0.70\n",
      "2020-08-08 15:49:42,921 INFO: | epoch   3 | step 32350 | batch 9850/11250 | lr 0.00000 0.00001 | loss 0.1437 | s/batch 0.72\n",
      "2020-08-08 15:50:15,077 INFO: | epoch   3 | step 32400 | batch 9900/11250 | lr 0.00000 0.00001 | loss 0.1040 | s/batch 0.64\n",
      "2020-08-08 15:50:49,576 INFO: | epoch   3 | step 32450 | batch 9950/11250 | lr 0.00000 0.00001 | loss 0.1549 | s/batch 0.69\n",
      "2020-08-08 15:51:26,146 INFO: | epoch   3 | step 32500 | batch 10000/11250 | lr 0.00000 0.00001 | loss 0.1148 | s/batch 0.73\n",
      "2020-08-08 15:51:59,779 INFO: | epoch   3 | step 32550 | batch 10050/11250 | lr 0.00000 0.00001 | loss 0.1074 | s/batch 0.67\n",
      "2020-08-08 15:52:32,835 INFO: | epoch   3 | step 32600 | batch 10100/11250 | lr 0.00000 0.00001 | loss 0.1267 | s/batch 0.66\n",
      "2020-08-08 15:53:06,034 INFO: | epoch   3 | step 32650 | batch 10150/11250 | lr 0.00000 0.00001 | loss 0.1383 | s/batch 0.66\n",
      "2020-08-08 15:53:46,185 INFO: | epoch   3 | step 32700 | batch 10200/11250 | lr 0.00000 0.00001 | loss 0.1349 | s/batch 0.80\n",
      "2020-08-08 15:54:18,086 INFO: | epoch   3 | step 32750 | batch 10250/11250 | lr 0.00000 0.00001 | loss 0.1181 | s/batch 0.64\n",
      "2020-08-08 15:54:50,531 INFO: | epoch   3 | step 32800 | batch 10300/11250 | lr 0.00000 0.00001 | loss 0.1073 | s/batch 0.65\n",
      "2020-08-08 15:55:27,598 INFO: | epoch   3 | step 32850 | batch 10350/11250 | lr 0.00000 0.00001 | loss 0.1314 | s/batch 0.74\n",
      "2020-08-08 15:56:01,855 INFO: | epoch   3 | step 32900 | batch 10400/11250 | lr 0.00000 0.00001 | loss 0.1426 | s/batch 0.69\n",
      "2020-08-08 15:56:33,673 INFO: | epoch   3 | step 32950 | batch 10450/11250 | lr 0.00000 0.00001 | loss 0.1186 | s/batch 0.64\n",
      "2020-08-08 15:57:06,705 INFO: | epoch   3 | step 33000 | batch 10500/11250 | lr 0.00000 0.00001 | loss 0.1335 | s/batch 0.66\n",
      "2020-08-08 15:57:37,975 INFO: | epoch   3 | step 33050 | batch 10550/11250 | lr 0.00000 0.00001 | loss 0.1078 | s/batch 0.63\n",
      "2020-08-08 15:58:12,650 INFO: | epoch   3 | step 33100 | batch 10600/11250 | lr 0.00000 0.00001 | loss 0.1807 | s/batch 0.69\n",
      "2020-08-08 15:58:44,514 INFO: | epoch   3 | step 33150 | batch 10650/11250 | lr 0.00000 0.00001 | loss 0.1374 | s/batch 0.64\n",
      "2020-08-08 15:59:16,696 INFO: | epoch   3 | step 33200 | batch 10700/11250 | lr 0.00000 0.00001 | loss 0.1554 | s/batch 0.64\n",
      "2020-08-08 15:59:53,092 INFO: | epoch   3 | step 33250 | batch 10750/11250 | lr 0.00000 0.00001 | loss 0.1002 | s/batch 0.73\n",
      "2020-08-08 16:00:26,560 INFO: | epoch   3 | step 33300 | batch 10800/11250 | lr 0.00000 0.00001 | loss 0.1137 | s/batch 0.67\n",
      "2020-08-08 16:00:59,215 INFO: | epoch   3 | step 33350 | batch 10850/11250 | lr 0.00000 0.00001 | loss 0.1345 | s/batch 0.65\n",
      "2020-08-08 16:01:27,436 INFO: | epoch   3 | step 33400 | batch 10900/11250 | lr 0.00000 0.00001 | loss 0.1188 | s/batch 0.56\n",
      "2020-08-08 16:02:04,633 INFO: | epoch   3 | step 33450 | batch 10950/11250 | lr 0.00000 0.00001 | loss 0.1522 | s/batch 0.74\n",
      "2020-08-08 16:02:46,080 INFO: | epoch   3 | step 33500 | batch 11000/11250 | lr 0.00000 0.00001 | loss 0.1636 | s/batch 0.83\n",
      "2020-08-08 16:03:19,481 INFO: | epoch   3 | step 33550 | batch 11050/11250 | lr 0.00000 0.00001 | loss 0.1287 | s/batch 0.67\n",
      "2020-08-08 16:03:55,065 INFO: | epoch   3 | step 33600 | batch 11100/11250 | lr 0.00000 0.00001 | loss 0.1186 | s/batch 0.71\n",
      "2020-08-08 16:04:30,098 INFO: | epoch   3 | step 33650 | batch 11150/11250 | lr 0.00000 0.00001 | loss 0.1259 | s/batch 0.70\n",
      "2020-08-08 16:05:01,550 INFO: | epoch   3 | step 33700 | batch 11200/11250 | lr 0.00000 0.00001 | loss 0.1411 | s/batch 0.63\n",
      "2020-08-08 16:05:32,770 INFO: | epoch   3 | step 33750 | batch 11250/11250 | lr 0.00000 0.00001 | loss 0.1303 | s/batch 0.62\n",
      "2020-08-08 16:05:33,052 INFO: | epoch   3 | score (95.56, 95.09, 95.32) | f1 95.32 | loss 0.1320 | time 7651.43\n",
      "2020-08-08 16:05:33,435 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9580    0.9615    0.9598     35027\n",
      "          股票     0.9641    0.9640    0.9641     33251\n",
      "          体育     0.9901    0.9918    0.9910     28283\n",
      "          娱乐     0.9722    0.9782    0.9752     19920\n",
      "          时政     0.9292    0.9395    0.9343     13515\n",
      "          社会     0.9283    0.9273    0.9278     11009\n",
      "          教育     0.9615    0.9608    0.9612      8987\n",
      "          财经     0.9184    0.8969    0.9076      7957\n",
      "          家居     0.9570    0.9538    0.9554      7063\n",
      "          游戏     0.9512    0.9291    0.9401      5291\n",
      "          房产     0.9879    0.9794    0.9837      4428\n",
      "          时尚     0.9481    0.9404    0.9442      2818\n",
      "          彩票     0.9582    0.9365    0.9472      1639\n",
      "          星座     0.9544    0.9532    0.9538       812\n",
      "\n",
      "    accuracy                         0.9605    180000\n",
      "   macro avg     0.9556    0.9509    0.9532    180000\n",
      "weighted avg     0.9605    0.9605    0.9605    180000\n",
      "\n",
      "2020-08-08 16:15:47,131 INFO: | epoch   3 | dev | score (95.72, 95.25, 95.47) | f1 95.47 | time 613.69\n",
      "2020-08-08 16:15:47,173 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9595    0.9627    0.9611      3891\n",
      "          股票     0.9746    0.9561    0.9653      3694\n",
      "          体育     0.9793    0.9943    0.9867      3142\n",
      "          娱乐     0.9724    0.9706    0.9715      2213\n",
      "          时政     0.9101    0.9447    0.9271      1501\n",
      "          社会     0.9396    0.9280    0.9338      1223\n",
      "          教育     0.9659    0.9649    0.9654       998\n",
      "          财经     0.9155    0.9072    0.9114       884\n",
      "          家居     0.9414    0.9630    0.9521       784\n",
      "          游戏     0.9547    0.9336    0.9440       587\n",
      "          房产     0.9979    0.9837    0.9908       492\n",
      "          时尚     0.9541    0.9297    0.9417       313\n",
      "          彩票     0.9763    0.9066    0.9402       182\n",
      "          星座     0.9596    0.9896    0.9744        96\n",
      "\n",
      "    accuracy                         0.9603     20000\n",
      "   macro avg     0.9572    0.9525    0.9547     20000\n",
      "weighted avg     0.9605    0.9603    0.9603     20000\n",
      "\n",
      "2020-08-08 16:15:47,174 INFO: Exceed history dev = 94.72, current dev = 95.47\n",
      "2020-08-08 16:16:19,885 INFO: | epoch   4 | step 33800 | batch  50/11250 | lr 0.00000 0.00001 | loss 0.1114 | s/batch 0.65\n",
      "2020-08-08 16:16:56,472 INFO: | epoch   4 | step 33850 | batch 100/11250 | lr 0.00000 0.00001 | loss 0.1035 | s/batch 0.73\n",
      "2020-08-08 16:17:28,617 INFO: | epoch   4 | step 33900 | batch 150/11250 | lr 0.00000 0.00001 | loss 0.0889 | s/batch 0.64\n",
      "2020-08-08 16:18:02,821 INFO: | epoch   4 | step 33950 | batch 200/11250 | lr 0.00000 0.00001 | loss 0.1091 | s/batch 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 16:18:36,044 INFO: | epoch   4 | step 34000 | batch 250/11250 | lr 0.00000 0.00001 | loss 0.1212 | s/batch 0.66\n",
      "2020-08-08 16:19:12,744 INFO: | epoch   4 | step 34050 | batch 300/11250 | lr 0.00000 0.00001 | loss 0.1152 | s/batch 0.73\n",
      "2020-08-08 16:19:43,011 INFO: | epoch   4 | step 34100 | batch 350/11250 | lr 0.00000 0.00001 | loss 0.0964 | s/batch 0.61\n",
      "2020-08-08 16:20:15,632 INFO: | epoch   4 | step 34150 | batch 400/11250 | lr 0.00000 0.00001 | loss 0.1491 | s/batch 0.65\n",
      "2020-08-08 16:20:47,582 INFO: | epoch   4 | step 34200 | batch 450/11250 | lr 0.00000 0.00001 | loss 0.1119 | s/batch 0.64\n",
      "2020-08-08 16:21:25,002 INFO: | epoch   4 | step 34250 | batch 500/11250 | lr 0.00000 0.00001 | loss 0.1338 | s/batch 0.75\n",
      "2020-08-08 16:22:01,597 INFO: | epoch   4 | step 34300 | batch 550/11250 | lr 0.00000 0.00001 | loss 0.1169 | s/batch 0.73\n",
      "2020-08-08 16:22:34,220 INFO: | epoch   4 | step 34350 | batch 600/11250 | lr 0.00000 0.00001 | loss 0.1161 | s/batch 0.65\n",
      "2020-08-08 16:23:12,051 INFO: | epoch   4 | step 34400 | batch 650/11250 | lr 0.00000 0.00001 | loss 0.1340 | s/batch 0.76\n",
      "2020-08-08 16:23:46,103 INFO: | epoch   4 | step 34450 | batch 700/11250 | lr 0.00000 0.00001 | loss 0.0978 | s/batch 0.68\n",
      "2020-08-08 16:24:19,488 INFO: | epoch   4 | step 34500 | batch 750/11250 | lr 0.00000 0.00001 | loss 0.1698 | s/batch 0.67\n",
      "2020-08-08 16:24:55,446 INFO: | epoch   4 | step 34550 | batch 800/11250 | lr 0.00000 0.00001 | loss 0.1094 | s/batch 0.72\n",
      "2020-08-08 16:25:30,172 INFO: | epoch   4 | step 34600 | batch 850/11250 | lr 0.00000 0.00001 | loss 0.0931 | s/batch 0.69\n",
      "2020-08-08 16:26:02,132 INFO: | epoch   4 | step 34650 | batch 900/11250 | lr 0.00000 0.00001 | loss 0.1127 | s/batch 0.64\n",
      "2020-08-08 16:26:37,107 INFO: | epoch   4 | step 34700 | batch 950/11250 | lr 0.00000 0.00001 | loss 0.1277 | s/batch 0.70\n",
      "2020-08-08 16:27:10,077 INFO: | epoch   4 | step 34750 | batch 1000/11250 | lr 0.00000 0.00001 | loss 0.0973 | s/batch 0.66\n",
      "2020-08-08 16:27:42,056 INFO: | epoch   4 | step 34800 | batch 1050/11250 | lr 0.00000 0.00001 | loss 0.1195 | s/batch 0.64\n",
      "2020-08-08 16:28:15,030 INFO: | epoch   4 | step 34850 | batch 1100/11250 | lr 0.00000 0.00001 | loss 0.1069 | s/batch 0.66\n",
      "2020-08-08 16:28:48,417 INFO: | epoch   4 | step 34900 | batch 1150/11250 | lr 0.00000 0.00001 | loss 0.1058 | s/batch 0.67\n",
      "2020-08-08 16:29:19,928 INFO: | epoch   4 | step 34950 | batch 1200/11250 | lr 0.00000 0.00001 | loss 0.0937 | s/batch 0.63\n",
      "2020-08-08 16:29:51,695 INFO: | epoch   4 | step 35000 | batch 1250/11250 | lr 0.00000 0.00001 | loss 0.1072 | s/batch 0.64\n",
      "2020-08-08 16:30:21,904 INFO: | epoch   4 | step 35050 | batch 1300/11250 | lr 0.00000 0.00001 | loss 0.0908 | s/batch 0.60\n",
      "2020-08-08 16:30:54,113 INFO: | epoch   4 | step 35100 | batch 1350/11250 | lr 0.00000 0.00001 | loss 0.1091 | s/batch 0.64\n",
      "2020-08-08 16:31:29,347 INFO: | epoch   4 | step 35150 | batch 1400/11250 | lr 0.00000 0.00001 | loss 0.0900 | s/batch 0.70\n",
      "2020-08-08 16:32:02,128 INFO: | epoch   4 | step 35200 | batch 1450/11250 | lr 0.00000 0.00001 | loss 0.1512 | s/batch 0.66\n",
      "2020-08-08 16:32:33,116 INFO: | epoch   4 | step 35250 | batch 1500/11250 | lr 0.00000 0.00001 | loss 0.1349 | s/batch 0.62\n",
      "2020-08-08 16:33:10,490 INFO: | epoch   4 | step 35300 | batch 1550/11250 | lr 0.00000 0.00001 | loss 0.0717 | s/batch 0.75\n",
      "2020-08-08 16:33:42,547 INFO: | epoch   4 | step 35350 | batch 1600/11250 | lr 0.00000 0.00001 | loss 0.1424 | s/batch 0.64\n",
      "2020-08-08 16:34:17,750 INFO: | epoch   4 | step 35400 | batch 1650/11250 | lr 0.00000 0.00001 | loss 0.1215 | s/batch 0.70\n",
      "2020-08-08 16:34:51,299 INFO: | epoch   4 | step 35450 | batch 1700/11250 | lr 0.00000 0.00001 | loss 0.1321 | s/batch 0.67\n",
      "2020-08-08 16:35:24,308 INFO: | epoch   4 | step 35500 | batch 1750/11250 | lr 0.00000 0.00001 | loss 0.1285 | s/batch 0.66\n",
      "2020-08-08 16:35:59,547 INFO: | epoch   4 | step 35550 | batch 1800/11250 | lr 0.00000 0.00001 | loss 0.0980 | s/batch 0.70\n",
      "2020-08-08 16:36:29,665 INFO: | epoch   4 | step 35600 | batch 1850/11250 | lr 0.00000 0.00001 | loss 0.1079 | s/batch 0.60\n",
      "2020-08-08 16:37:09,245 INFO: | epoch   4 | step 35650 | batch 1900/11250 | lr 0.00000 0.00001 | loss 0.1224 | s/batch 0.79\n",
      "2020-08-08 16:37:41,740 INFO: | epoch   4 | step 35700 | batch 1950/11250 | lr 0.00000 0.00001 | loss 0.0996 | s/batch 0.65\n",
      "2020-08-08 16:38:14,235 INFO: | epoch   4 | step 35750 | batch 2000/11250 | lr 0.00000 0.00001 | loss 0.0853 | s/batch 0.65\n",
      "2020-08-08 16:38:49,479 INFO: | epoch   4 | step 35800 | batch 2050/11250 | lr 0.00000 0.00001 | loss 0.1040 | s/batch 0.70\n",
      "2020-08-08 16:39:20,473 INFO: | epoch   4 | step 35850 | batch 2100/11250 | lr 0.00000 0.00001 | loss 0.1023 | s/batch 0.62\n",
      "2020-08-08 16:39:57,227 INFO: | epoch   4 | step 35900 | batch 2150/11250 | lr 0.00000 0.00001 | loss 0.1480 | s/batch 0.74\n",
      "2020-08-08 16:40:34,366 INFO: | epoch   4 | step 35950 | batch 2200/11250 | lr 0.00000 0.00001 | loss 0.1506 | s/batch 0.74\n",
      "2020-08-08 16:41:08,693 INFO: | epoch   4 | step 36000 | batch 2250/11250 | lr 0.00000 0.00001 | loss 0.1007 | s/batch 0.69\n",
      "2020-08-08 16:41:45,786 INFO: | epoch   4 | step 36050 | batch 2300/11250 | lr 0.00000 0.00001 | loss 0.0765 | s/batch 0.74\n",
      "2020-08-08 16:42:17,910 INFO: | epoch   4 | step 36100 | batch 2350/11250 | lr 0.00000 0.00001 | loss 0.1240 | s/batch 0.64\n",
      "2020-08-08 16:42:52,425 INFO: | epoch   4 | step 36150 | batch 2400/11250 | lr 0.00000 0.00001 | loss 0.0913 | s/batch 0.69\n",
      "2020-08-08 16:43:26,073 INFO: | epoch   4 | step 36200 | batch 2450/11250 | lr 0.00000 0.00001 | loss 0.1002 | s/batch 0.67\n",
      "2020-08-08 16:43:59,616 INFO: | epoch   4 | step 36250 | batch 2500/11250 | lr 0.00000 0.00001 | loss 0.1134 | s/batch 0.67\n",
      "2020-08-08 16:44:33,389 INFO: | epoch   4 | step 36300 | batch 2550/11250 | lr 0.00000 0.00001 | loss 0.1398 | s/batch 0.68\n",
      "2020-08-08 16:45:13,174 INFO: | epoch   4 | step 36350 | batch 2600/11250 | lr 0.00000 0.00001 | loss 0.0971 | s/batch 0.80\n",
      "2020-08-08 16:45:43,214 INFO: | epoch   4 | step 36400 | batch 2650/11250 | lr 0.00000 0.00001 | loss 0.0966 | s/batch 0.60\n",
      "2020-08-08 16:46:15,233 INFO: | epoch   4 | step 36450 | batch 2700/11250 | lr 0.00000 0.00001 | loss 0.0974 | s/batch 0.64\n",
      "2020-08-08 16:46:50,994 INFO: | epoch   4 | step 36500 | batch 2750/11250 | lr 0.00000 0.00001 | loss 0.0945 | s/batch 0.72\n",
      "2020-08-08 16:47:22,129 INFO: | epoch   4 | step 36550 | batch 2800/11250 | lr 0.00000 0.00001 | loss 0.0908 | s/batch 0.62\n",
      "2020-08-08 16:47:54,941 INFO: | epoch   4 | step 36600 | batch 2850/11250 | lr 0.00000 0.00001 | loss 0.1446 | s/batch 0.66\n",
      "2020-08-08 16:48:27,369 INFO: | epoch   4 | step 36650 | batch 2900/11250 | lr 0.00000 0.00001 | loss 0.1071 | s/batch 0.65\n",
      "2020-08-08 16:49:02,001 INFO: | epoch   4 | step 36700 | batch 2950/11250 | lr 0.00000 0.00001 | loss 0.1402 | s/batch 0.69\n",
      "2020-08-08 16:49:35,995 INFO: | epoch   4 | step 36750 | batch 3000/11250 | lr 0.00000 0.00001 | loss 0.1457 | s/batch 0.68\n",
      "2020-08-08 16:50:04,662 INFO: | epoch   4 | step 36800 | batch 3050/11250 | lr 0.00000 0.00001 | loss 0.1002 | s/batch 0.57\n",
      "2020-08-08 16:50:36,949 INFO: | epoch   4 | step 36850 | batch 3100/11250 | lr 0.00000 0.00001 | loss 0.1075 | s/batch 0.65\n",
      "2020-08-08 16:51:12,894 INFO: | epoch   4 | step 36900 | batch 3150/11250 | lr 0.00000 0.00001 | loss 0.1187 | s/batch 0.72\n",
      "2020-08-08 16:51:41,711 INFO: | epoch   4 | step 36950 | batch 3200/11250 | lr 0.00000 0.00001 | loss 0.1069 | s/batch 0.58\n",
      "2020-08-08 16:52:14,977 INFO: | epoch   4 | step 37000 | batch 3250/11250 | lr 0.00000 0.00001 | loss 0.1274 | s/batch 0.67\n",
      "2020-08-08 16:52:51,114 INFO: | epoch   4 | step 37050 | batch 3300/11250 | lr 0.00000 0.00001 | loss 0.1203 | s/batch 0.72\n",
      "2020-08-08 16:53:26,649 INFO: | epoch   4 | step 37100 | batch 3350/11250 | lr 0.00000 0.00001 | loss 0.1355 | s/batch 0.71\n",
      "2020-08-08 16:54:03,379 INFO: | epoch   4 | step 37150 | batch 3400/11250 | lr 0.00000 0.00001 | loss 0.0930 | s/batch 0.73\n",
      "2020-08-08 16:54:37,868 INFO: | epoch   4 | step 37200 | batch 3450/11250 | lr 0.00000 0.00001 | loss 0.0943 | s/batch 0.69\n",
      "2020-08-08 16:55:13,648 INFO: | epoch   4 | step 37250 | batch 3500/11250 | lr 0.00000 0.00001 | loss 0.1021 | s/batch 0.72\n",
      "2020-08-08 16:55:48,385 INFO: | epoch   4 | step 37300 | batch 3550/11250 | lr 0.00000 0.00001 | loss 0.1035 | s/batch 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 16:56:20,182 INFO: | epoch   4 | step 37350 | batch 3600/11250 | lr 0.00000 0.00001 | loss 0.1325 | s/batch 0.64\n",
      "2020-08-08 16:56:54,940 INFO: | epoch   4 | step 37400 | batch 3650/11250 | lr 0.00000 0.00001 | loss 0.0826 | s/batch 0.70\n",
      "2020-08-08 16:57:27,457 INFO: | epoch   4 | step 37450 | batch 3700/11250 | lr 0.00000 0.00001 | loss 0.1097 | s/batch 0.65\n",
      "2020-08-08 16:57:58,171 INFO: | epoch   4 | step 37500 | batch 3750/11250 | lr 0.00000 0.00001 | loss 0.0918 | s/batch 0.61\n",
      "2020-08-08 16:58:30,199 INFO: | epoch   4 | step 37550 | batch 3800/11250 | lr 0.00000 0.00001 | loss 0.1080 | s/batch 0.64\n",
      "2020-08-08 16:59:04,938 INFO: | epoch   4 | step 37600 | batch 3850/11250 | lr 0.00000 0.00001 | loss 0.1153 | s/batch 0.69\n",
      "2020-08-08 16:59:39,082 INFO: | epoch   4 | step 37650 | batch 3900/11250 | lr 0.00000 0.00001 | loss 0.0836 | s/batch 0.68\n",
      "2020-08-08 17:00:14,531 INFO: | epoch   4 | step 37700 | batch 3950/11250 | lr 0.00000 0.00001 | loss 0.1135 | s/batch 0.71\n",
      "2020-08-08 17:00:48,329 INFO: | epoch   4 | step 37750 | batch 4000/11250 | lr 0.00000 0.00001 | loss 0.0839 | s/batch 0.68\n",
      "2020-08-08 17:01:20,706 INFO: | epoch   4 | step 37800 | batch 4050/11250 | lr 0.00000 0.00001 | loss 0.1264 | s/batch 0.65\n",
      "2020-08-08 17:01:53,585 INFO: | epoch   4 | step 37850 | batch 4100/11250 | lr 0.00000 0.00001 | loss 0.1267 | s/batch 0.66\n",
      "2020-08-08 17:02:26,958 INFO: | epoch   4 | step 37900 | batch 4150/11250 | lr 0.00000 0.00001 | loss 0.1076 | s/batch 0.67\n",
      "2020-08-08 17:03:02,731 INFO: | epoch   4 | step 37950 | batch 4200/11250 | lr 0.00000 0.00001 | loss 0.1079 | s/batch 0.72\n",
      "2020-08-08 17:03:34,875 INFO: | epoch   4 | step 38000 | batch 4250/11250 | lr 0.00000 0.00001 | loss 0.0880 | s/batch 0.64\n",
      "2020-08-08 17:04:09,188 INFO: | epoch   4 | step 38050 | batch 4300/11250 | lr 0.00000 0.00001 | loss 0.0941 | s/batch 0.69\n",
      "2020-08-08 17:04:40,229 INFO: | epoch   4 | step 38100 | batch 4350/11250 | lr 0.00000 0.00001 | loss 0.1448 | s/batch 0.62\n",
      "2020-08-08 17:05:14,597 INFO: | epoch   4 | step 38150 | batch 4400/11250 | lr 0.00000 0.00001 | loss 0.1165 | s/batch 0.69\n",
      "2020-08-08 17:05:45,700 INFO: | epoch   4 | step 38200 | batch 4450/11250 | lr 0.00000 0.00001 | loss 0.0962 | s/batch 0.62\n",
      "2020-08-08 17:06:25,588 INFO: | epoch   4 | step 38250 | batch 4500/11250 | lr 0.00000 0.00001 | loss 0.1425 | s/batch 0.80\n",
      "2020-08-08 17:06:58,308 INFO: | epoch   4 | step 38300 | batch 4550/11250 | lr 0.00000 0.00001 | loss 0.1286 | s/batch 0.65\n",
      "2020-08-08 17:07:29,274 INFO: | epoch   4 | step 38350 | batch 4600/11250 | lr 0.00000 0.00001 | loss 0.0860 | s/batch 0.62\n",
      "2020-08-08 17:07:59,369 INFO: | epoch   4 | step 38400 | batch 4650/11250 | lr 0.00000 0.00001 | loss 0.1100 | s/batch 0.60\n",
      "2020-08-08 17:08:37,541 INFO: | epoch   4 | step 38450 | batch 4700/11250 | lr 0.00000 0.00001 | loss 0.1349 | s/batch 0.76\n",
      "2020-08-08 17:09:13,944 INFO: | epoch   4 | step 38500 | batch 4750/11250 | lr 0.00000 0.00001 | loss 0.0848 | s/batch 0.73\n",
      "2020-08-08 17:09:49,127 INFO: | epoch   4 | step 38550 | batch 4800/11250 | lr 0.00000 0.00001 | loss 0.1617 | s/batch 0.70\n",
      "2020-08-08 17:10:25,142 INFO: | epoch   4 | step 38600 | batch 4850/11250 | lr 0.00000 0.00001 | loss 0.0884 | s/batch 0.72\n",
      "2020-08-08 17:10:58,657 INFO: | epoch   4 | step 38650 | batch 4900/11250 | lr 0.00000 0.00001 | loss 0.1027 | s/batch 0.67\n",
      "2020-08-08 17:11:35,176 INFO: | epoch   4 | step 38700 | batch 4950/11250 | lr 0.00000 0.00001 | loss 0.0832 | s/batch 0.73\n",
      "2020-08-08 17:12:13,605 INFO: | epoch   4 | step 38750 | batch 5000/11250 | lr 0.00000 0.00001 | loss 0.1290 | s/batch 0.77\n",
      "2020-08-08 17:12:48,937 INFO: | epoch   4 | step 38800 | batch 5050/11250 | lr 0.00000 0.00001 | loss 0.1059 | s/batch 0.71\n",
      "2020-08-08 17:13:22,027 INFO: | epoch   4 | step 38850 | batch 5100/11250 | lr 0.00000 0.00001 | loss 0.1010 | s/batch 0.66\n",
      "2020-08-08 17:13:50,908 INFO: | epoch   4 | step 38900 | batch 5150/11250 | lr 0.00000 0.00001 | loss 0.0804 | s/batch 0.58\n",
      "2020-08-08 17:14:21,940 INFO: | epoch   4 | step 38950 | batch 5200/11250 | lr 0.00000 0.00001 | loss 0.1008 | s/batch 0.62\n",
      "2020-08-08 17:14:58,742 INFO: | epoch   4 | step 39000 | batch 5250/11250 | lr 0.00000 0.00001 | loss 0.0964 | s/batch 0.74\n",
      "2020-08-08 17:15:34,640 INFO: | epoch   4 | step 39050 | batch 5300/11250 | lr 0.00000 0.00001 | loss 0.0889 | s/batch 0.72\n",
      "2020-08-08 17:16:10,162 INFO: | epoch   4 | step 39100 | batch 5350/11250 | lr 0.00000 0.00001 | loss 0.0996 | s/batch 0.71\n",
      "2020-08-08 17:16:46,064 INFO: | epoch   4 | step 39150 | batch 5400/11250 | lr 0.00000 0.00001 | loss 0.1120 | s/batch 0.72\n",
      "2020-08-08 17:17:19,622 INFO: | epoch   4 | step 39200 | batch 5450/11250 | lr 0.00000 0.00001 | loss 0.0899 | s/batch 0.67\n",
      "2020-08-08 17:17:51,379 INFO: | epoch   4 | step 39250 | batch 5500/11250 | lr 0.00000 0.00001 | loss 0.1064 | s/batch 0.64\n",
      "2020-08-08 17:18:29,399 INFO: | epoch   4 | step 39300 | batch 5550/11250 | lr 0.00000 0.00001 | loss 0.0921 | s/batch 0.76\n",
      "2020-08-08 17:19:03,350 INFO: | epoch   4 | step 39350 | batch 5600/11250 | lr 0.00000 0.00001 | loss 0.1005 | s/batch 0.68\n",
      "2020-08-08 17:19:36,935 INFO: | epoch   4 | step 39400 | batch 5650/11250 | lr 0.00000 0.00001 | loss 0.1266 | s/batch 0.67\n",
      "2020-08-08 17:20:08,450 INFO: | epoch   4 | step 39450 | batch 5700/11250 | lr 0.00000 0.00001 | loss 0.0724 | s/batch 0.63\n",
      "2020-08-08 17:20:45,516 INFO: | epoch   4 | step 39500 | batch 5750/11250 | lr 0.00000 0.00001 | loss 0.0855 | s/batch 0.74\n",
      "2020-08-08 17:21:17,922 INFO: | epoch   4 | step 39550 | batch 5800/11250 | lr 0.00000 0.00001 | loss 0.1277 | s/batch 0.65\n",
      "2020-08-08 17:21:56,589 INFO: | epoch   4 | step 39600 | batch 5850/11250 | lr 0.00000 0.00001 | loss 0.1066 | s/batch 0.77\n",
      "2020-08-08 17:22:29,413 INFO: | epoch   4 | step 39650 | batch 5900/11250 | lr 0.00000 0.00001 | loss 0.1016 | s/batch 0.66\n",
      "2020-08-08 17:23:07,948 INFO: | epoch   4 | step 39700 | batch 5950/11250 | lr 0.00000 0.00001 | loss 0.0879 | s/batch 0.77\n",
      "2020-08-08 17:23:37,787 INFO: | epoch   4 | step 39750 | batch 6000/11250 | lr 0.00000 0.00001 | loss 0.1419 | s/batch 0.60\n",
      "2020-08-08 17:24:10,421 INFO: | epoch   4 | step 39800 | batch 6050/11250 | lr 0.00000 0.00001 | loss 0.1135 | s/batch 0.65\n",
      "2020-08-08 17:24:42,192 INFO: | epoch   4 | step 39850 | batch 6100/11250 | lr 0.00000 0.00001 | loss 0.1086 | s/batch 0.64\n",
      "2020-08-08 17:25:14,223 INFO: | epoch   4 | step 39900 | batch 6150/11250 | lr 0.00000 0.00001 | loss 0.1195 | s/batch 0.64\n",
      "2020-08-08 17:25:49,535 INFO: | epoch   4 | step 39950 | batch 6200/11250 | lr 0.00000 0.00001 | loss 0.1011 | s/batch 0.71\n",
      "2020-08-08 17:26:25,227 INFO: | epoch   4 | step 40000 | batch 6250/11250 | lr 0.00000 0.00001 | loss 0.0834 | s/batch 0.71\n",
      "2020-08-08 17:26:56,584 INFO: | epoch   4 | step 40050 | batch 6300/11250 | lr 0.00000 0.00001 | loss 0.0842 | s/batch 0.63\n",
      "2020-08-08 17:27:26,990 INFO: | epoch   4 | step 40100 | batch 6350/11250 | lr 0.00000 0.00001 | loss 0.0852 | s/batch 0.61\n",
      "2020-08-08 17:28:01,758 INFO: | epoch   4 | step 40150 | batch 6400/11250 | lr 0.00000 0.00001 | loss 0.0919 | s/batch 0.70\n",
      "2020-08-08 17:28:33,089 INFO: | epoch   4 | step 40200 | batch 6450/11250 | lr 0.00000 0.00001 | loss 0.1105 | s/batch 0.63\n",
      "2020-08-08 17:29:03,952 INFO: | epoch   4 | step 40250 | batch 6500/11250 | lr 0.00000 0.00001 | loss 0.1033 | s/batch 0.62\n",
      "2020-08-08 17:29:41,566 INFO: | epoch   4 | step 40300 | batch 6550/11250 | lr 0.00000 0.00001 | loss 0.0940 | s/batch 0.75\n",
      "2020-08-08 17:30:20,343 INFO: | epoch   4 | step 40350 | batch 6600/11250 | lr 0.00000 0.00001 | loss 0.1192 | s/batch 0.78\n",
      "2020-08-08 17:30:59,496 INFO: | epoch   4 | step 40400 | batch 6650/11250 | lr 0.00000 0.00001 | loss 0.0893 | s/batch 0.78\n",
      "2020-08-08 17:31:34,838 INFO: | epoch   4 | step 40450 | batch 6700/11250 | lr 0.00000 0.00001 | loss 0.0832 | s/batch 0.71\n",
      "2020-08-08 17:32:08,438 INFO: | epoch   4 | step 40500 | batch 6750/11250 | lr 0.00000 0.00001 | loss 0.0807 | s/batch 0.67\n",
      "2020-08-08 17:32:40,260 INFO: | epoch   4 | step 40550 | batch 6800/11250 | lr 0.00000 0.00000 | loss 0.1123 | s/batch 0.64\n",
      "2020-08-08 17:33:15,188 INFO: | epoch   4 | step 40600 | batch 6850/11250 | lr 0.00000 0.00000 | loss 0.0900 | s/batch 0.70\n",
      "2020-08-08 17:33:49,245 INFO: | epoch   4 | step 40650 | batch 6900/11250 | lr 0.00000 0.00000 | loss 0.0900 | s/batch 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 17:34:24,174 INFO: | epoch   4 | step 40700 | batch 6950/11250 | lr 0.00000 0.00000 | loss 0.1124 | s/batch 0.70\n",
      "2020-08-08 17:34:57,915 INFO: | epoch   4 | step 40750 | batch 7000/11250 | lr 0.00000 0.00000 | loss 0.1056 | s/batch 0.67\n",
      "2020-08-08 17:35:32,513 INFO: | epoch   4 | step 40800 | batch 7050/11250 | lr 0.00000 0.00000 | loss 0.0942 | s/batch 0.69\n",
      "2020-08-08 17:36:06,749 INFO: | epoch   4 | step 40850 | batch 7100/11250 | lr 0.00000 0.00000 | loss 0.1138 | s/batch 0.68\n",
      "2020-08-08 17:36:42,215 INFO: | epoch   4 | step 40900 | batch 7150/11250 | lr 0.00000 0.00000 | loss 0.0614 | s/batch 0.71\n",
      "2020-08-08 17:37:13,645 INFO: | epoch   4 | step 40950 | batch 7200/11250 | lr 0.00000 0.00000 | loss 0.1184 | s/batch 0.63\n",
      "2020-08-08 17:37:50,473 INFO: | epoch   4 | step 41000 | batch 7250/11250 | lr 0.00000 0.00000 | loss 0.0764 | s/batch 0.74\n",
      "2020-08-08 17:38:23,771 INFO: | epoch   4 | step 41050 | batch 7300/11250 | lr 0.00000 0.00000 | loss 0.1426 | s/batch 0.67\n",
      "2020-08-08 17:38:58,549 INFO: | epoch   4 | step 41100 | batch 7350/11250 | lr 0.00000 0.00000 | loss 0.1442 | s/batch 0.70\n",
      "2020-08-08 17:39:30,690 INFO: | epoch   4 | step 41150 | batch 7400/11250 | lr 0.00000 0.00000 | loss 0.1216 | s/batch 0.64\n",
      "2020-08-08 17:40:04,962 INFO: | epoch   4 | step 41200 | batch 7450/11250 | lr 0.00000 0.00000 | loss 0.0727 | s/batch 0.69\n",
      "2020-08-08 17:40:38,948 INFO: | epoch   4 | step 41250 | batch 7500/11250 | lr 0.00000 0.00000 | loss 0.1412 | s/batch 0.68\n",
      "2020-08-08 17:41:13,568 INFO: | epoch   4 | step 41300 | batch 7550/11250 | lr 0.00000 0.00000 | loss 0.0990 | s/batch 0.69\n",
      "2020-08-08 17:41:46,423 INFO: | epoch   4 | step 41350 | batch 7600/11250 | lr 0.00000 0.00000 | loss 0.0913 | s/batch 0.66\n",
      "2020-08-08 17:42:24,989 INFO: | epoch   4 | step 41400 | batch 7650/11250 | lr 0.00000 0.00000 | loss 0.1029 | s/batch 0.77\n",
      "2020-08-08 17:42:59,557 INFO: | epoch   4 | step 41450 | batch 7700/11250 | lr 0.00000 0.00000 | loss 0.1083 | s/batch 0.69\n",
      "2020-08-08 17:43:32,363 INFO: | epoch   4 | step 41500 | batch 7750/11250 | lr 0.00000 0.00000 | loss 0.1301 | s/batch 0.66\n",
      "2020-08-08 17:44:07,311 INFO: | epoch   4 | step 41550 | batch 7800/11250 | lr 0.00000 0.00000 | loss 0.0926 | s/batch 0.70\n",
      "2020-08-08 17:44:42,204 INFO: | epoch   4 | step 41600 | batch 7850/11250 | lr 0.00000 0.00000 | loss 0.1117 | s/batch 0.70\n",
      "2020-08-08 17:45:16,899 INFO: | epoch   4 | step 41650 | batch 7900/11250 | lr 0.00000 0.00000 | loss 0.1057 | s/batch 0.69\n",
      "2020-08-08 17:45:49,388 INFO: | epoch   4 | step 41700 | batch 7950/11250 | lr 0.00000 0.00000 | loss 0.0903 | s/batch 0.65\n",
      "2020-08-08 17:46:22,862 INFO: | epoch   4 | step 41750 | batch 8000/11250 | lr 0.00000 0.00000 | loss 0.1410 | s/batch 0.67\n",
      "2020-08-08 17:46:55,899 INFO: | epoch   4 | step 41800 | batch 8050/11250 | lr 0.00000 0.00000 | loss 0.1124 | s/batch 0.66\n",
      "2020-08-08 17:47:28,497 INFO: | epoch   4 | step 41850 | batch 8100/11250 | lr 0.00000 0.00000 | loss 0.0833 | s/batch 0.65\n",
      "2020-08-08 17:47:59,902 INFO: | epoch   4 | step 41900 | batch 8150/11250 | lr 0.00000 0.00000 | loss 0.1075 | s/batch 0.63\n",
      "2020-08-08 17:48:35,122 INFO: | epoch   4 | step 41950 | batch 8200/11250 | lr 0.00000 0.00000 | loss 0.1568 | s/batch 0.70\n",
      "2020-08-08 17:49:07,645 INFO: | epoch   4 | step 42000 | batch 8250/11250 | lr 0.00000 0.00000 | loss 0.1306 | s/batch 0.65\n",
      "2020-08-08 17:49:39,564 INFO: | epoch   4 | step 42050 | batch 8300/11250 | lr 0.00000 0.00000 | loss 0.1133 | s/batch 0.64\n",
      "2020-08-08 17:50:13,659 INFO: | epoch   4 | step 42100 | batch 8350/11250 | lr 0.00000 0.00000 | loss 0.1076 | s/batch 0.68\n",
      "2020-08-08 17:50:46,104 INFO: | epoch   4 | step 42150 | batch 8400/11250 | lr 0.00000 0.00000 | loss 0.0857 | s/batch 0.65\n",
      "2020-08-08 17:51:20,367 INFO: | epoch   4 | step 42200 | batch 8450/11250 | lr 0.00000 0.00000 | loss 0.1168 | s/batch 0.69\n",
      "2020-08-08 17:51:58,011 INFO: | epoch   4 | step 42250 | batch 8500/11250 | lr 0.00000 0.00000 | loss 0.1058 | s/batch 0.75\n",
      "2020-08-08 17:52:30,421 INFO: | epoch   4 | step 42300 | batch 8550/11250 | lr 0.00000 0.00000 | loss 0.0851 | s/batch 0.65\n",
      "2020-08-08 17:53:04,456 INFO: | epoch   4 | step 42350 | batch 8600/11250 | lr 0.00000 0.00000 | loss 0.1333 | s/batch 0.68\n",
      "2020-08-08 17:53:37,017 INFO: | epoch   4 | step 42400 | batch 8650/11250 | lr 0.00000 0.00000 | loss 0.0943 | s/batch 0.65\n",
      "2020-08-08 17:54:09,639 INFO: | epoch   4 | step 42450 | batch 8700/11250 | lr 0.00000 0.00000 | loss 0.0950 | s/batch 0.65\n",
      "2020-08-08 17:54:44,955 INFO: | epoch   4 | step 42500 | batch 8750/11250 | lr 0.00000 0.00000 | loss 0.1290 | s/batch 0.71\n",
      "2020-08-08 17:55:16,757 INFO: | epoch   4 | step 42550 | batch 8800/11250 | lr 0.00000 0.00000 | loss 0.0950 | s/batch 0.64\n",
      "2020-08-08 17:55:47,989 INFO: | epoch   4 | step 42600 | batch 8850/11250 | lr 0.00000 0.00000 | loss 0.0979 | s/batch 0.62\n",
      "2020-08-08 17:56:18,469 INFO: | epoch   4 | step 42650 | batch 8900/11250 | lr 0.00000 0.00000 | loss 0.0793 | s/batch 0.61\n",
      "2020-08-08 17:56:55,977 INFO: | epoch   4 | step 42700 | batch 8950/11250 | lr 0.00000 0.00000 | loss 0.1123 | s/batch 0.75\n",
      "2020-08-08 17:57:31,177 INFO: | epoch   4 | step 42750 | batch 9000/11250 | lr 0.00000 0.00000 | loss 0.0879 | s/batch 0.70\n",
      "2020-08-08 17:58:06,952 INFO: | epoch   4 | step 42800 | batch 9050/11250 | lr 0.00000 0.00000 | loss 0.1423 | s/batch 0.72\n",
      "2020-08-08 17:58:39,886 INFO: | epoch   4 | step 42850 | batch 9100/11250 | lr 0.00000 0.00000 | loss 0.0803 | s/batch 0.66\n",
      "2020-08-08 17:59:11,710 INFO: | epoch   4 | step 42900 | batch 9150/11250 | lr 0.00000 0.00000 | loss 0.0871 | s/batch 0.64\n",
      "2020-08-08 17:59:46,358 INFO: | epoch   4 | step 42950 | batch 9200/11250 | lr 0.00000 0.00000 | loss 0.0987 | s/batch 0.69\n",
      "2020-08-08 18:00:18,708 INFO: | epoch   4 | step 43000 | batch 9250/11250 | lr 0.00000 0.00000 | loss 0.0873 | s/batch 0.65\n",
      "2020-08-08 18:00:52,581 INFO: | epoch   4 | step 43050 | batch 9300/11250 | lr 0.00000 0.00000 | loss 0.0660 | s/batch 0.68\n",
      "2020-08-08 18:01:28,796 INFO: | epoch   4 | step 43100 | batch 9350/11250 | lr 0.00000 0.00000 | loss 0.1031 | s/batch 0.72\n",
      "2020-08-08 18:01:57,977 INFO: | epoch   4 | step 43150 | batch 9400/11250 | lr 0.00000 0.00000 | loss 0.0989 | s/batch 0.58\n",
      "2020-08-08 18:02:34,056 INFO: | epoch   4 | step 43200 | batch 9450/11250 | lr 0.00000 0.00000 | loss 0.0601 | s/batch 0.72\n",
      "2020-08-08 18:03:07,387 INFO: | epoch   4 | step 43250 | batch 9500/11250 | lr 0.00000 0.00000 | loss 0.1324 | s/batch 0.67\n",
      "2020-08-08 18:03:41,902 INFO: | epoch   4 | step 43300 | batch 9550/11250 | lr 0.00000 0.00000 | loss 0.1293 | s/batch 0.69\n",
      "2020-08-08 18:04:17,184 INFO: | epoch   4 | step 43350 | batch 9600/11250 | lr 0.00000 0.00000 | loss 0.1078 | s/batch 0.71\n",
      "2020-08-08 18:04:50,671 INFO: | epoch   4 | step 43400 | batch 9650/11250 | lr 0.00000 0.00000 | loss 0.1764 | s/batch 0.67\n",
      "2020-08-08 18:05:26,348 INFO: | epoch   4 | step 43450 | batch 9700/11250 | lr 0.00000 0.00000 | loss 0.1076 | s/batch 0.71\n",
      "2020-08-08 18:05:57,961 INFO: | epoch   4 | step 43500 | batch 9750/11250 | lr 0.00000 0.00000 | loss 0.0738 | s/batch 0.63\n",
      "2020-08-08 18:06:32,494 INFO: | epoch   4 | step 43550 | batch 9800/11250 | lr 0.00000 0.00000 | loss 0.0989 | s/batch 0.69\n",
      "2020-08-08 18:07:06,767 INFO: | epoch   4 | step 43600 | batch 9850/11250 | lr 0.00000 0.00000 | loss 0.1235 | s/batch 0.69\n",
      "2020-08-08 18:07:38,132 INFO: | epoch   4 | step 43650 | batch 9900/11250 | lr 0.00000 0.00000 | loss 0.1348 | s/batch 0.63\n",
      "2020-08-08 18:08:10,849 INFO: | epoch   4 | step 43700 | batch 9950/11250 | lr 0.00000 0.00000 | loss 0.0961 | s/batch 0.65\n",
      "2020-08-08 18:08:41,862 INFO: | epoch   4 | step 43750 | batch 10000/11250 | lr 0.00000 0.00000 | loss 0.1265 | s/batch 0.62\n",
      "2020-08-08 18:09:11,320 INFO: | epoch   4 | step 43800 | batch 10050/11250 | lr 0.00000 0.00000 | loss 0.1428 | s/batch 0.59\n",
      "2020-08-08 18:09:47,284 INFO: | epoch   4 | step 43850 | batch 10100/11250 | lr 0.00000 0.00000 | loss 0.1192 | s/batch 0.72\n",
      "2020-08-08 18:10:18,460 INFO: | epoch   4 | step 43900 | batch 10150/11250 | lr 0.00000 0.00000 | loss 0.0991 | s/batch 0.62\n",
      "2020-08-08 18:10:54,961 INFO: | epoch   4 | step 43950 | batch 10200/11250 | lr 0.00000 0.00000 | loss 0.1248 | s/batch 0.73\n",
      "2020-08-08 18:11:27,550 INFO: | epoch   4 | step 44000 | batch 10250/11250 | lr 0.00000 0.00000 | loss 0.1211 | s/batch 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-08 18:12:00,256 INFO: | epoch   4 | step 44050 | batch 10300/11250 | lr 0.00000 0.00000 | loss 0.0704 | s/batch 0.65\n",
      "2020-08-08 18:12:31,792 INFO: | epoch   4 | step 44100 | batch 10350/11250 | lr 0.00000 0.00000 | loss 0.0962 | s/batch 0.63\n",
      "2020-08-08 18:13:02,422 INFO: | epoch   4 | step 44150 | batch 10400/11250 | lr 0.00000 0.00000 | loss 0.1400 | s/batch 0.61\n",
      "2020-08-08 18:13:34,094 INFO: | epoch   4 | step 44200 | batch 10450/11250 | lr 0.00000 0.00000 | loss 0.0704 | s/batch 0.63\n",
      "2020-08-08 18:14:14,294 INFO: | epoch   4 | step 44250 | batch 10500/11250 | lr 0.00000 0.00000 | loss 0.1224 | s/batch 0.80\n",
      "2020-08-08 18:14:50,567 INFO: | epoch   4 | step 44300 | batch 10550/11250 | lr 0.00000 0.00000 | loss 0.1398 | s/batch 0.73\n",
      "2020-08-08 18:15:25,603 INFO: | epoch   4 | step 44350 | batch 10600/11250 | lr 0.00000 0.00000 | loss 0.1227 | s/batch 0.70\n",
      "2020-08-08 18:15:59,841 INFO: | epoch   4 | step 44400 | batch 10650/11250 | lr 0.00000 0.00000 | loss 0.0802 | s/batch 0.68\n",
      "2020-08-08 18:16:37,826 INFO: | epoch   4 | step 44450 | batch 10700/11250 | lr 0.00000 0.00000 | loss 0.1480 | s/batch 0.76\n",
      "2020-08-08 18:17:12,646 INFO: | epoch   4 | step 44500 | batch 10750/11250 | lr 0.00000 0.00000 | loss 0.1260 | s/batch 0.70\n",
      "2020-08-08 18:17:43,325 INFO: | epoch   4 | step 44550 | batch 10800/11250 | lr 0.00000 0.00000 | loss 0.0739 | s/batch 0.61\n",
      "2020-08-08 18:18:18,240 INFO: | epoch   4 | step 44600 | batch 10850/11250 | lr 0.00000 0.00000 | loss 0.0965 | s/batch 0.70\n",
      "2020-08-08 18:18:53,323 INFO: | epoch   4 | step 44650 | batch 10900/11250 | lr 0.00000 0.00000 | loss 0.1545 | s/batch 0.70\n",
      "2020-08-08 18:19:30,280 INFO: | epoch   4 | step 44700 | batch 10950/11250 | lr 0.00000 0.00000 | loss 0.1359 | s/batch 0.74\n",
      "2020-08-08 18:20:09,174 INFO: | epoch   4 | step 44750 | batch 11000/11250 | lr 0.00000 0.00000 | loss 0.1527 | s/batch 0.78\n",
      "2020-08-08 18:20:41,983 INFO: | epoch   4 | step 44800 | batch 11050/11250 | lr 0.00000 0.00000 | loss 0.0983 | s/batch 0.66\n",
      "2020-08-08 18:21:18,499 INFO: | epoch   4 | step 44850 | batch 11100/11250 | lr 0.00000 0.00000 | loss 0.1217 | s/batch 0.73\n",
      "2020-08-08 18:21:50,155 INFO: | epoch   4 | step 44900 | batch 11150/11250 | lr 0.00000 0.00000 | loss 0.0978 | s/batch 0.63\n",
      "2020-08-08 18:22:21,541 INFO: | epoch   4 | step 44950 | batch 11200/11250 | lr 0.00000 0.00000 | loss 0.0681 | s/batch 0.63\n",
      "2020-08-08 18:22:51,243 INFO: | epoch   4 | step 45000 | batch 11250/11250 | lr 0.00000 0.00000 | loss 0.1023 | s/batch 0.59\n",
      "2020-08-08 18:22:51,500 INFO: | epoch   4 | score (96.42, 96.1, 96.26) | f1 96.26 | loss 0.1086 | time 7624.06\n",
      "2020-08-08 18:22:51,853 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9647    0.9670    0.9659     35027\n",
      "          股票     0.9699    0.9679    0.9689     33251\n",
      "          体育     0.9919    0.9933    0.9926     28283\n",
      "          娱乐     0.9773    0.9823    0.9798     19920\n",
      "          时政     0.9416    0.9490    0.9453     13515\n",
      "          社会     0.9403    0.9411    0.9407     11009\n",
      "          教育     0.9695    0.9701    0.9698      8987\n",
      "          财经     0.9278    0.9152    0.9214      7957\n",
      "          家居     0.9682    0.9642    0.9662      7063\n",
      "          游戏     0.9553    0.9420    0.9486      5291\n",
      "          房产     0.9914    0.9860    0.9887      4428\n",
      "          时尚     0.9618    0.9571    0.9594      2818\n",
      "          彩票     0.9646    0.9487    0.9566      1639\n",
      "          星座     0.9740    0.9704    0.9722       812\n",
      "\n",
      "    accuracy                         0.9672    180000\n",
      "   macro avg     0.9642    0.9610    0.9626    180000\n",
      "weighted avg     0.9672    0.9672    0.9672    180000\n",
      "\n",
      "2020-08-08 18:33:09,665 INFO: | epoch   4 | dev | score (95.7, 95.54, 95.61) | f1 95.61 | time 617.81\n",
      "2020-08-08 18:33:09,707 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9647    0.9550    0.9598      3891\n",
      "          股票     0.9662    0.9659    0.9660      3694\n",
      "          体育     0.9845    0.9930    0.9887      3142\n",
      "          娱乐     0.9738    0.9747    0.9743      2213\n",
      "          时政     0.9181    0.9414    0.9296      1501\n",
      "          社会     0.9375    0.9321    0.9348      1223\n",
      "          教育     0.9632    0.9699    0.9666       998\n",
      "          财经     0.9119    0.9016    0.9067       884\n",
      "          家居     0.9520    0.9605    0.9562       784\n",
      "          游戏     0.9545    0.9302    0.9422       587\n",
      "          房产     0.9979    0.9878    0.9928       492\n",
      "          时尚     0.9427    0.9457    0.9442       313\n",
      "          彩票     0.9713    0.9286    0.9494       182\n",
      "          星座     0.9596    0.9896    0.9744        96\n",
      "\n",
      "    accuracy                         0.9612     20000\n",
      "   macro avg     0.9570    0.9554    0.9561     20000\n",
      "weighted avg     0.9612    0.9612    0.9612     20000\n",
      "\n",
      "2020-08-08 18:33:09,709 INFO: Exceed history dev = 95.47, current dev = 95.61\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
